-   [F.6. bloom --- bloom filter index access
    method](#f.6.-bloom-bloom-filter-index-access-method){#toc-f.6.-bloom-bloom-filter-index-access-method}
-   [F.6. bloom --- bloom filter index access
    method](#f.6.-bloom-bloom-filter-index-access-method-1){#toc-f.6.-bloom-bloom-filter-index-access-method-1}
    -   [F.6.1. Parameters](#f.6.1.-parameters){#toc-f.6.1.-parameters}
    -   [F.6.2. Examples](#f.6.2.-examples){#toc-f.6.2.-examples}
    -   [F.6.3. Operator Class
        Interface](#f.6.3.-operator-class-interface){#toc-f.6.3.-operator-class-interface}
    -   [F.6.4.
        Limitations](#f.6.4.-limitations){#toc-f.6.4.-limitations}
    -   [F.6.5. Authors](#f.6.5.-authors){#toc-f.6.5.-authors}
-   [F.9. citext --- a case-insensitive character string
    type](#f.9.-citext-a-case-insensitive-character-string-type){#toc-f.9.-citext-a-case-insensitive-character-string-type}
-   [F.9. citext --- a case-insensitive character string
    type](#f.9.-citext-a-case-insensitive-character-string-type-1){#toc-f.9.-citext-a-case-insensitive-character-string-type-1}
    -   [Tip](#tip){#toc-tip}
    -   [F.9.1. Rationale](#f.9.1.-rationale){#toc-f.9.1.-rationale}
    -   [F.9.2. How to Use
        It](#f.9.2.-how-to-use-it){#toc-f.9.2.-how-to-use-it}
    -   [F.9.3. String Comparison
        Behavior](#f.9.3.-string-comparison-behavior){#toc-f.9.3.-string-comparison-behavior}
    -   [F.9.4.
        Limitations](#f.9.4.-limitations){#toc-f.9.4.-limitations}
    -   [F.9.5. Author](#f.9.5.-author){#toc-f.9.5.-author}
-   [F.39. spi --- Server Programming Interface
    features/examples](#f.39.-spi-server-programming-interface-featuresexamples){#toc-f.39.-spi-server-programming-interface-featuresexamples}
-   [F.39. spi --- Server Programming Interface
    features/examples](#f.39.-spi-server-programming-interface-featuresexamples-1){#toc-f.39.-spi-server-programming-interface-featuresexamples-1}
    -   [F.39.1. refint --- Functions for Implementing Referential
        Integrity](#f.39.1.-refint-functions-for-implementing-referential-integrity){#toc-f.39.1.-refint-functions-for-implementing-referential-integrity}
    -   [F.39.2. autoinc --- Functions for Autoincrementing
        Fields](#f.39.2.-autoinc-functions-for-autoincrementing-fields){#toc-f.39.2.-autoinc-functions-for-autoincrementing-fields}
    -   [F.39.3. insert_username --- Functions for Tracking Who Changed
        a
        Table](#f.39.3.-insert_username-functions-for-tracking-who-changed-a-table){#toc-f.39.3.-insert_username-functions-for-tracking-who-changed-a-table}
    -   [F.39.4. moddatetime --- Functions for Tracking Last
        Modification
        Time](#f.39.4.-moddatetime-functions-for-tracking-last-modification-time){#toc-f.39.4.-moddatetime-functions-for-tracking-last-modification-time}
-   [F.12. dict_int --- example full-text search dictionary for
    integers](#f.12.-dict_int-example-full-text-search-dictionary-for-integers){#toc-f.12.-dict_int-example-full-text-search-dictionary-for-integers}
-   [F.12. dict_int --- example full-text search dictionary for
    integers](#f.12.-dict_int-example-full-text-search-dictionary-for-integers-1){#toc-f.12.-dict_int-example-full-text-search-dictionary-for-integers-1}
    -   [F.12.1.
        Configuration](#f.12.1.-configuration){#toc-f.12.1.-configuration}
    -   [F.12.2. Usage](#f.12.2.-usage){#toc-f.12.2.-usage}
-   [envvar 1.0.0](#envvar-1.0.0){#toc-envvar-1.0.0}
    -   [Synopsis](#synopsis){#toc-synopsis}
    -   [Description](#description){#toc-description}
    -   [Author](#author){#toc-author}
    -   [Copyright and
        License](#copyright-and-license){#toc-copyright-and-license}
    -   [F.16. fuzzystrmatch --- determine string similarities and
        distance](#f.16.-fuzzystrmatch-determine-string-similarities-and-distance){#toc-f.16.-fuzzystrmatch-determine-string-similarities-and-distance}
    -   [F.16. fuzzystrmatch --- determine string similarities and
        distance](#f.16.-fuzzystrmatch-determine-string-similarities-and-distance-1){#toc-f.16.-fuzzystrmatch-determine-string-similarities-and-distance-1}
        -   [Caution](#caution){#toc-caution}
        -   [F.16.1. Soundex](#f.16.1.-soundex){#toc-f.16.1.-soundex}
        -   [F.16.2. Daitch-Mokotoff
            Soundex](#f.16.2.-daitch-mokotoff-soundex){#toc-f.16.2.-daitch-mokotoff-soundex}
        -   [F.16.3.
            Levenshtein](#f.16.3.-levenshtein){#toc-f.16.3.-levenshtein}
        -   [F.16.4.
            Metaphone](#f.16.4.-metaphone){#toc-f.16.4.-metaphone}
        -   [F.16.5. Double
            Metaphone](#f.16.5.-double-metaphone){#toc-f.16.5.-double-metaphone}
    -   [F.17. hstore --- hstore key/value
        datatype](#f.17.-hstore-hstore-keyvalue-datatype){#toc-f.17.-hstore-hstore-keyvalue-datatype}
    -   [F.17. hstore --- hstore key/value
        datatype](#f.17.-hstore-hstore-keyvalue-datatype-1){#toc-f.17.-hstore-hstore-keyvalue-datatype-1}
        -   [F.17.1. `hstore` External
            Representation](#f.17.1.-hstore-external-representation){#toc-f.17.1.-hstore-external-representation}
        -   [Note](#note){#toc-note}
        -   [F.17.2. `hstore` Operators and
            Functions](#f.17.2.-hstore-operators-and-functions){#toc-f.17.2.-hstore-operators-and-functions}
    -   [Operator Description
        Example(s)](#operator-description-examples){#toc-operator-description-examples}
    -   [Function Description
        Example(s)](#function-description-examples){#toc-function-description-examples}
        -   [F.17.3. Indexes](#f.17.3.-indexes){#toc-f.17.3.-indexes}
        -   [F.17.4. Examples](#f.17.4.-examples){#toc-f.17.4.-examples}
        -   [F.17.5.
            Statistics](#f.17.5.-statistics){#toc-f.17.5.-statistics}
        -   [F.17.6.
            Compatibility](#f.17.6.-compatibility){#toc-f.17.6.-compatibility}
        -   [F.17.7.
            Transforms](#f.17.7.-transforms){#toc-f.17.7.-transforms}
        -   [Caution](#caution-1){#toc-caution-1}
        -   [F.17.8. Authors](#f.17.8.-authors){#toc-f.17.8.-authors}
    -   [F.18. intagg --- integer aggregator and
        enumerator](#f.18.-intagg-integer-aggregator-and-enumerator){#toc-f.18.-intagg-integer-aggregator-and-enumerator}
    -   [F.18. intagg --- integer aggregator and
        enumerator](#f.18.-intagg-integer-aggregator-and-enumerator-1){#toc-f.18.-intagg-integer-aggregator-and-enumerator-1}
        -   [F.18.1.
            Functions](#f.18.1.-functions){#toc-f.18.1.-functions}
        -   [F.18.2. Sample
            Uses](#f.18.2.-sample-uses){#toc-f.18.2.-sample-uses}
    -   [F.19. intarray --- manipulate arrays of
        integers](#f.19.-intarray-manipulate-arrays-of-integers){#toc-f.19.-intarray-manipulate-arrays-of-integers}
    -   [F.19. intarray --- manipulate arrays of
        integers](#f.19.-intarray-manipulate-arrays-of-integers-1){#toc-f.19.-intarray-manipulate-arrays-of-integers-1}
        -   [F.19.1. `intarray` Functions and
            Operators](#f.19.1.-intarray-functions-and-operators){#toc-f.19.1.-intarray-functions-and-operators}
    -   [Function Description
        Example(s)](#function-description-examples-1){#toc-function-description-examples-1}
    -   [Operator
        Description](#operator-description){#toc-operator-description}
        -   [F.19.2. Index
            Support](#f.19.2.-index-support){#toc-f.19.2.-index-support}
        -   [F.19.3. Example](#f.19.3.-example){#toc-f.19.3.-example}
        -   [F.19.4.
            Benchmark](#f.19.4.-benchmark){#toc-f.19.4.-benchmark}
        -   [F.19.5. Authors](#f.19.5.-authors){#toc-f.19.5.-authors}
-   [jsonschema](#jsonschema){#toc-jsonschema}
    -   [Synopsis](#synopsis-1){#toc-synopsis-1}
    -   [Description](#description-1){#toc-description-1}
    -   [What, Another One?](#what-another-one){#toc-what-another-one}
    -   [Schema
        Composition](#schema-composition){#toc-schema-composition}
    -   [Configuration](#configuration){#toc-configuration}
    -   [Functions](#functions){#toc-functions}
        -   [`jsonschema_is_valid(schema)`](#jsonschema_is_validschema){#toc-jsonschema_is_validschema}
        -   [`jsonschema_is_valid(id, schema)`](#jsonschema_is_validid-schema){#toc-jsonschema_is_validid-schema}
        -   [`jsonschema_validates(data, schema)`](#jsonschema_validatesdata-schema){#toc-jsonschema_validatesdata-schema}
        -   [`jsonschema_validates(data, id, schema)`](#jsonschema_validatesdata-id-schema){#toc-jsonschema_validatesdata-id-schema}
        -   [`json_matches_schema(schema, instance)`](#json_matches_schemaschema-instance){#toc-json_matches_schemaschema-instance}
    -   [Prior Art](#prior-art){#toc-prior-art}
    -   [Support](#support){#toc-support}
    -   [Authors](#authors){#toc-authors}
    -   [Copyright and
        License](#copyright-and-license-1){#toc-copyright-and-license-1}
    -   [F.22. ltree --- hierarchical tree-like data
        type](#f.22.-ltree-hierarchical-tree-like-data-type){#toc-f.22.-ltree-hierarchical-tree-like-data-type}
    -   [F.22. ltree --- hierarchical tree-like data
        type](#f.22.-ltree-hierarchical-tree-like-data-type-1){#toc-f.22.-ltree-hierarchical-tree-like-data-type-1}
        -   [F.22.1.
            Definitions](#f.22.1.-definitions){#toc-f.22.1.-definitions}
        -   [F.22.2. Operators and
            Functions](#f.22.2.-operators-and-functions){#toc-f.22.2.-operators-and-functions}
    -   [Operator
        Description](#operator-description-1){#toc-operator-description-1}
    -   [Function Description
        Example(s)](#function-description-examples-2){#toc-function-description-examples-2}
        -   [F.22.3. Indexes](#f.22.3.-indexes){#toc-f.22.3.-indexes}
        -   [F.22.4. Example](#f.22.4.-example){#toc-f.22.4.-example}
        -   [F.22.5.
            Transforms](#f.22.5.-transforms){#toc-f.22.5.-transforms}
        -   [Caution](#caution-2){#toc-caution-2}
        -   [F.22.6. Authors](#f.22.6.-authors){#toc-f.22.6.-authors}
    -   [What is pg_cron?](#what-is-pg_cron){#toc-what-is-pg_cron}
    -   [Installing
        pg_cron](#installing-pg_cron){#toc-installing-pg_cron}
    -   [Setting up
        pg_cron](#setting-up-pg_cron){#toc-setting-up-pg_cron}
        -   [Ensuring pg_cron can start
            jobs](#ensuring-pg_cron-can-start-jobs){#toc-ensuring-pg_cron-can-start-jobs}
    -   [Viewing job run
        details](#viewing-job-run-details){#toc-viewing-job-run-details}
    -   [Extension
        settings](#extension-settings){#toc-extension-settings}
        -   [Changing
            settings](#changing-settings){#toc-changing-settings}
    -   [Example use cases](#example-use-cases){#toc-example-use-cases}
    -   [Managed services](#managed-services){#toc-managed-services}
    -   [Code of Conduct](#code-of-conduct){#toc-code-of-conduct}
-   [pg_embedding](#pg_embedding){#toc-pg_embedding}
    -   [Using the pg_embedding
        extension](#using-the-pg_embedding-extension){#toc-using-the-pg_embedding-extension}
        -   [Usage summary](#usage-summary){#toc-usage-summary}
        -   [Enable the
            extension](#enable-the-extension){#toc-enable-the-extension}
        -   [Create a table for your vector
            data](#create-a-table-for-your-vector-data){#toc-create-a-table-for-your-vector-data}
        -   [Insert data](#insert-data){#toc-insert-data}
    -   [Query](#query){#toc-query}
        -   [Create an HNSW
            index](#create-an-hnsw-index){#toc-create-an-hnsw-index}
        -   [Tuning the HNSW
            algorithm](#tuning-the-hnsw-algorithm){#toc-tuning-the-hnsw-algorithm}
    -   [How HNSW search
        works](#how-hnsw-search-works){#toc-how-hnsw-search-works}
    -   [References](#references){#toc-references}
-   [pg_later](#pg_later){#toc-pg_later}
    -   [Installation](#installation){#toc-installation}
        -   [Run with docker](#run-with-docker){#toc-run-with-docker}
        -   [Using the
            extension](#using-the-extension){#toc-using-the-extension}
-   [PostgreSQL Partition
    Manager](#postgresql-partition-manager){#toc-postgresql-partition-manager}
    -   [DOCUMENTATION](#documentation){#toc-documentation}
    -   [INSTALLATION](#installation-1){#toc-installation-1}
        -   [From Source](#from-source){#toc-from-source}
        -   [Package](#package){#toc-package}
        -   [Setup](#setup){#toc-setup}
    -   [UPGRADE](#upgrade){#toc-upgrade}
    -   [EXAMPLES](#examples){#toc-examples}
    -   [TESTING](#testing){#toc-testing}
-   [**pg_vectorize: a VectorDB for
    Postgres**](#pg_vectorize-a-vectordb-for-postgres){#toc-pg_vectorize-a-vectordb-for-postgres}
    -   [Features](#features){#toc-features}
    -   [Table of Contents](#table-of-contents){#toc-table-of-contents}
    -   [Installation](#installation-2){#toc-installation-2}
    -   [Vector Search
        Example](#vector-search-example){#toc-vector-search-example}
    -   [RAG Example](#rag-example){#toc-rag-example}
    -   [Updating
        Embeddings](#updating-embeddings){#toc-updating-embeddings}
    -   [Directly Interact with
        LLMs](#directly-interact-with-llms){#toc-directly-interact-with-llms}
    -   [Importing Pre-existing
        Embeddings](#importing-pre-existing-embeddings){#toc-importing-pre-existing-embeddings}
    -   [Creating a Table from Existing
        Embeddings](#creating-a-table-from-existing-embeddings){#toc-creating-a-table-from-existing-embeddings}
    -   [Contributing](#contributing){#toc-contributing}
    -   [Community Support](#community-support){#toc-community-support}
-   [Postgres Message Queue
    (PGMQ)](#postgres-message-queue-pgmq){#toc-postgres-message-queue-pgmq}
    -   [Features](#features-1){#toc-features-1}
    -   [Table of
        Contents](#table-of-contents-1){#toc-table-of-contents-1}
    -   [Installation](#installation-3){#toc-installation-3}
        -   [Updating](#updating){#toc-updating}
    -   [Client Libraries](#client-libraries){#toc-client-libraries}
    -   [SQL Examples](#sql-examples){#toc-sql-examples}
        -   [Creating a queue](#creating-a-queue){#toc-creating-a-queue}
        -   [Send two
            messages](#send-two-messages){#toc-send-two-messages}
        -   [Read messages](#read-messages){#toc-read-messages}
        -   [Pop a message](#pop-a-message){#toc-pop-a-message}
        -   [Archive a
            message](#archive-a-message){#toc-archive-a-message}
        -   [Delete a message](#delete-a-message){#toc-delete-a-message}
        -   [Drop a queue](#drop-a-queue){#toc-drop-a-queue}
    -   [Configuration](#configuration-1){#toc-configuration-1}
        -   [Partitioned
            Queues](#partitioned-queues){#toc-partitioned-queues}
    -   [Visibility Timeout
        (vt)](#visibility-timeout-vt){#toc-visibility-timeout-vt}
    -   [Who uses pgmq?](#who-uses-pgmq){#toc-who-uses-pgmq}
    -   [✨ Contributors](#contributors){#toc-contributors}
-   [PostgreSQL HTTP
    Client](#postgresql-http-client){#toc-postgresql-http-client}
    -   [Motivation](#motivation){#toc-motivation}
    -   [Examples](#examples-1){#toc-examples-1}
    -   [Concepts](#concepts){#toc-concepts}
    -   [Functions](#functions-1){#toc-functions-1}
    -   [CURL Options](#curl-options){#toc-curl-options}
    -   [User Agents](#user-agents){#toc-user-agents}
    -   [Keep-Alive &
        Timeouts](#keep-alive-timeouts){#toc-keep-alive-timeouts}
    -   [Installation](#installation-4){#toc-installation-4}
        -   [Debian / Ubuntu
            apt.postgresql.org](#debian-ubuntu-apt.postgresql.org){#toc-debian-ubuntu-apt.postgresql.org}
        -   [Compile from
            Source](#compile-from-source){#toc-compile-from-source}
        -   [Windows](#windows){#toc-windows}
        -   [Testing](#testing-1){#toc-testing-1}
    -   [Why This is a Bad
        Idea](#why-this-is-a-bad-idea){#toc-why-this-is-a-bad-idea}
    -   [To Do](#to-do){#toc-to-do}
    -   [F.33. pg_trgm --- support for similarity of text using trigram
        matching](#f.33.-pg_trgm-support-for-similarity-of-text-using-trigram-matching){#toc-f.33.-pg_trgm-support-for-similarity-of-text-using-trigram-matching}
    -   [F.33. pg_trgm --- support for similarity of text using trigram
        matching](#f.33.-pg_trgm-support-for-similarity-of-text-using-trigram-matching-1){#toc-f.33.-pg_trgm-support-for-similarity-of-text-using-trigram-matching-1}
        -   [F.33.1. Trigram (or Trigraph)
            Concepts](#f.33.1.-trigram-or-trigraph-concepts){#toc-f.33.1.-trigram-or-trigraph-concepts}
        -   [Note](#note-1){#toc-note-1}
        -   [F.33.2. Functions and
            Operators](#f.33.2.-functions-and-operators){#toc-f.33.2.-functions-and-operators}
    -   [Function
        Description](#function-description){#toc-function-description}
    -   [Operator
        Description](#operator-description-2){#toc-operator-description-2}
        -   [F.33.3. GUC
            Parameters](#f.33.3.-guc-parameters){#toc-f.33.3.-guc-parameters}
        -   [F.33.4. Index
            Support](#f.33.4.-index-support){#toc-f.33.4.-index-support}
        -   [F.33.5. Text Search
            Integration](#f.33.5.-text-search-integration){#toc-f.33.5.-text-search-integration}
        -   [Note](#note-2){#toc-note-2}
        -   [F.33.6.
            References](#f.33.6.-references){#toc-f.33.6.-references}
        -   [F.33.7. Authors](#f.33.7.-authors){#toc-f.33.7.-authors}
-   [pgvector](#pgvector){#toc-pgvector}
    -   [Installation](#installation-5){#toc-installation-5}
        -   [Linux and Mac](#linux-and-mac){#toc-linux-and-mac}
        -   [Windows](#windows-1){#toc-windows-1}
    -   [Getting Started](#getting-started){#toc-getting-started}
    -   [Storing](#storing){#toc-storing}
    -   [Querying](#querying){#toc-querying}
    -   [Indexing](#indexing){#toc-indexing}
    -   [HNSW](#hnsw){#toc-hnsw}
        -   [Index Options](#index-options){#toc-index-options}
        -   [Query Options](#query-options){#toc-query-options}
        -   [Index Build Time](#index-build-time){#toc-index-build-time}
        -   [Indexing
            Progress](#indexing-progress){#toc-indexing-progress}
    -   [IVFFlat](#ivfflat){#toc-ivfflat}
        -   [Query Options](#query-options-1){#toc-query-options-1}
        -   [Index Build
            Time](#index-build-time-1){#toc-index-build-time-1}
        -   [Indexing
            Progress](#indexing-progress-1){#toc-indexing-progress-1}
    -   [Filtering](#filtering){#toc-filtering}
    -   [Iterative Index
        Scans](#iterative-index-scans){#toc-iterative-index-scans}
        -   [Iterative Scan
            Options](#iterative-scan-options){#toc-iterative-scan-options}
    -   [Half-Precision
        Vectors](#half-precision-vectors){#toc-half-precision-vectors}
    -   [Half-Precision
        Indexing](#half-precision-indexing){#toc-half-precision-indexing}
    -   [Binary Vectors](#binary-vectors){#toc-binary-vectors}
    -   [Binary
        Quantization](#binary-quantization){#toc-binary-quantization}
    -   [Sparse Vectors](#sparse-vectors){#toc-sparse-vectors}
    -   [Hybrid Search](#hybrid-search){#toc-hybrid-search}
    -   [Indexing
        Subvectors](#indexing-subvectors){#toc-indexing-subvectors}
    -   [Performance](#performance){#toc-performance}
        -   [Tuning](#tuning){#toc-tuning}
        -   [Loading](#loading){#toc-loading}
        -   [Indexing](#indexing-1){#toc-indexing-1}
        -   [Querying](#querying-1){#toc-querying-1}
        -   [Vacuuming](#vacuuming){#toc-vacuuming}
    -   [Monitoring](#monitoring){#toc-monitoring}
    -   [Scaling](#scaling){#toc-scaling}
    -   [Languages](#languages){#toc-languages}
    -   [Frequently Asked
        Questions](#frequently-asked-questions){#toc-frequently-asked-questions}
    -   [Troubleshooting](#troubleshooting){#toc-troubleshooting}
    -   [Reference](#reference){#toc-reference}
        -   [Vector Type](#vector-type){#toc-vector-type}
        -   [Vector Operators](#vector-operators){#toc-vector-operators}
        -   [Vector Functions](#vector-functions){#toc-vector-functions}
        -   [Vector Aggregate
            Functions](#vector-aggregate-functions){#toc-vector-aggregate-functions}
        -   [Halfvec Type](#halfvec-type){#toc-halfvec-type}
        -   [Halfvec
            Operators](#halfvec-operators){#toc-halfvec-operators}
        -   [Halfvec
            Functions](#halfvec-functions){#toc-halfvec-functions}
        -   [Halfvec Aggregate
            Functions](#halfvec-aggregate-functions){#toc-halfvec-aggregate-functions}
        -   [Bit Type](#bit-type){#toc-bit-type}
        -   [Bit Operators](#bit-operators){#toc-bit-operators}
        -   [Bit Functions](#bit-functions){#toc-bit-functions}
        -   [Sparsevec Type](#sparsevec-type){#toc-sparsevec-type}
        -   [Sparsevec
            Operators](#sparsevec-operators){#toc-sparsevec-operators}
        -   [Sparsevec
            Functions](#sparsevec-functions){#toc-sparsevec-functions}
    -   [Installation Notes - Linux and
        Mac](#installation-notes---linux-and-mac){#toc-installation-notes---linux-and-mac}
        -   [Postgres
            Location](#postgres-location){#toc-postgres-location}
        -   [Missing Header](#missing-header){#toc-missing-header}
        -   [Missing SDK](#missing-sdk){#toc-missing-sdk}
        -   [Portability](#portability){#toc-portability}
    -   [Installation Notes -
        Windows](#installation-notes---windows){#toc-installation-notes---windows}
        -   [Missing Header](#missing-header-1){#toc-missing-header-1}
        -   [Mismatched
            Architecture](#mismatched-architecture){#toc-mismatched-architecture}
        -   [Missing Symbol](#missing-symbol){#toc-missing-symbol}
        -   [Permissions](#permissions){#toc-permissions}
    -   [Additional Installation
        Methods](#additional-installation-methods){#toc-additional-installation-methods}
        -   [Docker](#docker){#toc-docker}
        -   [Homebrew](#homebrew){#toc-homebrew}
        -   [PGXN](#pgxn){#toc-pgxn}
        -   [APT](#apt){#toc-apt}
        -   [Yum](#yum){#toc-yum}
        -   [pkg](#pkg){#toc-pkg}
        -   [conda-forge](#conda-forge){#toc-conda-forge}
        -   [Postgres.app](#postgres.app){#toc-postgres.app}
    -   [Hosted Postgres](#hosted-postgres){#toc-hosted-postgres}
    -   [Upgrading](#upgrading){#toc-upgrading}
    -   [Thanks](#thanks){#toc-thanks}
    -   [History](#history){#toc-history}
    -   [Contributing](#contributing-1){#toc-contributing-1}
    -   [Chapter 44. PL/Python --- Python Procedural
        Language](#chapter-44.-plpython-python-procedural-language){#toc-chapter-44.-plpython-python-procedural-language}
    -   [Chapter 44. PL/Python --- Python Procedural
        Language](#chapter-44.-plpython-python-procedural-language-1){#toc-chapter-44.-plpython-python-procedural-language-1}
        -   [Tip](#tip-1){#toc-tip-1}
        -   [Note](#note-3){#toc-note-3}
    -   [F.47. uuid-ossp --- a UUID
        generator](#f.47.-uuid-ossp-a-uuid-generator){#toc-f.47.-uuid-ossp-a-uuid-generator}
    -   [F.47. uuid-ossp --- a UUID
        generator](#f.47.-uuid-ossp-a-uuid-generator-1){#toc-f.47.-uuid-ossp-a-uuid-generator-1}
        -   [F.47.1. `uuid-ossp`
            Functions](#f.47.1.-uuid-ossp-functions){#toc-f.47.1.-uuid-ossp-functions}
    -   [Function
        Description](#function-description-1){#toc-function-description-1}
    -   [Function
        Description](#function-description-2){#toc-function-description-2}
        -   [F.47.2. Building
            `uuid-ossp`](#f.47.2.-building-uuid-ossp){#toc-f.47.2.-building-uuid-ossp}
        -   [F.47.3. Author](#f.47.3.-author){#toc-f.47.3.-author}
    -   [F.48. xml2 --- XPath querying and XSLT
        functionality](#f.48.-xml2-xpath-querying-and-xslt-functionality){#toc-f.48.-xml2-xpath-querying-and-xslt-functionality}
    -   [F.48. xml2 --- XPath querying and XSLT
        functionality](#f.48.-xml2-xpath-querying-and-xslt-functionality-1){#toc-f.48.-xml2-xpath-querying-and-xslt-functionality-1}
        -   [F.48.1. Deprecation
            Notice](#f.48.1.-deprecation-notice){#toc-f.48.1.-deprecation-notice}
        -   [F.48.2. Description of
            Functions](#f.48.2.-description-of-functions){#toc-f.48.2.-description-of-functions}
    -   [Function
        Description](#function-description-3){#toc-function-description-3}
        -   [F.48.3.
            `xpath_table`](#f.48.3.-xpath_table){#toc-f.48.3.-xpath_table}
        -   [F.48.4. XSLT
            Functions](#f.48.4.-xslt-functions){#toc-f.48.4.-xslt-functions}
        -   [F.48.5. Author](#f.48.5.-author){#toc-f.48.5.-author}

## F.6. bloom --- bloom filter index access method

[Prev](https://www.postgresql.org/docs/basic-archive.html "F.5. basic_archive — an example WAL archive module")
\|
[Up](https://www.postgresql.org/docs/contrib.html "Appendix F. Additional Supplied Modules and Extensions")
\| Appendix F. Additional Supplied Modules and Extensions \|
[Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")
\|
[Next](https://www.postgresql.org/docs/btree-gin.html "F.7. btree_gin — GIN operator classes with B-tree behavior")

------------------------------------------------------------------------

## F.6. bloom --- bloom filter index access method

[F.6.1.
Parameters](https://www.postgresql.org/docs/bloom.html#BLOOM-%20PARAMETERS)

[F.6.2.
Examples](https://www.postgresql.org/docs/bloom.html#BLOOM-EXAMPLES)

[F.6.3. Operator Class
Interface](https://www.postgresql.org/docs/bloom.html#BLOOM-OPERATOR-CLASS-%20INTERFACE)

[F.6.4.
Limitations](https://www.postgresql.org/docs/bloom.html#BLOOM-%20LIMITATIONS)

[F.6.5.
Authors](https://www.postgresql.org/docs/bloom.html#BLOOM-AUTHORS)

`bloom` provides an index access method based on [Bloom
filters](https://en.wikipedia.org/wiki/Bloom_filter).

A Bloom filter is a space-efficient data structure that is used to test
whether an element is a member of a set. In the case of an index access
method, it allows fast exclusion of non-matching tuples via signatures
whose size is determined at index creation.

A signature is a lossy representation of the indexed attribute(s), and
as such is prone to reporting false positives; that is, it may be
reported that an element is in the set, when it is not. So index search
results must always be rechecked using the actual attribute values from
the heap entry. Larger signatures reduce the odds of a false positive
and thus reduce the number of useless heap visits, but of course also
make the index larger and hence slower to scan.

This type of index is most useful when a table has many attributes and
queries test arbitrary combinations of them. A traditional btree index
is faster than a bloom index, but it can require many btree indexes to
support all possible queries where one needs only a single bloom index.
Note however that bloom indexes only support equality queries, whereas
btree indexes can also perform inequality and range searches.

### F.6.1. Parameters

A `bloom` index accepts the following parameters in its `WITH` clause:

`length`

Length of each signature (index entry) in bits. It is rounded up to the
nearest multiple of `16`. The default is `80` bits and the maximum is
`4096`.

`col1 — col32`

Number of bits generated for each index column. Each parameter's name
refers to the number of the index column that it controls. The default
is `2` bits and the maximum is `4095`. Parameters for index columns not
actually used are ignored.

### F.6.2. Examples

This is an example of creating a bloom index:

    CREATE INDEX bloomidx ON tbloom USING bloom (i1,i2,i3)
           WITH (length=80, col1=2, col2=2, col3=4);

The index is created with a signature length of 80 bits, with attributes
i1 and i2 mapped to 2 bits, and attribute i3 mapped to 4 bits. We could
have omitted the `length`, `col1`, and `col2` specifications since those
have the default values.

Here is a more complete example of bloom index definition and usage, as
well as a comparison with equivalent btree indexes. The bloom index is
considerably smaller than the btree index, and can perform better.

    =# CREATE TABLE tbloom AS
       SELECT
         (random() * 1000000)::int as i1,
         (random() * 1000000)::int as i2,
         (random() * 1000000)::int as i3,
         (random() * 1000000)::int as i4,
         (random() * 1000000)::int as i5,
         (random() * 1000000)::int as i6
       FROM
      generate_series(1,10000000);
    SELECT 10000000

A sequential scan over this large table takes a long time:

    =# EXPLAIN ANALYZE SELECT * FROM tbloom WHERE i2 = 898732 AND i5 = 123451;
                                                  QUERY PLAN
    -------------------------------------------------------------------​-----------------------------------
     Seq Scan on tbloom  (cost=0.00..213744.00 rows=250 width=24) (actual time=357.059..357.059 rows=0 loops=1)
       Filter: ((i2 = 898732) AND (i5 = 123451))
       Rows Removed by Filter: 10000000
     Planning Time: 0.346 ms
     Execution Time: 357.076 ms
    (5 rows)

Even with the btree index defined the result will still be a sequential
scan:

    =# CREATE INDEX btreeidx ON tbloom (i1, i2, i3, i4, i5, i6);
    CREATE INDEX
    =# SELECT pg_size_pretty(pg_relation_size('btreeidx'));
     pg_size_pretty
    ----------------
     386 MB
    (1 row)
    =# EXPLAIN ANALYZE SELECT * FROM tbloom WHERE i2 = 898732 AND i5 = 123451;
                                                  QUERY PLAN
    -------------------------------------------------------------------​-----------------------------------
     Seq Scan on tbloom  (cost=0.00..213744.00 rows=2 width=24) (actual time=351.016..351.017 rows=0 loops=1)
       Filter: ((i2 = 898732) AND (i5 = 123451))
       Rows Removed by Filter: 10000000
     Planning Time: 0.138 ms
     Execution Time: 351.035 ms
    (5 rows)

Having the bloom index defined on the table is better than btree in
handling this type of search:

    =# CREATE INDEX bloomidx ON tbloom USING bloom (i1, i2, i3, i4, i5, i6);
    CREATE INDEX
    =# SELECT pg_size_pretty(pg_relation_size('bloomidx'));
     pg_size_pretty
    ----------------
     153 MB
    (1 row)
    =# EXPLAIN ANALYZE SELECT * FROM tbloom WHERE i2 = 898732 AND i5 = 123451;
                                                         QUERY PLAN
    -------------------------------------------------------------------​--------------------------------------------------
     Bitmap Heap Scan on tbloom  (cost=1792.00..1799.69 rows=2 width=24) (actual time=22.605..22.606 rows=0 loops=1)
       Recheck Cond: ((i2 = 898732) AND (i5 = 123451))
       Rows Removed by Index Recheck: 2300
       Heap Blocks: exact=2256
       ->  Bitmap Index Scan on bloomidx  (cost=0.00..178436.00 rows=1 width=0) (actual time=20.005..20.005 rows=2300 loops=1)
             Index Cond: ((i2 = 898732) AND (i5 = 123451))
     Planning Time: 0.099 ms
     Execution Time: 22.632 ms
    (8 rows)

Now, the main problem with the btree search is that btree is inefficient
when the search conditions do not constrain the leading index column(s).
A better strategy for btree is to create a separate index on each
column. Then the planner will choose something like this:

    =# CREATE INDEX btreeidx1 ON tbloom (i1);
    CREATE INDEX
    =# CREATE INDEX btreeidx2 ON tbloom (i2);
    CREATE INDEX
    =# CREATE INDEX btreeidx3 ON tbloom (i3);
    CREATE INDEX
    =# CREATE INDEX btreeidx4 ON tbloom (i4);
    CREATE INDEX
    =# CREATE INDEX btreeidx5 ON tbloom (i5);
    CREATE INDEX
    =# CREATE INDEX btreeidx6 ON tbloom (i6);
    CREATE INDEX
    =# EXPLAIN ANALYZE SELECT * FROM tbloom WHERE i2 = 898732 AND i5 = 123451;
                                                            QUERY PLAN
    -------------------------------------------------------------------​--------------------------------------------------------
     Bitmap Heap Scan on tbloom  (cost=9.29..13.30 rows=1 width=24) (actual time=0.032..0.033 rows=0 loops=1)
       Recheck Cond: ((i5 = 123451) AND (i2 = 898732))
       ->  BitmapAnd  (cost=9.29..9.29 rows=1 width=0) (actual time=0.047..0.047 rows=0 loops=1)
             ->  Bitmap Index Scan on btreeidx5  (cost=0.00..4.52 rows=11 width=0) (actual time=0.026..0.026 rows=7 loops=1)
                   Index Cond: (i5 = 123451)
             ->  Bitmap Index Scan on btreeidx2  (cost=0.00..4.52 rows=11 width=0) (actual time=0.007..0.007 rows=8 loops=1)
                   Index Cond: (i2 = 898732)
     Planning Time: 0.264 ms
     Execution Time: 0.047 ms
    (9 rows)

Although this query runs much faster than with either of the single
indexes, we pay a penalty in index size. Each of the single-column btree
indexes occupies 88.5 MB, so the total space needed is 531 MB, over
three times the space used by the bloom index.

### F.6.3. Operator Class Interface

An operator class for bloom indexes requires only a hash function for
the indexed data type and an equality operator for searching. This
example shows the operator class definition for the `text` data type:

    CREATE OPERATOR CLASS text_ops
    DEFAULT FOR TYPE text USING bloom AS
        OPERATOR    1   =(text, text),
        FUNCTION    1   hashtext(text);

### F.6.4. Limitations

-   Only operator classes for `int4` and `text` are included with the
    module.

-   Only the `=` operator is supported for search. But it is possible to
    add support for arrays with union and intersection operations in the
    future.

-   `bloom` access method doesn't support `UNIQUE` indexes.

-   `bloom` access method doesn't support searching for `NULL` values.

### F.6.5. Authors

Teodor Sigaev `<[teodor@postgrespro.ru](mailto:teodor@postgrespro.ru)>`,
Postgres Professional, Moscow, Russia

Alexander Korotkov
`<[a.korotkov@postgrespro.ru](mailto:a.korotkov@postgrespro.ru)>`,
Postgres Professional, Moscow, Russia

Oleg Bartunov
`<[obartunov@postgrespro.ru](mailto:obartunov@postgrespro.ru)>`,
Postgres Professional, Moscow, Russia

------------------------------------------------------------------------

  -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  [Prev](https://www.postgresql.org/docs/basic-archive.html "F.5. basic_archive — an example WAL archive module")   [Up](https://www.postgresql.org/docs/contrib.html "Appendix F. Additional Supplied Modules and Extensions")   [Next](https://www.postgresql.org/docs/btree-gin.html "F.7. btree_gin — GIN operator classes with B-tree behavior")
  ----------------------------------------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------------------------- ---------------------------------------------------------------------------------------------------------------------
  F.5. basic_archive --- an example WAL archive module                                                              [Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")                            F.7. btree_gin --- GIN operator classes with B-tree behavior

  -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## F.9. citext --- a case-insensitive character string type

[Prev](https://www.postgresql.org/docs/btree-gist.html "F.8. btree_gist — GiST operator classes with B-tree behavior")
\|
[Up](https://www.postgresql.org/docs/contrib.html "Appendix F. Additional Supplied Modules and Extensions")
\| Appendix F. Additional Supplied Modules and Extensions \|
[Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")
\|
[Next](https://www.postgresql.org/docs/cube.html "F.10. cube — a multi-dimensional cube data type")

------------------------------------------------------------------------

## F.9. citext --- a case-insensitive character string type

[F.9.1.
Rationale](https://www.postgresql.org/docs/citext.html#CITEXT-%20RATIONALE)

[F.9.2. How to Use
It](https://www.postgresql.org/docs/citext.html#CITEXT-HOW-%20TO-USE-IT)

[F.9.3. String Comparison
Behavior](https://www.postgresql.org/docs/citext.html#CITEXT-STRING-%20COMPARISON-BEHAVIOR)

[F.9.4.
Limitations](https://www.postgresql.org/docs/citext.html#CITEXT-%20LIMITATIONS)

[F.9.5.
Author](https://www.postgresql.org/docs/citext.html#CITEXT-AUTHOR)

The `citext` module provides a case-insensitive character string type,
`citext`. Essentially, it internally calls `lower` when comparing
values. Otherwise, it behaves almost exactly like `text`.

### Tip

Consider using *nondeterministic collations* (see [Section
23.2.2.4](https://www.postgresql.org/docs/collation.html#COLLATION-%20NONDETERMINISTIC "23.2.2.4. Nondeterministic Collations"))
instead of this module. They can be used for case-insensitive
comparisons, accent-insensitive comparisons, and other combinations, and
they handle more Unicode special cases correctly.

This module is considered "trusted", that is, it can be installed by
non- superusers who have `CREATE` privilege on the current database.

### F.9.1. Rationale

The standard approach to doing case-insensitive matches in PostgreSQL
has been to use the `lower` function when comparing values, for example

    SELECT * FROM tab WHERE lower(col) = LOWER(?);

This works reasonably well, but has a number of drawbacks:

-   It makes your SQL statements verbose, and you always have to
    remember to use `lower` on both the column and the query value.

-   It won't use an index, unless you create a functional index using
    `lower`.

-   If you declare a column as `UNIQUE` or `PRIMARY KEY`, the implicitly
    generated index is case-sensitive. So it's useless for
    case-insensitive searches, and it won't enforce uniqueness
    case-insensitively.

The `citext` data type allows you to eliminate calls to `lower` in SQL
queries, and allows a primary key to be case-insensitive. `citext` is
locale- aware, just like `text`, which means that the matching of upper
case and lower case characters is dependent on the rules of the
database's `LC_CTYPE` setting. Again, this behavior is identical to the
use of `lower` in queries. But because it's done transparently by the
data type, you don't have to remember to do anything special in your
queries.

### F.9.2. How to Use It

Here's a simple example of usage:

    CREATE TABLE users (
        nick CITEXT PRIMARY KEY,
        pass TEXT   NOT NULL
    );

    INSERT INTO users VALUES ( 'larry',  sha256(random()::text::bytea) );
    INSERT INTO users VALUES ( 'Tom',    sha256(random()::text::bytea) );
    INSERT INTO users VALUES ( 'Damian', sha256(random()::text::bytea) );
    INSERT INTO users VALUES ( 'NEAL',   sha256(random()::text::bytea) );
    INSERT INTO users VALUES ( 'Bjørn',  sha256(random()::text::bytea) );

    SELECT * FROM users WHERE nick = 'Larry';

The `SELECT` statement will return one tuple, even though the `nick`
column was set to `larry` and the query was for `Larry`.

### F.9.3. String Comparison Behavior

`citext` performs comparisons by converting each string to lower case
(as though `lower` were called) and then comparing the results normally.
Thus, for example, two strings are considered equal if `lower` would
produce identical results for them.

In order to emulate a case-insensitive collation as closely as possible,
there are `citext`-specific versions of a number of string-processing
operators and functions. So, for example, the regular expression
operators `~` and `~*` exhibit the same behavior when applied to
`citext`: they both match case- insensitively. The same is true for `!~`
and `!~*`, as well as for the `LIKE` operators `~~` and `~~*`, and `!~~`
and `!~~*`. If you'd like to match case- sensitively, you can cast the
operator's arguments to `text`.

Similarly, all of the following functions perform matching
case-insensitively if their arguments are `citext`:

-   `regexp_match()`

-   `regexp_matches()`

-   `regexp_replace()`

-   `regexp_split_to_array()`

-   `regexp_split_to_table()`

-   `replace()`

-   `split_part()`

-   `strpos()`

-   `translate()`

For the regexp functions, if you want to match case-sensitively, you can
specify the "c" flag to force a case-sensitive match. Otherwise, you
must cast to `text` before using one of these functions if you want
case-sensitive behavior.

### F.9.4. Limitations

-   `citext`'s case-folding behavior depends on the `LC_CTYPE` setting
    of your database. How it compares values is therefore determined
    when the database is created. It is not truly case-insensitive in
    the terms defined by the Unicode standard. Effectively, what this
    means is that, as long as you're happy with your collation, you
    should be happy with `citext`'s comparisons. But if you have data in
    different languages stored in your database, users of one language
    may find their query results are not as expected if the collation is
    for another language.

-   As of PostgreSQL 9.1, you can attach a `COLLATE` specification to
    `citext` columns or data values. Currently, `citext` operators will
    honor a non-default `COLLATE` specification while comparing
    case-folded strings, but the initial folding to lower case is always
    done according to the database's `LC_CTYPE` setting (that is, as
    though `COLLATE "default"` were given). This may be changed in a
    future release so that both steps follow the input `COLLATE`
    specification.

-   `citext` is not as efficient as `text` because the operator
    functions and the B-tree comparison functions must make copies of
    the data and convert it to lower case for comparisons. Also, only
    `text` can support B-Tree deduplication. However, `citext` is
    slightly more efficient than using `lower` to get case-insensitive
    matching.

-   `citext` doesn't help much if you need data to compare
    case-sensitively in some contexts and case-insensitively in other
    contexts. The standard answer is to use the `text` type and manually
    use the `lower` function when you need to compare
    case-insensitively; this works all right if case-insensitive
    comparison is needed only infrequently. If you need case-insensitive
    behavior most of the time and case-sensitive infrequently, consider
    storing the data as `citext` and explicitly casting the column to
    `text` when you want case-sensitive comparison. In either situation,
    you will need two indexes if you want both types of searches to be
    fast.

-   The schema containing the `citext` operators must be in the current
    `search_path` (typically `public`); if it is not, the normal
    case-sensitive `text` operators will be invoked instead.

-   The approach of lower-casing strings for comparison does not handle
    some Unicode special cases correctly, for example when one
    upper-case letter has two lower-case letter equivalents. Unicode
    distinguishes between *case mapping* and *case folding* for this
    reason. Use nondeterministic collations instead of `citext` to
    handle that correctly.

### F.9.5. Author

David E. Wheeler `<[david@kineticode.com](mailto:david@kineticode.com)>`

Inspired by the original `citext` module by Donald Fraser.

------------------------------------------------------------------------

  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  [Prev](https://www.postgresql.org/docs/btree-gist.html "F.8. btree_gist — GiST operator classes with B-tree behavior")   [Up](https://www.postgresql.org/docs/contrib.html "Appendix F. Additional Supplied Modules and Extensions")   [Next](https://www.postgresql.org/docs/cube.html "F.10. cube — a multi-dimensional cube data type")
  ------------------------------------------------------------------------------------------------------------------------ ------------------------------------------------------------------------------------------------------------- -----------------------------------------------------------------------------------------------------
  F.8. btree_gist --- GiST operator classes with B-tree behavior                                                           [Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")                            F.10. cube --- a multi-dimensional cube data type

  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## F.39. spi --- Server Programming Interface features/examples

[Prev](https://www.postgresql.org/docs/sepgsql.html "F.38. sepgsql — SELinux-, label-based mandatory access control (MAC) security module")
\|
[Up](https://www.postgresql.org/docs/contrib.html "Appendix F. Additional Supplied Modules and Extensions")
\| Appendix F. Additional Supplied Modules and Extensions \|
[Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")
\|
[Next](https://www.postgresql.org/docs/sslinfo.html "F.40. sslinfo — obtain client SSL information")

------------------------------------------------------------------------

## F.39. spi --- Server Programming Interface features/examples

[F.39.1. refint --- Functions for Implementing Referential
Integrity](https://www.postgresql.org/docs/contrib-spi.html#CONTRIB-SPI-%20REFINT)

[F.39.2. autoinc --- Functions for Autoincrementing
Fields](https://www.postgresql.org/docs/contrib-spi.html#CONTRIB-SPI-AUTOINC)

[F.39.3. insert_username --- Functions for Tracking Who Changed a
Table](https://www.postgresql.org/docs/contrib-spi.html#CONTRIB-SPI-INSERT-%20USERNAME)

[F.39.4. moddatetime --- Functions for Tracking Last Modification
Time](https://www.postgresql.org/docs/contrib-spi.html#CONTRIB-SPI-%20MODDATETIME)

The spi module provides several workable examples of using the [Server
Programming
Interface](https://www.postgresql.org/docs/spi.html%20%22Chapter%2045.%20Server%20Programming%20Interface%22)
(SPI) and triggers. While these functions are of some value in their own
right, they are even more useful as examples to modify for your own
purposes. The functions are general enough to be used with any table,
but you have to specify table and field names (as described below) while
creating a trigger.

Each of the groups of functions described below is provided as a
separately- installable extension.

### F.39.1. refint --- Functions for Implementing Referential Integrity

`check_primary_key()` and `check_foreign_key()` are used to check
foreign key constraints. (This functionality is long since superseded by
the built-in foreign key mechanism, of course, but the module is still
useful as an example.)

`check_primary_key()` checks the referencing table. To use, create a
`BEFORE INSERT OR UPDATE` trigger using this function on a table
referencing another table. Specify as the trigger arguments: the
referencing table's column name(s) which form the foreign key, the
referenced table name, and the column names in the referenced table
which form the primary/unique key. To handle multiple foreign keys,
create a trigger for each reference.

`check_foreign_key()` checks the referenced table. To use, create a
`BEFORE DELETE OR UPDATE` trigger using this function on a table
referenced by other table(s). Specify as the trigger arguments: the
number of referencing tables for which the function has to perform
checking, the action if a referencing key is found (`cascade` --- to
delete the referencing row, `restrict` --- to abort transaction if
referencing keys exist, `setnull` --- to set referencing key fields to
null), the triggered table's column names which form the primary/unique
key, then the referencing table name and column names (repeated for as
many referencing tables as were specified by first argument). Note that
the primary/unique key columns should be marked NOT NULL and should have
a unique index.

There are examples in `refint.example`.

### F.39.2. autoinc --- Functions for Autoincrementing Fields

`autoinc()` is a trigger that stores the next value of a sequence into
an integer field. This has some overlap with the built-in "serial
column" feature, but it is not the same: `autoinc()` will override
attempts to substitute a different field value during inserts, and
optionally it can be used to increment the field during updates, too.

To use, create a `BEFORE INSERT` (or optionally
`BEFORE INSERT OR UPDATE`) trigger using this function. Specify two
trigger arguments: the name of the integer column to be modified, and
the name of the sequence object that will supply values. (Actually, you
can specify any number of pairs of such names, if you'd like to update
more than one autoincrementing column.)

There is an example in `autoinc.example`.

### F.39.3. insert_username --- Functions for Tracking Who Changed a Table

`insert_username()` is a trigger that stores the current user's name
into a text field. This can be useful for tracking who last modified a
particular row within a table.

To use, create a `BEFORE INSERT` and/or `UPDATE` trigger using this
function. Specify a single trigger argument: the name of the text column
to be modified.

There is an example in `insert_username.example`.

### F.39.4. moddatetime --- Functions for Tracking Last Modification Time

`moddatetime()` is a trigger that stores the current time into a
`timestamp` field. This can be useful for tracking the last modification
time of a particular row within a table.

To use, create a `BEFORE UPDATE` trigger using this function. Specify a
single trigger argument: the name of the column to be modified. The
column must be of type `timestamp` or `timestamp with time zone`.

There is an example in `moddatetime.example`.

------------------------------------------------------------------------

  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  [Prev](https://www.postgresql.org/docs/sepgsql.html "F.38. sepgsql — SELinux-, label-based mandatory access control (MAC) security module")   [Up](https://www.postgresql.org/docs/contrib.html "Appendix F. Additional Supplied Modules and Extensions")   [Next](https://www.postgresql.org/docs/sslinfo.html "F.40. sslinfo — obtain client SSL information")
  --------------------------------------------------------------------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------------------
  F.38. sepgsql --- SELinux-, label-based mandatory access control (MAC) security module                                                        [Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")                            F.40. sslinfo --- obtain client SSL information

  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## F.12. dict_int --- example full-text search dictionary for integers

[Prev](https://www.postgresql.org/docs/contrib-dblink-build-sql-update.html "dblink_build_sql_update")
\|
[Up](https://www.postgresql.org/docs/contrib.html "Appendix F. Additional Supplied Modules and Extensions")
\| Appendix F. Additional Supplied Modules and Extensions \|
[Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")
\|
[Next](https://www.postgresql.org/docs/dict-xsyn.html "F.13. dict_xsyn — example synonym full-text search dictionary")

------------------------------------------------------------------------

## F.12. dict_int --- example full-text search dictionary for integers

[F.12.1.
Configuration](https://www.postgresql.org/docs/dict-int.html#DICT-%20INT-CONFIG)

[F.12.2.
Usage](https://www.postgresql.org/docs/dict-int.html#DICT-INT-USAGE)

`dict_int` is an example of an add-on dictionary template for full-text
search. The motivation for this example dictionary is to control the
indexing of integers (signed and unsigned), allowing such numbers to be
indexed while preventing excessive growth in the number of unique words,
which greatly affects the performance of searching.

This module is considered "trusted", that is, it can be installed by
non- superusers who have `CREATE` privilege on the current database.

### F.12.1. Configuration

The dictionary accepts three options:

-   The `maxlen` parameter specifies the maximum number of digits
    allowed in an integer word. The default value is 6.

-   The `rejectlong` parameter specifies whether an overlength integer
    should be truncated or ignored. If `rejectlong` is `false` (the
    default), the dictionary returns the first `maxlen` digits of the
    integer. If `rejectlong` is `true`, the dictionary treats an
    overlength integer as a stop word, so that it will not be indexed.
    Note that this also means that such an integer cannot be searched
    for.

-   The `absval` parameter specifies whether leading "`+`" or "`-`"
    signs should be removed from integer words. The default is `false`.
    When `true`, the sign is removed before `maxlen` is applied.

### F.12.2. Usage

Installing the `dict_int` extension creates a text search template
`intdict_template` and a dictionary `intdict` based on it, with the
default parameters. You can alter the parameters, for example

    mydb# ALTER TEXT SEARCH DICTIONARY intdict (MAXLEN = 4, REJECTLONG = true);
    ALTER TEXT SEARCH DICTIONARY

or create new dictionaries based on the template.

To test the dictionary, you can try

    mydb# select ts_lexize('intdict', '12345678');
     ts_lexize
    -----------
     {123456}

but real-world usage will involve including it in a text search
configuration as described in [Chapter
12](https://www.postgresql.org/docs/textsearch.html%20%22Chapter%2012.%20Full%20Text%20Search%22).
That might look like this:

    ALTER TEXT SEARCH CONFIGURATION english
        ALTER MAPPING FOR int, uint WITH intdict;

------------------------------------------------------------------------

  -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  [Prev](https://www.postgresql.org/docs/contrib-dblink-build-sql-update.html "dblink_build_sql_update")   [Up](https://www.postgresql.org/docs/contrib.html "Appendix F. Additional Supplied Modules and Extensions")   [Next](https://www.postgresql.org/docs/dict-xsyn.html "F.13. dict_xsyn — example synonym full-text search dictionary")
  -------------------------------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------------------------------------
  dblink_build_sql_update                                                                                  [Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")                            F.13. dict_xsyn --- example synonym full-text search dictionary

  -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

# envvar 1.0.0

## Synopsis

    # CREATE EXTENSION envvar;
    CREATE EXTENSION

    # SELECT get_env('PGTZ');
     get_env 
    ---------
     UTC
    (1 row)

## Description

This library contains a single PostgreSQL function, `get_env()`, which
returns the value of the specified environment variable on the
PostgreSQL server. The function uses `getenv()` from `<stdlib.h>` to get
the value. Returns `NULL` if the environment variable is not set. If you
need it not to return `NULL`, use `COALESCE()`:

    # SELECT COALESCE(get_env('HOME'), '[unknown]');
     coalesce  
    -----------
     [unknown]

## Author

[David E. Wheeler](https://justatheory.com/)

## Copyright and License

Copyright (c) 2024 David E. Wheeler.

This module is free software; you can redistribute it and/or modify it
under the [PostgreSQL
License](http://www.opensource.org/licenses/postgresql).

Permission to use, copy, modify, and distribute this software and its
documentation for any purpose, without fee, and without a written
agreement is hereby granted, provided that the above copyright notice
and this paragraph and the following two paragraphs appear in all
copies.

In no event shall David E. Wheeler be liable to any party for direct,
indirect, special, incidental, or consequential damages, including lost
profits, arising out of the use of this software and its documentation,
even if David E. Wheeler has been advised of the possibility of such
damage.

David E. Wheeler specifically disclaim any warranties, including, but
not limited to, the implied warranties of merchantability and fitness
for a particular purpose. The software provided hereunder is on an "as
is" basis, and David E. Wheeler have no obligations to provide
maintenance, support, updates, enhancements, or modifications.

## F.16. fuzzystrmatch --- determine string similarities and distance

[Prev](https://www.postgresql.org/docs/file-fdw.html "F.15. file_fdw — access data files in the server's file system")
\|
[Up](https://www.postgresql.org/docs/contrib.html "Appendix F. Additional Supplied Modules and Extensions")
\| Appendix F. Additional Supplied Modules and Extensions \|
[Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")
\|
[Next](https://www.postgresql.org/docs/hstore.html "F.17. hstore — hstore key/value datatype")

------------------------------------------------------------------------

## F.16. fuzzystrmatch --- determine string similarities and distance

[F.16.1.
Soundex](https://www.postgresql.org/docs/fuzzystrmatch.html#FUZZYSTRMATCH-%20SOUNDEX)

[F.16.2. Daitch-Mokotoff
Soundex](https://www.postgresql.org/docs/fuzzystrmatch.html#FUZZYSTRMATCH-%20DAITCH-MOKOTOFF)

[F.16.3.
Levenshtein](https://www.postgresql.org/docs/fuzzystrmatch.html#FUZZYSTRMATCH-%20LEVENSHTEIN)

[F.16.4.
Metaphone](https://www.postgresql.org/docs/fuzzystrmatch.html#FUZZYSTRMATCH-%20METAPHONE)

[F.16.5. Double
Metaphone](https://www.postgresql.org/docs/fuzzystrmatch.html#FUZZYSTRMATCH-%20DOUBLE-METAPHONE)

The `fuzzystrmatch` module provides several functions to determine
similarities and distance between strings.

### Caution

At present, the `soundex`, `metaphone`, `dmetaphone`, and
`dmetaphone_alt` functions do not work well with multibyte encodings
(such as UTF-8). Use `daitch_mokotoff` or `levenshtein` with such data.

This module is considered "trusted", that is, it can be installed by
non- superusers who have `CREATE` privilege on the current database.

### F.16.1. Soundex

The Soundex system is a method of matching similar-sounding names by
converting them to the same code. It was initially used by the United
States Census in 1880, 1900, and 1910. Note that Soundex is not very
useful for non- English names.

The `fuzzystrmatch` module provides two functions for working with
Soundex codes:

    soundex(text) returns text
    difference(text, text) returns int

The `soundex` function converts a string to its Soundex code. The
`difference` function converts two strings to their Soundex codes and
then reports the number of matching code positions. Since Soundex codes
have four characters, the result ranges from zero to four, with zero
being no match and four being an exact match. (Thus, the function is
misnamed --- `similarity` would have been a better name.)

Here are some usage examples:

    SELECT soundex('hello world!');

    SELECT soundex('Anne'), soundex('Ann'), difference('Anne', 'Ann');
    SELECT soundex('Anne'), soundex('Andrew'), difference('Anne', 'Andrew');
    SELECT soundex('Anne'), soundex('Margaret'), difference('Anne', 'Margaret');

    CREATE TABLE s (nm text);

    INSERT INTO s VALUES ('john');
    INSERT INTO s VALUES ('joan');
    INSERT INTO s VALUES ('wobbly');
    INSERT INTO s VALUES ('jack');

    SELECT * FROM s WHERE soundex(nm) = soundex('john');

    SELECT * FROM s WHERE difference(s.nm, 'john') > 2;

### F.16.2. Daitch-Mokotoff Soundex

Like the original Soundex system, Daitch-Mokotoff Soundex matches
similar- sounding names by converting them to the same code. However,
Daitch-Mokotoff Soundex is significantly more useful for non-English
names than the original system. Major improvements over the original
system include:

-   The code is based on the first six meaningful letters rather than
    four.

-   A letter or combination of letters maps into ten possible codes
    rather than seven.

-   Where two consecutive letters have a single sound, they are coded as
    a single number.

-   When a letter or combination of letters may have different sounds,
    multiple codes are emitted to cover all possibilities.

This function generates the Daitch-Mokotoff soundex codes for its input:

    daitch_mokotoff(_source_ text) returns text[]

The result may contain one or more codes depending on how many plausible
pronunciations there are, so it is represented as an array.

Since a Daitch-Mokotoff soundex code consists of only 6 digits,
*`source`* should be preferably a single word or name.

Here are some examples:

    SELECT daitch_mokotoff('George');
     daitch_mokotoff
    -----------------
     {595000}

    SELECT daitch_mokotoff('John');
     daitch_mokotoff
    -----------------
     {160000,460000}

    SELECT daitch_mokotoff('Bierschbach');
                          daitch_mokotoff
    -----------------------------------------------------------
     {794575,794574,794750,794740,745750,745740,747500,747400}

    SELECT daitch_mokotoff('Schwartzenegger');
     daitch_mokotoff
    -----------------
     {479465}

For matching of single names, returned text arrays can be matched
directly using the `&&` operator: any overlap can be considered a match.
A GIN index may be used for efficiency, see [Section
64.4](https://www.postgresql.org/docs/gin.html "64.4. GIN Indexes") and
this example:

    CREATE TABLE s (nm text);
    CREATE INDEX ix_s_dm ON s USING gin (daitch_mokotoff(nm)) WITH (fastupdate = off);

    INSERT INTO s (nm) VALUES
      ('Schwartzenegger'),
      ('John'),
      ('James'),
      ('Steinman'),
      ('Steinmetz');

    SELECT * FROM s WHERE daitch_mokotoff(nm) && daitch_mokotoff('Swartzenegger');
    SELECT * FROM s WHERE daitch_mokotoff(nm) && daitch_mokotoff('Jane');
    SELECT * FROM s WHERE daitch_mokotoff(nm) && daitch_mokotoff('Jens');

For indexing and matching of any number of names in any order, Full Text
Search features can be used. See [Chapter
12](https://www.postgresql.org/docs/textsearch.html "Chapter 12. Full Text Search")
and this example:

    CREATE FUNCTION soundex_tsvector(v_name text) RETURNS tsvector
    BEGIN ATOMIC
      SELECT to_tsvector('simple',
                         string_agg(array_to_string(daitch_mokotoff(n), ' '), ' '))
      FROM regexp_split_to_table(v_name, '\s+') AS n;
    END;

    CREATE FUNCTION soundex_tsquery(v_name text) RETURNS tsquery
    BEGIN ATOMIC
      SELECT string_agg('(' || array_to_string(daitch_mokotoff(n), '|') || ')', '&')::tsquery
      FROM regexp_split_to_table(v_name, '\s+') AS n;
    END;

    CREATE TABLE s (nm text);
    CREATE INDEX ix_s_txt ON s USING gin (soundex_tsvector(nm)) WITH (fastupdate = off);

    INSERT INTO s (nm) VALUES
      ('John Doe'),
      ('Jane Roe'),
      ('Public John Q.'),
      ('George Best'),
      ('John Yamson');

    SELECT * FROM s WHERE soundex_tsvector(nm) @@ soundex_tsquery('john');
    SELECT * FROM s WHERE soundex_tsvector(nm) @@ soundex_tsquery('jane doe');
    SELECT * FROM s WHERE soundex_tsvector(nm) @@ soundex_tsquery('john public');
    SELECT * FROM s WHERE soundex_tsvector(nm) @@ soundex_tsquery('besst, giorgio');
    SELECT * FROM s WHERE soundex_tsvector(nm) @@ soundex_tsquery('Jameson John');

If it is desired to avoid recalculation of soundex codes during index
rechecks, an index on a separate column can be used instead of an index
on an expression. A stored generated column can be used for this; see
[Section
5.4](https://www.postgresql.org/docs/ddl-generated-columns.html%20%225.4.%20Generated%20Columns%22).

### F.16.3. Levenshtein

This function calculates the Levenshtein distance between two strings:

    levenshtein(source text, target text, ins_cost int, del_cost int, sub_cost int) returns int
    levenshtein(source text, target text) returns int
    levenshtein_less_equal(source text, target text, ins_cost int, del_cost int, sub_cost int, max_d int) returns int
    levenshtein_less_equal(source text, target text, max_d int) returns int

Both `source` and `target` can be any non-null string, with a maximum of
255 characters. The cost parameters specify how much to charge for a
character insertion, deletion, or substitution, respectively. You can
omit the cost parameters, as in the second version of the function; in
that case they all default to 1.

`levenshtein_less_equal` is an accelerated version of the Levenshtein
function for use when only small distances are of interest. If the
actual distance is less than or equal to `max_d`, then
`levenshtein_less_equal` returns the correct distance; otherwise it
returns some value greater than `max_d`. If `max_d` is negative then the
behavior is the same as `levenshtein`.

Examples:

    test=# SELECT levenshtein('GUMBO', 'GAMBOL');
     levenshtein
    -------------
               2
    (1 row)

    test=# SELECT levenshtein('GUMBO', 'GAMBOL', 2, 1, 1);
     levenshtein
    -------------
               3
    (1 row)

    test=# SELECT levenshtein_less_equal('extensive', 'exhaustive', 2);
     levenshtein_less_equal
    ------------------------
                          3
    (1 row)

    test=# SELECT levenshtein_less_equal('extensive', 'exhaustive', 4);
     levenshtein_less_equal
    ------------------------
                          4
    (1 row)

### F.16.4. Metaphone

Metaphone, like Soundex, is based on the idea of constructing a
representative code for an input string. Two strings are then deemed
similar if they have the same codes.

This function calculates the metaphone code of an input string:

    metaphone(source text, max_output_length int) returns text

`source` has to be a non-null string with a maximum of 255 characters.
`max_output_length` sets the maximum length of the output metaphone
code; if longer, the output is truncated to this length.

Example:

    test=# SELECT metaphone('GUMBO', 4);
     metaphone
    -----------
     KM
    (1 row)

### F.16.5. Double Metaphone

The Double Metaphone system computes two "sounds like" strings for a
given input string --- a "primary" and an "alternate". In most cases
they are the same, but for non-English names especially they can be a
bit different, depending on pronunciation. These functions compute the
primary and alternate codes:

    dmetaphone(source text) returns text
    dmetaphone_alt(source text) returns text

There is no length limit on the input strings.

Example:

    test=# SELECT dmetaphone('gumbo');
     dmetaphone
    ------------
     KMP
    (1 row)

------------------------------------------------------------------------

  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  [Prev](https://www.postgresql.org/docs/file-fdw.html "F.15. file_fdw — access data files in the server's file system")   [Up](https://www.postgresql.org/docs/contrib.html "Appendix F. Additional Supplied Modules and Extensions")   [Next](https://www.postgresql.org/docs/hstore.html "F.17. hstore — hstore key/value datatype")
  ------------------------------------------------------------------------------------------------------------------------ ------------------------------------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------------
  F.15. file_fdw --- access data files in the server's file system                                                         [Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")                            F.17. hstore --- hstore key/value datatype

  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## F.17. hstore --- hstore key/value datatype

[Prev](https://www.postgresql.org/docs/fuzzystrmatch.html "F.16. fuzzystrmatch — determine string similarities and distance")
\|
[Up](https://www.postgresql.org/docs/contrib.html "Appendix F. Additional Supplied Modules and Extensions")
\| Appendix F. Additional Supplied Modules and Extensions \|
[Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")
\|
[Next](https://www.postgresql.org/docs/intagg.html "F.18. intagg — integer aggregator and enumerator")

------------------------------------------------------------------------

## F.17. hstore --- hstore key/value datatype

[F.17.1. `hstore` External
Representation](https://www.postgresql.org/docs/hstore.html#HSTORE-EXTERNAL-%20REP)

[F.17.2. `hstore` Operators and
Functions](https://www.postgresql.org/docs/hstore.html#HSTORE-OPS-FUNCS)

[F.17.3.
Indexes](https://www.postgresql.org/docs/hstore.html#HSTORE-INDEXES)

[F.17.4.
Examples](https://www.postgresql.org/docs/hstore.html#HSTORE-%20EXAMPLES)

[F.17.5.
Statistics](https://www.postgresql.org/docs/hstore.html#HSTORE-%20STATISTICS)

[F.17.6.
Compatibility](https://www.postgresql.org/docs/hstore.html#HSTORE-%20COMPATIBILITY)

[F.17.7.
Transforms](https://www.postgresql.org/docs/hstore.html#HSTORE-%20TRANSFORMS)

[F.17.8.
Authors](https://www.postgresql.org/docs/hstore.html#HSTORE-AUTHORS)

This module implements the `hstore` data type for storing sets of
key/value pairs within a single PostgreSQL value. This can be useful in
various scenarios, such as rows with many attributes that are rarely
examined, or semi-structured data. Keys and values are simply text
strings.

This module is considered "trusted", that is, it can be installed by
non- superusers who have `CREATE` privilege on the current database.

### F.17.1. `hstore` External Representation

The text representation of an `hstore`, used for input and output,
includes zero or more *`key`* `=>` *`value`* pairs separated by commas.
Some examples:

    k => v
    foo => bar, baz => whatever
    "1-a" => "anything at all"

The order of the pairs is not significant (and may not be reproduced on
output). Whitespace between pairs or around the `=>` sign is ignored.
Double- quote keys and values that include whitespace, commas, `=`s or
`>`s. To include a double quote or a backslash in a key or value, escape
it with a backslash.

Each key in an `hstore` is unique. If you declare an `hstore` with
duplicate keys, only one will be stored in the `hstore` and there is no
guarantee as to which will be kept:

    SELECT 'a=>1,a=>2'::hstore;
      hstore
    ----------
     "a"=>"1"

A value (but not a key) can be an SQL `NULL`. For example:

    key => NULL

The `NULL` keyword is case-insensitive. Double-quote the `NULL` to treat
it as the ordinary string "NULL".

### Note

Keep in mind that the `hstore` text format, when used for input, applies
*before* any required quoting or escaping. If you are passing an
`hstore` literal via a parameter, then no additional processing is
needed. But if you're passing it as a quoted literal constant, then any
single-quote characters and (depending on the setting of the
`standard_conforming_strings` configuration parameter) backslash
characters need to be escaped correctly. See [Section
4.1.2.1](https://www.postgresql.org/docs/sql-syntax-%20lexical.html#SQL-SYNTAX-STRINGS "4.1.2.1. String Constants")
for more on the handling of string constants.

On output, double quotes always surround keys and values, even when it's
not strictly necessary.

### F.17.2. `hstore` Operators and Functions

The operators provided by the `hstore` module are shown in [Table
F.6](https://www.postgresql.org/docs/hstore.html#HSTORE-OP-TABLE%20%22Table%20F.6.%20hstore%20Operators%22),
the functions in [Table
F.7](https://www.postgresql.org/docs/hstore.html#HSTORE-FUNC-TABLE%20%22Table%20F.7.%20hstore%20Functions%22).

**Table F.6.`hstore` Operators**

## Operator Description Example(s)

`hstore` `->` `text` → `text` Returns value associated with given key,
or `NULL` if not present. `'a=>x, b=>y'::hstore -> 'a'` → `x`\
`hstore` `->` `text[]` → `text[]` Returns values associated with given
keys, or `NULL` if not present.
`'a=>x, b=>y, c=>z'::hstore -> ARRAY['c','a']` → `{"z","x"}`\
`hstore` `||` `hstore` → `hstore` Concatenates two `hstore`s.
`'a=>b, c=>d'::hstore || 'c=>x, d=>q'::hstore` →
`"a"=>"b", "c"=>"x", "d"=>"q"`\
`hstore` `?` `text` → `boolean` Does `hstore` contain key?
`'a=>1'::hstore ? 'a'` → `t`\
`hstore` `?&` `text[]` → `boolean` Does `hstore` contain all the
specified keys? `'a=>1,b=>2'::hstore ?& ARRAY['a','b']` → `t`\
`hstore` `?|` `text[]` → `boolean` Does `hstore` contain any of the
specified keys? `'a=>1,b=>2'::hstore ?| ARRAY['b','c']` → `t`\
`hstore` `@>` `hstore` → `boolean` Does left operand contain right?
`'a=>b, b=>1, c=>NULL'::hstore @> 'b=>1'` → `t`\
`hstore` `<@` `hstore` → `boolean` Is left operand contained in right?
`'a=>c'::hstore <@ 'a=>b, b=>1, c=>NULL'` → `f`\
`hstore` `-` `text` → `hstore` Deletes key from left operand.
`'a=>1, b=>2, c=>3'::hstore - 'b'::text` → `"a"=>"1", "c"=>"3"`\
`hstore` `-` `text[]` → `hstore` Deletes keys from left operand.
`'a=>1, b=>2, c=>3'::hstore - ARRAY['a','b']` → `"c"=>"3"`\
`hstore` `-` `hstore` → `hstore` Deletes pairs from left operand that
match pairs in the right operand.
`'a=>1, b=>2, c=>3'::hstore - 'a=>4, b=>2'::hstore` →
`"a"=>"1", "c"=>"3"`\
`anyelement` `#=` `hstore` → `anyelement` Replaces fields in the left
operand (which must be a composite type) with matching values from
`hstore`. `ROW(1,3) #= 'f1=>11'::hstore` → `(11,3)`\
`%%` `hstore` → `text[]` Converts `hstore` to an array of alternating
keys and values. `%% 'a=>foo, b=>bar'::hstore` → `{a,foo,b,bar}`\
`%#` `hstore` → `text[]` Converts `hstore` to a two-dimensional
key/value array. `%# 'a=>foo, b=>bar'::hstore` → `{{a,foo},{b,bar}}`

**Table F.7.`hstore` Functions**

## Function Description Example(s)

`hstore` ( `record` ) → `hstore` Constructs an `hstore` from a record or
row. `hstore(ROW(1,2))` → `"f1"=>"1", "f2"=>"2"`\
`hstore` ( `text[]` ) → `hstore` Constructs an `hstore` from an array,
which may be either a key/value array, or a two-dimensional array.
`hstore(ARRAY['a','1','b','2'])` → `"a"=>"1", "b"=>"2"`
`hstore(ARRAY[['c','3'],['d','4']])` → `"c"=>"3", "d"=>"4"`\
`hstore` ( `text[]`, `text[]` ) → `hstore` Constructs an `hstore` from
separate key and value arrays. `hstore(ARRAY['a','b'], ARRAY['1','2'])`
→ `"a"=>"1", "b"=>"2"`\
`hstore` ( `text`, `text` ) → `hstore` Makes a single-item `hstore`.
`hstore('a', 'b')` → `"a"=>"b"`\
`akeys` ( `hstore` ) → `text[]` Extracts an `hstore`'s keys as an array.
`akeys('a=>1,b=>2')` → `{a,b}`\
`skeys` ( `hstore` ) → `setof text` Extracts an `hstore`'s keys as a
set. `skeys('a=>1,b=>2')` →

    a
    b
      

`avals` ( `hstore` ) → `text[]` Extracts an `hstore`'s values as an
array. `avals('a=>1,b=>2')` → `{1,2}`\
`svals` ( `hstore` ) → `setof text` Extracts an `hstore`'s values as a
set. `svals('a=>1,b=>2')` →

    1
    2
      

`hstore_to_array` ( `hstore` ) → `text[]` Extracts an `hstore`'s keys
and values as an array of alternating keys and values.
`hstore_to_array('a=>1,b=>2')` → `{a,1,b,2}`\
`hstore_to_matrix` ( `hstore` ) → `text[]` Extracts an `hstore`'s keys
and values as a two-dimensional array. `hstore_to_matrix('a=>1,b=>2')` →
`{{a,1},{b,2}}`\
`hstore_to_json` ( `hstore` ) → `json` Converts an `hstore` to a `json`
value, converting all non-null values to JSON strings. This function is
used implicitly when an `hstore` value is cast to `json`.
`hstore_to_json('"a key"=>1, b=>t, c=>null, d=>12345, e=>012345, f=>1.234, g=>2.345e+4')`
→
`{"a key": "1", "b": "t", "c": null, "d": "12345", "e": "012345", "f": "1.234", "g": "2.345e+4"}`\
`hstore_to_jsonb` ( `hstore` ) → `jsonb` Converts an `hstore` to a
`jsonb` value, converting all non-null values to JSON strings. This
function is used implicitly when an `hstore` value is cast to `jsonb`.
`hstore_to_jsonb('"a key"=>1, b=>t, c=>null, d=>12345, e=>012345, f=>1.234, g=>2.345e+4')`
→
`{"a key": "1", "b": "t", "c": null, "d": "12345", "e": "012345", "f": "1.234", "g": "2.345e+4"}`\
`hstore_to_json_loose` ( `hstore` ) → `json` Converts an `hstore` to a
`json` value, but attempts to distinguish numerical and Boolean values
so they are unquoted in the JSON.
`hstore_to_json_loose('"a key"=>1, b=>t, c=>null, d=>12345, e=>012345, f=>1.234, g=>2.345e+4')`
→
`{"a key": 1, "b": true, "c": null, "d": 12345, "e": "012345", "f": 1.234, "g": 2.345e+4}`\
`hstore_to_jsonb_loose` ( `hstore` ) → `jsonb` Converts an `hstore` to a
`jsonb` value, but attempts to distinguish numerical and Boolean values
so they are unquoted in the JSON.
`hstore_to_jsonb_loose('"a key"=>1, b=>t, c=>null, d=>12345, e=>012345, f=>1.234, g=>2.345e+4')`
→
`{"a key": 1, "b": true, "c": null, "d": 12345, "e": "012345", "f": 1.234, "g": 2.345e+4}`\
`slice` ( `hstore`, `text[]` ) → `hstore` Extracts a subset of an
`hstore` containing only the specified keys.
`slice('a=>1,b=>2,c=>3'::hstore, ARRAY['b','c','x'])` →
`"b"=>"2", "c"=>"3"`\
`each` ( `hstore` ) → `setof record` ( *`key`* `text`, *`value`* `text`
) Extracts an `hstore`'s keys and values as a set of records.
`select * from each('a=>1,b=>2')` →

     key | value
    -----+-------
     a   | 1
     b   | 2
      

`exist` ( `hstore`, `text` ) → `boolean` Does `hstore` contain key?
`exist('a=>1', 'a')` → `t`\
`defined` ( `hstore`, `text` ) → `boolean` Does `hstore` contain a
non-`NULL` value for key? `defined('a=>NULL', 'a')` → `f`\
`delete` ( `hstore`, `text` ) → `hstore` Deletes pair with matching key.
`delete('a=>1,b=>2', 'b')` → `"a"=>"1"`\
`delete` ( `hstore`, `text[]` ) → `hstore` Deletes pairs with matching
keys. `delete('a=>1,b=>2,c=>3', ARRAY['a','b'])` → `"c"=>"3"`\
`delete` ( `hstore`, `hstore` ) → `hstore` Deletes pairs matching those
in the second argument. `delete('a=>1,b=>2', 'a=>4,b=>2'::hstore)` →
`"a"=>"1"`\
`populate_record` ( `anyelement`, `hstore` ) → `anyelement` Replaces
fields in the left operand (which must be a composite type) with
matching values from `hstore`.
`populate_record(ROW(1,2), 'f1=>42'::hstore)` → `(42,2)`

In addition to these operators and functions, values of the `hstore`
type can be subscripted, allowing them to act like associative arrays.
Only a single subscript of type `text` can be specified; it is
interpreted as a key and the corresponding value is fetched or stored.
For example,

    CREATE TABLE mytable (h hstore);
    INSERT INTO mytable VALUES ('a=>b, c=>d');
    SELECT h['a'] FROM mytable;
     h
    ---
     b
    (1 row)

    UPDATE mytable SET h['c'] = 'new';
    SELECT h FROM mytable;
              h
    ----------------------
     "a"=>"b", "c"=>"new"
    (1 row)

A subscripted fetch returns `NULL` if the subscript is `NULL` or that
key does not exist in the `hstore`. (Thus, a subscripted fetch is not
greatly different from the `->` operator.) A subscripted update fails if
the subscript is `NULL`; otherwise, it replaces the value for that key,
adding an entry to the `hstore` if the key does not already exist.

### F.17.3. Indexes

`hstore` has GiST and GIN index support for the `@>`, `?`, `?&` and `?|`
operators. For example:

    CREATE INDEX hidx ON testhstore USING GIST (h);

    CREATE INDEX hidx ON testhstore USING GIN (h);

`gist_hstore_ops` GiST opclass approximates a set of key/value pairs as
a bitmap signature. Its optional integer parameter `siglen` determines
the signature length in bytes. The default length is 16 bytes. Valid
values of signature length are between 1 and 2024 bytes. Longer
signatures lead to a more precise search (scanning a smaller fraction of
the index and fewer heap pages), at the cost of a larger index.

Example of creating such an index with a signature length of 32 bytes:

    CREATE INDEX hidx ON testhstore USING GIST (h gist_hstore_ops(siglen=32));

`hstore` also supports `btree` or `hash` indexes for the `=` operator.
This allows `hstore` columns to be declared `UNIQUE`, or to be used in
`GROUP BY`, `ORDER BY` or `DISTINCT` expressions. The sort ordering for
`hstore` values is not particularly useful, but these indexes may be
useful for equivalence lookups. Create indexes for `=` comparisons as
follows:

    CREATE INDEX hidx ON testhstore USING BTREE (h);

    CREATE INDEX hidx ON testhstore USING HASH (h);

### F.17.4. Examples

Add a key, or update an existing key with a new value:

    UPDATE tab SET h['c'] = '3';

Another way to do the same thing is:

    UPDATE tab SET h = h || hstore('c', '3');

If multiple keys are to be added or changed in one operation, the
concatenation approach is more efficient than subscripting:

    UPDATE tab SET h = h || hstore(array['q', 'w'], array['11', '12']);

Delete a key:

    UPDATE tab SET h = delete(h, 'k1');

Convert a `record` to an `hstore`:

    CREATE TABLE test (col1 integer, col2 text, col3 text);
    INSERT INTO test VALUES (123, 'foo', 'bar');

    SELECT hstore(t) FROM test AS t;
                       hstore
    ---------------------------------------------
     "col1"=>"123", "col2"=>"foo", "col3"=>"bar"
    (1 row)

Convert an `hstore` to a predefined `record` type:

    CREATE TABLE test (col1 integer, col2 text, col3 text);

    SELECT * FROM populate_record(null::test,
                                  '"col1"=>"456", "col2"=>"zzz"');
     col1 | col2 | col3
    ------+------+------
      456 | zzz  |
    (1 row)

Modify an existing record using the values from an `hstore`:

    CREATE TABLE test (col1 integer, col2 text, col3 text);
    INSERT INTO test VALUES (123, 'foo', 'bar');

    SELECT (r).* FROM (SELECT t #= '"col3"=>"baz"' AS r FROM test t) s;
     col1 | col2 | col3
    ------+------+------
      123 | foo  | baz
    (1 row)

### F.17.5. Statistics

The `hstore` type, because of its intrinsic liberality, could contain a
lot of different keys. Checking for valid keys is the task of the
application. The following examples demonstrate several techniques for
checking keys and obtaining statistics.

Simple example:

    SELECT * FROM each('aaa=>bq, b=>NULL, ""=>1');

Using a table:

    CREATE TABLE stat AS SELECT (each(h)).key, (each(h)).value FROM testhstore;

Online statistics:

    SELECT key, count(*) FROM
      (SELECT (each(h)).key FROM testhstore) AS stat
      GROUP BY key
      ORDER BY count DESC, key;
        key    | count
    -----------+-------
     line      |   883
     query     |   207
     pos       |   203
     node      |   202
     space     |   197
     status    |   195
     public    |   194
     title     |   190
     org       |   189
    ...................

### F.17.6. Compatibility

As of PostgreSQL 9.0, `hstore` uses a different internal representation
than previous versions. This presents no obstacle for dump/restore
upgrades since the text representation (used in the dump) is unchanged.

In the event of a binary upgrade, upward compatibility is maintained by
having the new code recognize old-format data. This will entail a slight
performance penalty when processing data that has not yet been modified
by the new code. It is possible to force an upgrade of all values in a
table column by doing an `UPDATE` statement as follows:

    UPDATE tablename SET hstorecol = hstorecol || '';

Another way to do it is:

    ALTER TABLE tablename ALTER hstorecol TYPE hstore USING hstorecol || '';

The `ALTER TABLE` method requires an `ACCESS EXCLUSIVE` lock on the
table, but does not result in bloating the table with old row versions.

### F.17.7. Transforms

Additional extensions are available that implement transforms for the
`hstore` type for the languages PL/Perl and PL/Python. The extensions
for PL/Perl are called `hstore_plperl` and `hstore_plperlu`, for trusted
and untrusted PL/Perl. If you install these transforms and specify them
when creating a function, `hstore` values are mapped to Perl hashes. The
extension for PL/Python is called `hstore_plpython3u`. If you use it,
`hstore` values are mapped to Python dictionaries.

### Caution

It is strongly recommended that the transform extensions be installed in
the same schema as `hstore`. Otherwise there are installation-time
security hazards if a transform extension's schema contains objects
defined by a hostile user.

### F.17.8. Authors

Oleg Bartunov `<[oleg@sai.msu.su](mailto:oleg@sai.msu.su)>`, Moscow,
Moscow University, Russia

Teodor Sigaev `<[teodor@sigaev.ru](mailto:teodor@sigaev.ru)>`, Moscow,
Delta- Soft Ltd., Russia

Additional enhancements by Andrew Gierth
`<[andrew@tao11.riddles.org.uk](mailto:andrew@tao11.riddles.org.uk)>`,
United Kingdom

------------------------------------------------------------------------

  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  [Prev](https://www.postgresql.org/docs/fuzzystrmatch.html "F.16. fuzzystrmatch — determine string similarities and distance")   [Up](https://www.postgresql.org/docs/contrib.html "Appendix F. Additional Supplied Modules and Extensions")   [Next](https://www.postgresql.org/docs/intagg.html "F.18. intagg — integer aggregator and enumerator")
  ------------------------------------------------------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------------------------- --------------------------------------------------------------------------------------------------------
  F.16. fuzzystrmatch --- determine string similarities and distance                                                              [Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")                            F.18. intagg --- integer aggregator and enumerator

  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## F.18. intagg --- integer aggregator and enumerator

[Prev](https://www.postgresql.org/docs/hstore.html "F.17. hstore — hstore key/value datatype")
\|
[Up](https://www.postgresql.org/docs/contrib.html "Appendix F. Additional Supplied Modules and Extensions")
\| Appendix F. Additional Supplied Modules and Extensions \|
[Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")
\|
[Next](https://www.postgresql.org/docs/intarray.html "F.19. intarray — manipulate arrays of integers")

------------------------------------------------------------------------

## F.18. intagg --- integer aggregator and enumerator

[F.18.1.
Functions](https://www.postgresql.org/docs/intagg.html#INTAGG-%20FUNCTIONS)

[F.18.2. Sample
Uses](https://www.postgresql.org/docs/intagg.html#INTAGG-%20SAMPLES)

The `intagg` module provides an integer aggregator and an enumerator.
`intagg` is now obsolete, because there are built-in functions that
provide a superset of its capabilities. However, the module is still
provided as a compatibility wrapper around the built-in functions.

### F.18.1. Functions

The aggregator is an aggregate function `int_array_aggregate(integer)`
that produces an integer array containing exactly the integers it is
fed. This is a wrapper around `array_agg`, which does the same thing for
any array type.

The enumerator is a function `int_array_enum(integer[])` that returns
`setof integer`. It is essentially the reverse operation of the
aggregator: given an array of integers, expand it into a set of rows.
This is a wrapper around `unnest`, which does the same thing for any
array type.

### F.18.2. Sample Uses

Many database systems have the notion of a many to many table. Such a
table usually sits between two indexed tables, for example:

    CREATE TABLE left_table  (id INT PRIMARY KEY, ...);
    CREATE TABLE right_table (id INT PRIMARY KEY, ...);
    CREATE TABLE many_to_many(id_left  INT REFERENCES left_table,
                              id_right INT REFERENCES right_table);

It is typically used like this:

    SELECT right_table.*
    FROM right_table JOIN many_to_many ON (right_table.id = many_to_many.id_right)
    WHERE many_to_many.id_left = _item_ ;

This will return all the items in the right hand table for an entry in
the left hand table. This is a very common construct in SQL.

Now, this methodology can be cumbersome with a very large number of
entries in the `many_to_many` table. Often, a join like this would
result in an index scan and a fetch for each right hand entry in the
table for a particular left hand entry. If you have a very dynamic
system, there is not much you can do. However, if you have some data
which is fairly static, you can create a summary table with the
aggregator.

    CREATE TABLE summary AS
      SELECT id_left, int_array_aggregate(id_right) AS rights
      FROM many_to_many
      GROUP BY id_left;

This will create a table with one row per left item, and an array of
right items. Now this is pretty useless without some way of using the
array; that's why there is an array enumerator. You can do

    SELECT id_left, int_array_enum(rights) FROM summary WHERE id_left = _item_ ;

The above query using `int_array_enum` produces the same results as

    SELECT id_left, id_right FROM many_to_many WHERE id_left = _item_ ;

The difference is that the query against the summary table has to get
only one row from the table, whereas the direct query against
`many_to_many` must index scan and fetch a row for each entry.

On one system, an `EXPLAIN` showed a query with a cost of 8488 was
reduced to a cost of 329. The original query was a join involving the
`many_to_many` table, which was replaced by:

    SELECT id_right, count(id_right) FROM
      ( SELECT id_left, int_array_enum(rights) AS id_right
        FROM summary
        JOIN (SELECT id FROM left_table
              WHERE id = _item_) AS lefts
        ON (summary.id_left = lefts.id)
      ) AS list
      GROUP BY id_right
      ORDER BY count DESC;

------------------------------------------------------------------------

  -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  [Prev](https://www.postgresql.org/docs/hstore.html "F.17. hstore — hstore key/value datatype")   [Up](https://www.postgresql.org/docs/contrib.html "Appendix F. Additional Supplied Modules and Extensions")   [Next](https://www.postgresql.org/docs/intarray.html "F.19. intarray — manipulate arrays of integers")
  ------------------------------------------------------------------------------------------------ ------------------------------------------------------------------------------------------------------------- --------------------------------------------------------------------------------------------------------
  F.17. hstore --- hstore key/value datatype                                                       [Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")                            F.19. intarray --- manipulate arrays of integers

  -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## F.19. intarray --- manipulate arrays of integers

[Prev](https://www.postgresql.org/docs/intagg.html "F.18. intagg — integer aggregator and enumerator")
\|
[Up](https://www.postgresql.org/docs/contrib.html "Appendix F. Additional Supplied Modules and Extensions")
\| Appendix F. Additional Supplied Modules and Extensions \|
[Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")
\|
[Next](https://www.postgresql.org/docs/isn.html "F.20. isn — data types for international standard numbers (ISBN, EAN, UPC, etc.)")

------------------------------------------------------------------------

## F.19. intarray --- manipulate arrays of integers

[F.19.1. `intarray` Functions and
Operators](https://www.postgresql.org/docs/intarray.html#INTARRAY-FUNCS-OPS)

[F.19.2. Index
Support](https://www.postgresql.org/docs/intarray.html#INTARRAY-INDEX)

[F.19.3.
Example](https://www.postgresql.org/docs/intarray.html#INTARRAY-%20EXAMPLE)

[F.19.4.
Benchmark](https://www.postgresql.org/docs/intarray.html#INTARRAY-%20BENCHMARK)

[F.19.5.
Authors](https://www.postgresql.org/docs/intarray.html#INTARRAY-%20AUTHORS)

The `intarray` module provides a number of useful functions and
operators for manipulating null-free arrays of integers. There is also
support for indexed searches using some of the operators.

All of these operations will throw an error if a supplied array contains
any NULL elements.

Many of these operations are only sensible for one-dimensional arrays.
Although they will accept input arrays of more dimensions, the data is
treated as though it were a linear array in storage order.

This module is considered "trusted", that is, it can be installed by
non- superusers who have `CREATE` privilege on the current database.

### F.19.1. `intarray` Functions and Operators

The functions provided by the `intarray` module are shown in [Table
F.8](https://www.postgresql.org/docs/intarray.html#INTARRAY-FUNC-TABLE%20%22Table%20F.8.%20intarray%20Functions%22),
the operators in [Table
F.9](https://www.postgresql.org/docs/intarray.html#INTARRAY-OP-TABLE%20%22Table%20F.9.%20intarray%20Operators%22).

**Table F.8.`intarray` Functions**

## Function Description Example(s)

`icount` ( `integer[]` ) → `integer` Returns the number of elements in
the array. `icount('{1,2,3}'::integer[])` → `3`\
`sort` ( `integer[]`, *`dir`* `text` ) → `integer[]` Sorts the array in
either ascending or descending order. *`dir`* must be `asc` or `desc`.
`sort('{1,3,2}'::integer[], 'desc')` → `{3,2,1}`\
`sort` ( `integer[]` ) → `integer[]` `sort_asc` ( `integer[]` ) →
`integer[]` Sorts in ascending order. `sort(array[11,77,44])` →
`{11,44,77}`\
`sort_desc` ( `integer[]` ) → `integer[]` Sorts in descending order.
`sort_desc(array[11,77,44])` → `{77,44,11}`\
`uniq` ( `integer[]` ) → `integer[]` Removes adjacent duplicates. Often
used with `sort` to remove all duplicates.
`uniq('{1,2,2,3,1,1}'::integer[])` → `{1,2,3,1}`
`uniq(sort('{1,2,3,2,1}'::integer[]))` → `{1,2,3}`\
`idx` ( `integer[]`, *`item`* `integer` ) → `integer` Returns index of
the first array element matching *`item`* , or 0 if no match.
`idx(array[11,22,33,22,11], 22)` → `2`\
`subarray` ( `integer[]`, *`start`* `integer`, *`len`* `integer` ) →
`integer[]` Extracts the portion of the array starting at position
*`start`* , with *`len`* elements.
`subarray('{1,2,3,2,1}'::integer[], 2, 3)` → `{2,3,2}`\
`subarray` ( `integer[]`, *`start`* `integer` ) → `integer[]` Extracts
the portion of the array starting at position *`start`*.
`subarray('{1,2,3,2,1}'::integer[], 2)` → `{2,3,2,1}`\
`intset` ( `integer` ) → `integer[]` Makes a single-element array.
`intset(42)` → `{42}`

**Table F.9.`intarray` Operators**

## Operator Description

`integer[]` `&&` `integer[]` → `boolean` Do arrays overlap (have at
least one element in common)?\
`integer[]` `@>` `integer[]` → `boolean` Does left array contain right
array?\
`integer[]` `<@` `integer[]` → `boolean` Is left array contained in
right array?\
`#` `integer[]` → `integer` Returns the number of elements in the
array.\
`integer[]` `#` `integer` → `integer` Returns index of the first array
element matching the right argument, or 0 if no match. (Same as `idx`
function.)\
`integer[]` `+` `integer` → `integer[]` Adds element to end of array.\
`integer[]` `+` `integer[]` → `integer[]` Concatenates the arrays.\
`integer[]` `-` `integer` → `integer[]` Removes entries matching the
right argument from the array.\
`integer[]` `-` `integer[]` → `integer[]` Removes elements of the right
array from the left array.\
`integer[]` `|` `integer` → `integer[]` Computes the union of the
arguments.\
`integer[]` `|` `integer[]` → `integer[]` Computes the union of the
arguments.\
`integer[]` `&` `integer[]` → `integer[]` Computes the intersection of
the arguments.\
`integer[]` `@@` `query_int` → `boolean` Does array satisfy query? (see
below)\
`query_int` `~~` `integer[]` → `boolean` Does array satisfy query?
(commutator of `@@`)

The operators `&&`, `@>` and `<@` are equivalent to PostgreSQL's
built-in operators of the same names, except that they work only on
integer arrays that do not contain nulls, while the built-in operators
work for any array type. This restriction makes them faster than the
built-in operators in many cases.

The `@@` and `~~` operators test whether an array satisfies a *query* ,
which is expressed as a value of a specialized data type `query_int`. A
*query* consists of integer values that are checked against the elements
of the array, possibly combined using the operators `&` (AND), `|` (OR),
and `!` (NOT). Parentheses can be used as needed. For example, the query
`1&(2|3)` matches arrays that contain 1 and also contain either 2 or 3.

### F.19.2. Index Support

`intarray` provides index support for the `&&`, `@>`, and `@@`
operators, as well as regular array equality.

Two parameterized GiST index operator classes are provided:
`gist__int_ops` (used by default) is suitable for small- to medium-size
data sets, while `gist__intbig_ops` uses a larger signature and is more
suitable for indexing large data sets (i.e., columns containing a large
number of distinct array values). The implementation uses an RD-tree
data structure with built-in lossy compression.

`gist__int_ops` approximates an integer set as an array of integer
ranges. Its optional integer parameter `numranges` determines the
maximum number of ranges in one index key. The default value of
`numranges` is 100. Valid values are between 1 and 253. Using larger
arrays as GiST index keys leads to a more precise search (scanning a
smaller fraction of the index and fewer heap pages), at the cost of a
larger index.

`gist__intbig_ops` approximates an integer set as a bitmap signature.
Its optional integer parameter `siglen` determines the signature length
in bytes. The default signature length is 16 bytes. Valid values of
signature length are between 1 and 2024 bytes. Longer signatures lead to
a more precise search (scanning a smaller fraction of the index and
fewer heap pages), at the cost of a larger index.

There is also a non-default GIN operator class `gin__int_ops`, which
supports these operators as well as `<@`.

The choice between GiST and GIN indexing depends on the relative
performance characteristics of GiST and GIN, which are discussed
elsewhere.

### F.19.3. Example

    -- a message can be in one or more “sections”
    CREATE TABLE message (mid INT PRIMARY KEY, sections INT[], ...);

    -- create specialized index with signature length of 32 bytes
    CREATE INDEX message_rdtree_idx ON message USING GIST (sections gist__intbig_ops (siglen = 32));

    -- select messages in section 1 OR 2 - OVERLAP operator
    SELECT message.mid FROM message WHERE message.sections && '{1,2}';

    -- select messages in sections 1 AND 2 - CONTAINS operator
    SELECT message.mid FROM message WHERE message.sections @> '{1,2}';

    -- the same, using QUERY operator
    SELECT message.mid FROM message WHERE message.sections @@ '1&2'::query_int;

### F.19.4. Benchmark

The source directory `contrib/intarray/bench` contains a benchmark test
suite, which can be run against an installed PostgreSQL server. (It also
requires `DBD::Pg` to be installed.) To run:

    cd .../contrib/intarray/bench
    createdb TEST
    psql -c "CREATE EXTENSION intarray" TEST
    ./create_test.pl | psql TEST
    ./bench.pl

The `bench.pl` script has numerous options, which are displayed when it
is run without any arguments.

### F.19.5. Authors

All work was done by Teodor Sigaev
(`<[teodor@sigaev.ru](mailto:teodor@sigaev.ru)>`) and Oleg Bartunov
(`<[oleg@sai.msu.su](mailto:oleg@sai.msu.su)>`). See
<http://www.sai.msu.su/~megera/postgres/gist/> for additional
information. Andrey Oktyabrski did a great work on adding new functions
and operations.

------------------------------------------------------------------------

  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  [Prev](https://www.postgresql.org/docs/intagg.html "F.18. intagg — integer aggregator and enumerator")   [Up](https://www.postgresql.org/docs/contrib.html "Appendix F. Additional Supplied Modules and Extensions")   [Next](https://www.postgresql.org/docs/isn.html "F.20. isn — data types for international standard numbers (ISBN, EAN, UPC, etc.)")
  -------------------------------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------------------------- -------------------------------------------------------------------------------------------------------------------------------------
  F.18. intagg --- integer aggregator and enumerator                                                       [Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")                            F.20. isn --- data types for international standard numbers (ISBN, EAN, UPC, etc.)

  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

# jsonschema

## Synopsis

    try=# -- install the extension
    try=# CREATE EXTENSION jsonschema;
    CREATE EXTENSION

    try=# -- Define a JSON schema
    try=# SELECT '{
       "type": "object",
       "required": [ "name", "email" ],
       "properties": {
         "name": { "type": "string" },
         "age": { "type": "number", "minimum": 0 },
         "email": {"type": "string", "format": "email" }
       }
     }' AS schema \gset

    try=# -- Make sure it's valid
    try=# SELECT jsonschema_is_valid(:'schema'::json);
     jsonschema_is_valid
    ---------------------
     t

    try=# -- Define an object to validate
    try=# SELECT '{
       "name": "Amos Burton",
       "email": "amos@rocinante.ship"
     }' AS person \gset

    try=# -- Validate it against the schema
    try=# SELECT jsonschema_validates(:'person'::json, :'schema'::json);
     jsonschema_validates
    ----------------------
     t

## Description

This extension adds two functions (with varying signatures) to Postgres:
`jsonschema_is_valid()`, which validates a [JSON
Schema](https://json-%20schema.org) against the meta-schemas that define
the [JSON specification](https://json-schema.org/specification), and
`jsonschema_validates()`, which validates a JSON value against a JSON
Schema. It supports the following [specification
drafts](https://json-%20schema.org/specification):

-   [![draft 4
    badge](https://img.shields.io/endpoint?url=https://bowtie.report/badges/rust-boon/compliance/draft4.json)](https://bowtie.report/#/dialects/draft4 "boon draft 4 report")
-   [![draft 6
    badge](https://img.shields.io/endpoint?url=https://bowtie.report/badges/rust-boon/compliance/draft6.json)](https://bowtie.report/#/dialects/draft6 "boon draft 6 report")
-   [![draft 7
    badge](https://img.shields.io/endpoint?url=https://bowtie.report/badges/rust-boon/compliance/draft7.json)](https://bowtie.report/#/dialects/draft7 "boon draft 7 report")
-   [![draft 2019-09
    badge](https://img.shields.io/endpoint?url=https://bowtie.report/badges/rust-boon/compliance/draft2019-09.json)](https://bowtie.report/#/dialects/draft2019-09 "boon draft 2019-09 report")
-   [![draft 2020-12
    badge](https://img.shields.io/endpoint?url=https://bowtie.report/badges/rust-boon/compliance/draft2020-12.json)](https://bowtie.report/#/dialects/draft2020-12 "boon draft 2020-12 report")

## What, Another One?

This is not the first JSON Schema validation extension for Postgres. Why
another one? (Let's just ignore that the work was pretty far along
before learning of the prior art.) What's different about this extension
compared to the prior art?

-   **Full 2020-12 draft compatibility.** The [boon
    crate](https://github.com/santhosh-tekuri/boon/ "boon: JSON Schema (draft 2020-12, draft 2019-09, draft-7, draft-6, draft-4) Validation in Rust")
    used by jsonschema is the only Rust crate that fully supports the
    JSON Schema [2020-12
    draft](https://json-schema.org/draft/2020-12/release-notes "JSON Schema: 2020-12 Release Notes").
-   **Complex Schema Composition.** While the existing extensions nicely
    support simple, single-file JSON Schemas, this extension fully
    supports [multi-file schema
    definition](https://json-schema.org/understanding-json-schema/structuring "JSON Schema: Structuring a complex schema").
    This allows multiple smaller schemas to be composed into larger,
    more complicated schemas.
-   **Performance.** The use of Rust provides the highest level of
    schema validation performance. In a [simple
    test](https://github.com/tembo-io/pg-jsonschema-boon/#benchmark),
    the jsonschema extension validates JSON and JSONB objects in a
    `CHECK` constraint at around 77,000 inserts/second.

For many use cases these features aren't necessary. But once schemas
become more complicated or require more advanced features of the
[2020-12
draft](https://json-schema.org/draft/2020-12/release-notes "JSON Schema: 2020-12 Release Notes"),
give the jsonschema extension a try.

## Schema Composition

Schema composition enables componentized structuring of a schema across
multiple sub-schemas. JSON Schema refers to this pattern as [Structuring
a complex
schema](https://json-schema.org/understanding-json-schema/structuring%20%22JSON%20Schema:%20Structuring%20a%20complex%20schema%22).
The idea is that each schema has a URI in an `$id` property that
identifiers it, and other schemas can reference it.

For example, an address schema defined like so:

    {
      "$id": "https://example.com/schemas/address",
      "type": "object",
      "properties": {
        "street_address": { "type": "string" },
        "city": { "type": "string" },
        "state": { "type": "string" }
      },
      "required": ["street_address", "city", "state"]
    }

Can be referenced in another schema by using the `$ref` keyword. This is
handy to avoid duplication, as in this example:

    {
      "$id": "https://example.com/schemas/customer",
      "type": "object",
      "properties": {
        "first_name": { "type": "string" },
        "last_name": { "type": "string" },
        "shipping_address": { "$ref": "/schemas/address" },
        "billing_address": { "$ref": "/schemas/address" }
      },
      "required": ["first_name", "last_name", "shipping_address", "billing_address"]
    }

Note that `shipping_address` and `billing_address` both reference
`/schemas/address`. This URI is resolved against the schema `$id`,
`https://example.com/schemas/customer`, which is the base URI for the
schema. Relative to that URI, `/schemas/address` becomes
`https://example.com/schemas/address` resolving to the address schema's
`$id`.

The jsonschema extension supports this pattern by allowing multiple
schemas to be passed to its functions. Say we have the above two schemas
loaded into
[psql](https://www.postgresql.org/docs/current/app-psql.html "PostgreSQL Docs: psql")
variables (you can paste these commands into `psql` to refer to in the
example below):

    SELECT '{
      "$id": "https://example.com/schemas/address",
      "type": "object",
      "properties": {
        "street_address": { "type": "string" },
        "city": { "type": "string" },
        "state": { "type": "string" }
      },
      "required": ["street_address", "city", "state"]
    }'  AS addr_schema \gset

    SELECT '{
      "$id": "https://example.com/schemas/customer",
      "type": "object",
      "properties": {
        "first_name": { "type": "string" },
        "last_name": { "type": "string" },
        "shipping_address": { "$ref": "/schemas/address" },
        "billing_address": { "$ref": "/schemas/address" }
      },
      "required": ["first_name", "last_name", "shipping_address", "billing_address"]
    }' AS cust_schema \gset

We validate the customer schema by its ID and passing both schemas to
`jsonschema_is_valid()`:

    SELECT jsonschema_is_valid(
        'https://example.com/schemas/customer',
        :'addr_schema'::json, :'cust_schema'::json
    );
     jsonschema_is_valid
    ---------------------
     t

Any number of schemas can be passed, allowing for arbitrarily complex
schemas. The same is true for validating JSON values against a composed
schema:

    SELECT jsonschema_validates(
        json_build_object(
          'first_name', 'Naomi',
          'last_name', 'Nagata',
          'shipping_address', json_build_object(
            'street_address', '1 Rocinante Way',
            'city', 'Ceres Station',
            'state', 'The Belt'
          ),
          'billing_address', json_build_object(
            'street_address', '2112 Rush Ave',
            'city', 'Londres Nova',
            'state', 'Mars'
          )
        ),
        'https://example.com/schemas/customer',
        :'addr_schema'::json, :'cust_schema'::json
    );
     jsonschema_validates
    ----------------------
     t

Of course, if your build pipeline supports it you can also
[bundle](https://json-schema.org/understanding-json-%20schema/structuring#bundling)
all of the sub-schemas required to compose a schema and then just have
the one, with no need to refer to it by `$id`. For example,

    SELECT '{
      "$id": "https://example.com/schemas/customer",
      "type": "object",
      "properties": {
        "first_name": { "type": "string" },
        "last_name": { "type": "string" },
        "shipping_address": { "$ref": "/schemas/address" },
        "billing_address": { "$ref": "/schemas/address" }
      },
      "required": ["first_name", "last_name", "shipping_address", "billing_address"],
      "$defs": {
        "https://example.com/schemas/address": {
          "$id": "https://example.com/schemas/address",
          "type": "object",
          "properties": {
            "street_address": { "type": "string" },
            "city": { "type": "string" },
            "state": { "type": "string" }
          },
          "required": ["street_address", "city", "state"]
        }
      }
    }' AS cust_schema \gset

Note that the only change to the address schema is tha it has been
embedded in the `$defs` object using its `$id` for the key. With this
schema bundle, we can omit the `id` parameter:

    SELECT jsonschema_is_valid(:'cust_schema'::json);
     jsonschema_is_valid
    ---------------------
     t

    SELECT jsonschema_validates(
        json_build_object(
          'first_name', 'Naomi',
          'last_name', 'Nagata',
          'shipping_address', json_build_object(
            'street_address', '1 Rocinante Way',
            'city', 'Ceres Station',
            'state', 'The Belt'
          ),
          'billing_address', json_build_object(
            'street_address', '2112 Rush Ave',
            'city', 'Londres Nova',
            'state', 'Mars'
          )
        ),
        :'cust_schema'::json
    );
     jsonschema_validates
    ----------------------
     t

## Configuration

The jsonschema extension fully supports all of the drafts of the
[spec](https://json-schema.org/specification), but a schema is not
required to identify its draft (as in the synopsis). When none is
specified, jsonschema defaults to the latest draft, 2020-12. If,
however, you need it to default to some other draft (or ensure
consistency of behavior should a new draft be released and become the
default), set the `jsonschema.default_draft` configuration to your
preferred default. To set the default for the current session to
2019-09, run:

    SET jsonschema.default_draft TO 'V2019';

For a system-wide default, set it in the `postgresql.conf` file:

    jsonschema.default_draft = 'V7'

The supported values are:

-   `V4`: Draft for `http://json-schema.org/draft-04/schema`
-   `V6`: Draft for `http://json-schema.org/draft-06/schema`
-   `V7`: Draft for `http://json-schema.org/draft-07/schema`
-   `V2019`: Draft for `https://json-schema.org/draft/2019-09/schema`
-   `V2020`: Draft for `https://json-schema.org/draft/2020-12/schema`

## Functions

### `jsonschema_is_valid(schema)`

    SELECT jsonschema_is_valid(schema::json);
    SELECT jsonschema_is_valid(schema::jsonb);

**Parameters**

-   `schema`: A JSON Schema in a JSON or JSONB value

This function verifies that a JSON schema is valid against the JSON
Schema spec. Returns true if the schema validates. If `schema` does not
have a [`$schema`
field](https://json-schema.org/draft/2020-12/json-schema-core#name-%20the-schema-keyword),
it will be validated against the draft defined by the
`jsonschema.default_draft` configuration or, if it's not defined, the
latest draft, currently 2020-12.

If `schema` has no [`$id`
field](https://json-schema.org/draft/2020-12/json-%20schema-core#name-the-id-keyword),
the function will refer to it as `file:///schema.json` in error
messages.

Returns false if `schema` is invalid, or does not compile, logging the
reason to at the `INFO` level.

### `jsonschema_is_valid(id, schema)`

    SELECT jsonschema_is_valid(id::text, VARIADIC schema::json);
    SELECT jsonschema_is_valid(id::text, VARIADIC schema::jsonb);

**Parameters**

-   `id`: The ID of the schema to validate
-   `schema`: A list JSON Schemas in JSON or JSONB values

This function verifies that the JSON schema with the [`$id`
field](https://json-schema.org/draft/2020-12/json-schema-core#name-the-id-%20keyword)
corresponding to the `id` parameter is valid against the JSON Schema
spec. Returns true if the schema validates.

In general, each `schema` parameter should have an [`$id`
field](https://json-%20schema.org/draft/2020-12/json-schema-core#name-the-id-keyword),
since that's how they reference each other.

However, if the first `schema` has no [`$id`
field](https://json-%20schema.org/draft/2020-12/json-schema-core#name-the-id-keyword),
the function will assume its ID is `id`. Subsequent `schema` parameters
without IDs will be assigned the concatenation of `id` with an integer
for its position in the variadic list. But don't depend on that!

If any `schema` has no [`$schema`
field](https://json-%20schema.org/draft/2020-12/json-schema-core#name-the-schema-keyword),
it will be validated against the draft defined by the
`jsonschema.default_draft` configuration or, if it's not defined, the
latest draft, currently 2020-12.

Raises an error if any `schema` is `NULL`. Returns `false` if any
`schema` fails to compile, is invalid, none has an [`$id`
field](https://json-%20schema.org/draft/2020-12/json-schema-core#name-the-id-keyword)
matching the `id` parameter. Logs the reason for the failure at the
`INFO` level.

### `jsonschema_validates(data, schema)`

    SELECT jsonschema_validates(data::json,  schema::json);
    SELECT jsonschema_validates(data::jsonb, schema::jsonb);
    SELECT jsonschema_validates(data::json,  schema::jsonb);
    SELECT jsonschema_validates(data::jsonb, schema::json);

**Parameters**

-   `data`: JSON or JSONB data to validate
-   `schema`: A JSON Schema in a JSON or JSONB value

This function validates data in JSON or JSONB against a JSON Schema in
JSON or JSONB. Returns `NULL` if either `data` or `schema` is `NULL`.

If `schema` has no [`$id`
field](https://json-schema.org/draft/2020-12/json-%20schema-core#name-the-id-keyword),
the function will refer to it as `file:///schema.json` in error
messages.

Raises an error if `schema` is invalid or does not compile. Returns
`false` if `data` fails to validate, logging validation errors at the
`INFO` level.

### `jsonschema_validates(data, id, schema)`

    SELECT jsonschema_validates(data::json,  id::text, VARIADIC schema::json);
    SELECT jsonschema_validates(data::jsonb, id::text, VARIADIC schema::jsonb);
    SELECT jsonschema_validates(data::json,  id::text, VARIADIC schema::jsonb);
    SELECT jsonschema_validates(data::jsonb, id::text, VARIADIC schema::json);

**Parameters**

-   `data`: JSON or JSONB data to validate
-   `id`: The ID of the schema to validate
-   `schema`: A list JSON Schemas in JSON or JSONB values

This function validates data in JSON or JSONB against he JSON schema
with the [`$id`
field](https://json-schema.org/draft/2020-12/json-schema-core#name-the-%20id-keyword)
corresponding to the `id` parameter. In general, each `schema` parameter
should have an [`$id`
field](https://json-%20schema.org/draft/2020-12/json-schema-core#name-the-id-keyword),
since that's how they reference each other.

However, if the first `schema` has no [`$id`
field](https://json-%20schema.org/draft/2020-12/json-schema-core#name-the-id-keyword),
the function will assume its ID is `id`. Subsequent `schema` parameters
without IDs will be assigned the concatenation of `id` with an integer
for its position in the variadic list. But don't depend on that!

Raises an error if any`schema` is invalid or does not compile. Returns
`false` if `data` fails to validate, logging validation errors at the
`INFO` level.

### `json_matches_schema(schema, instance)`

    SELECT json_matches_schema(schema::json, instance::json);
    SELECT jsonb_matches_schema(schema::json, instance::jsonb);

These functions correspond to `jsonschema_validates(data, schema)` but
match the API of the
[pg_jsonschema](https://github.com/supabase/pg_jsonschema) extension.

The one other [pg_jsonschema](https://github.com/supabase/pg_jsonschema)
function, `jsonschema_is_valid()`, goes by the same name and has a
compatible signature in this extension.

## Prior Art

-   [pg_jsonschema](https://github.com/supabase/pg_jsonschema): JSON
    Schema Postgres extension written with pgrx + the [jsonschema
    crate](https://docs.rs/jsonschema/latest/jsonschema/); ca. 20-30%
    faster in a [simple
    test](https://github.com/tembo-io/pg-jsonschema-boon/#benchmark).
-   [pgx_json_schema](https://github.com/jefbarn/pgx_json_schema):
    Slightly older JSON Schema Postgres extension written with pgrx +
    the [jsonschema
    crate](https://docs.rs/jsonschema/latest/jsonschema/)
-   [postgres-json-schema](https://github.com/gavinwahl/postgres-json-schema):
    JSON Schema Postgres extension written in PL/pgSQL
-   [is_jsonb_valid](https://github.com/furstenheim/is_jsonb_valid):
    JSON Schema Postgres extension written in C

## Support

This library is stored in a public [GitHub
repository](https://github.com/tembo-io/pg-jsonschema-boon). Feel free
to fork and contribute! Please file bug reports via [GitHub
Issues](https://github.com/tembo-io/pg-jsonschema-boon/issues/).

## Authors

-   [David E. Wheeler](https://justatheory.com/)

## Copyright and License

Copyright (c) 2024 Tembo

Permission is hereby granted, free of charge, to any person obtaining a
copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be included
in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

## F.22. ltree --- hierarchical tree-like data type

[Prev](https://www.postgresql.org/docs/lo.html "F.21. lo — manage large objects")
\|
[Up](https://www.postgresql.org/docs/contrib.html "Appendix F. Additional Supplied Modules and Extensions")
\| Appendix F. Additional Supplied Modules and Extensions \|
[Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")
\|
[Next](https://www.postgresql.org/docs/pageinspect.html "F.23. pageinspect — low-level inspection of database pages")

------------------------------------------------------------------------

## F.22. ltree --- hierarchical tree-like data type

[F.22.1.
Definitions](https://www.postgresql.org/docs/ltree.html#LTREE-%20DEFINITIONS)

[F.22.2. Operators and
Functions](https://www.postgresql.org/docs/ltree.html#LTREE-OPS-FUNCS)

[F.22.3.
Indexes](https://www.postgresql.org/docs/ltree.html#LTREE-INDEXES)

[F.22.4.
Example](https://www.postgresql.org/docs/ltree.html#LTREE-EXAMPLE)

[F.22.5.
Transforms](https://www.postgresql.org/docs/ltree.html#LTREE-%20TRANSFORMS)

[F.22.6.
Authors](https://www.postgresql.org/docs/ltree.html#LTREE-AUTHORS)

This module implements a data type `ltree` for representing labels of
data stored in a hierarchical tree-like structure. Extensive facilities
for searching through label trees are provided.

This module is considered "trusted", that is, it can be installed by
non- superusers who have `CREATE` privilege on the current database.

### F.22.1. Definitions

A *label* is a sequence of alphanumeric characters, underscores, and
hyphens. Valid alphanumeric character ranges are dependent on the
database locale. For example, in C locale, the characters `A-Za-z0-9_-`
are allowed. Labels must be no more than 1000 characters long.

Examples: `42`, `Personal_Services`

A *label path* is a sequence of zero or more labels separated by dots,
for example `L1.L2.L3`, representing a path from the root of a
hierarchical tree to a particular node. The length of a label path
cannot exceed 65535 labels.

Example: `Top.Countries.Europe.Russia`

The `ltree` module provides several data types:

-   `ltree` stores a label path.

-   `lquery` represents a regular-expression-like pattern for matching
    `ltree` values. A simple word matches that label within a path. A
    star symbol (`*`) matches zero or more labels. These can be joined
    with dots to form a pattern that must match the whole label path.
    For example:

        foo         _Match the exact label pathfoo_

    *.foo.* *Match any label path containing the labelfoo* \*.foo *Match
    any label path whose last label isfoo*

Both star symbols and simple words can be quantified to restrict how
many labels they can match:

        *{_n_}        _Match exactly _n_ labels_
    *{_n_ ,}       _Match at least _n_ labels_
    *{_n_ ,_m_}      _Match at least _n_ but not more than _m_ labels_
    *{,_m_}       _Match at most _m_ labels — same as _*{0,_m_}
    foo{_n_ ,_m_}    _Match at least _n_ but not more than _m_ occurrences of foo_
    foo{,}      _Match any number of occurrences offoo, including zero_

In the absence of any explicit quantifier, the default for a star symbol
is to match any number of labels (that is, `{,}`) while the default for
a non-star item is to match exactly once (that is, `{1}`).

There are several modifiers that can be put at the end of a non-star
`lquery` item to make it match more than just the exact match:

        @           _Match case-insensitively, for examplea@ matches A_
    *           _Match any label with this prefix, for examplefoo* matches foobar_
    %           _Match initial underscore-separated words_

The behavior of `%` is a bit complicated. It tries to match words rather
than the entire label. For example `foo_bar%` matches `foo_bar_baz` but
not `foo_barbaz`. If combined with `*`, prefix matching applies to each
word separately, for example `foo_bar%*` matches `foo1_bar2_baz` but not
`foo1_br2_baz`.

Also, you can write several possibly-modified non-star items separated
with `|` (OR) to match any of those items, and you can put `!` (NOT) at
the start of a non-star group to match any label that doesn't match any
of the alternatives. A quantifier, if any, goes at the end of the group;
it means some number of matches for the group as a whole (that is, some
number of labels matching or not matching any of the alternatives).

Here's an annotated example of `lquery`:

        Top.*{0,2}.sport*@.!football|tennis{1,}.Russ*|Spain
    a.  b.     c.      d.                   e.

This query will match any label path that:

    1. begins with the label `Top`

    2. and next has zero to two labels before

    3. a label beginning with the case-insensitive prefix `sport`

    4. then has one or more labels, none of which match `football` nor `tennis`

    5. and then ends with a label beginning with `Russ` or exactly matching `Spain`.

-   `ltxtquery` represents a full-text-search-like pattern for matching
    `ltree` values. An `ltxtquery` value contains words, possibly with
    the modifiers `@`, `*`, `%` at the end; the modifiers have the same
    meanings as in `lquery`. Words can be combined with `&` (AND), `|`
    (OR), `!` (NOT), and parentheses. The key difference from `lquery`
    is that `ltxtquery` matches words without regard to their position
    in the label path.

Here's an example `ltxtquery`:

        Europe & Russia*@ & !Transportation

This will match paths that contain the label `Europe` and any label
beginning with `Russia` (case-insensitive), but not paths containing the
label `Transportation`. The location of these words within the path is
not important. Also, when `%` is used, the word can be matched to any
underscore- separated word within a label, regardless of position.

Note: `ltxtquery` allows whitespace between symbols, but `ltree` and
`lquery` do not.

### F.22.2. Operators and Functions

Type `ltree` has the usual comparison operators `=`, `<>`, `<`, `>`,
`<=`, `>=`. Comparison sorts in the order of a tree traversal, with the
children of a node sorted by label text. In addition, the specialized
operators shown in [Table
F.12](https://www.postgresql.org/docs/ltree.html#LTREE-OP-TABLE%20%22Table%20F.12.%20ltree%20Operators%22)
are available.

**Table F.12.`ltree` Operators**

## Operator Description

`ltree` `@>` `ltree` → `boolean` Is left argument an ancestor of right
(or equal)?\
`ltree` `<@` `ltree` → `boolean` Is left argument a descendant of right
(or equal)?\
`ltree` `~` `lquery` → `boolean` `lquery` `~` `ltree` → `boolean` Does
`ltree` match `lquery`?\
`ltree` `?` `lquery[]` → `boolean` `lquery[]` `?` `ltree` → `boolean`
Does `ltree` match any `lquery` in array?\
`ltree` `@` `ltxtquery` → `boolean` `ltxtquery` `@` `ltree` → `boolean`
Does `ltree` match `ltxtquery`?\
`ltree` `||` `ltree` → `ltree` Concatenates `ltree` paths.\
`ltree` `||` `text` → `ltree` `text` `||` `ltree` → `ltree` Converts
text to `ltree` and concatenates.\
`ltree[]` `@>` `ltree` → `boolean` `ltree` `<@` `ltree[]` → `boolean`
Does array contain an ancestor of `ltree`?\
`ltree[]` `<@` `ltree` → `boolean` `ltree` `@>` `ltree[]` → `boolean`
Does array contain a descendant of `ltree`?\
`ltree[]` `~` `lquery` → `boolean` `lquery` `~` `ltree[]` → `boolean`
Does array contain any path matching `lquery`?\
`ltree[]` `?` `lquery[]` → `boolean` `lquery[]` `?` `ltree[]` →
`boolean` Does `ltree` array contain any path matching any `lquery`?\
`ltree[]` `@` `ltxtquery` → `boolean` `ltxtquery` `@` `ltree[]` →
`boolean` Does array contain any path matching `ltxtquery`?\
`ltree[]` `?@>` `ltree` → `ltree` Returns first array entry that is an
ancestor of `ltree`, or `NULL` if none.\
`ltree[]` `?<@` `ltree` → `ltree` Returns first array entry that is a
descendant of `ltree`, or `NULL` if none.\
`ltree[]` `?~` `lquery` → `ltree` Returns first array entry that matches
`lquery`, or `NULL` if none.\
`ltree[]` `?@` `ltxtquery` → `ltree` Returns first array entry that
matches `ltxtquery`, or `NULL` if none.

The operators `<@`, `@>`, `@` and `~` have analogues `^<@`, `^@>`, `^@`,
`^~`, which are the same except they do not use indexes. These are
useful only for testing purposes.

The available functions are shown in [Table
F.13](https://www.postgresql.org/docs/ltree.html#LTREE-FUNC-TABLE%20%22Table%20F.13.%20ltree%20Functions%22).

**Table F.13.`ltree` Functions**

## Function Description Example(s)

`subltree` ( `ltree`, *`start`* `integer`, *`end`* `integer` ) → `ltree`
Returns subpath of `ltree` from position *`start`* to position *`end`*
-1 (counting from 0). `subltree('Top.Child1.Child2', 1, 2)` → `Child1`\
`subpath` ( `ltree`, *`offset`* `integer`, *`len`* `integer` ) → `ltree`
Returns subpath of `ltree` starting at position *`offset`* , with length
*`len`*. If *`offset`* is negative, subpath starts that far from the end
of the path. If *`len`* is negative, leaves that many labels off the end
of the path. `subpath('Top.Child1.Child2', 0, 2)` → `Top.Child1`\
`subpath` ( `ltree`, *`offset`* `integer` ) → `ltree` Returns subpath of
`ltree` starting at position *`offset`* , extending to end of path. If
*`offset`* is negative, subpath starts that far from the end of the
path. `subpath('Top.Child1.Child2', 1)` → `Child1.Child2`\
`nlevel` ( `ltree` ) → `integer` Returns number of labels in path.
`nlevel('Top.Child1.Child2')` → `3`\
`index` ( *`a`* `ltree`, *`b`* `ltree` ) → `integer` Returns position of
first occurrence of *`b`* in *`a`* , or -1 if not found.
`index('0.1.2.3.5.4.5.6.8.5.6.8', '5.6')` → `6`\
`index` ( *`a`* `ltree`, *`b`* `ltree`, *`offset`* `integer` ) →
`integer` Returns position of first occurrence of *`b`* in *`a`* , or -1
if not found. The search starts at position *`offset`* ; negative
*`offset`* means start *`-offset`* labels from the end of the path.
`index('0.1.2.3.5.4.5.6.8.5.6.8', '5.6', -4)` → `9`\
`text2ltree` ( `text` ) → `ltree` Casts `text` to `ltree`.\
`ltree2text` ( `ltree` ) → `text` Casts `ltree` to `text`.\
`lca` ( `ltree` \[, `ltree` \[, ... \]\] ) → `ltree` Computes longest
common ancestor of paths (up to 8 arguments are supported).
`lca('1.2.3', '1.2.3.4.5.6')` → `1.2`\
`lca` ( `ltree[]` ) → `ltree` Computes longest common ancestor of paths
in array. `lca(array['1.2.3'::ltree,'1.2.3.4'])` → `1.2`

### F.22.3. Indexes

`ltree` supports several types of indexes that can speed up the
indicated operators:

-   B-tree index over `ltree`: `<`, `<=`, `=`, `>=`, `>`

-   Hash index over `ltree`: `=`

-   GiST index over `ltree` (`gist_ltree_ops` opclass): `<`, `<=`, `=`,
    `>=`, `>`, `@>`, `<@`, `@`, `~`, `?`

`gist_ltree_ops` GiST opclass approximates a set of path labels as a
bitmap signature. Its optional integer parameter `siglen` determines the
signature length in bytes. The default signature length is 8 bytes. The
length must be a positive multiple of `int` alignment (4 bytes on most
machines)) up to 2024. Longer signatures lead to a more precise search
(scanning a smaller fraction of the index and fewer heap pages), at the
cost of a larger index.

Example of creating such an index with the default signature length of 8
bytes:

        CREATE INDEX path_gist_idx ON test USING GIST (path);

Example of creating such an index with a signature length of 100 bytes:

        CREATE INDEX path_gist_idx ON test USING GIST (path gist_ltree_ops(siglen=100));

-   GiST index over `ltree[]` (`gist__ltree_ops` opclass):
    `ltree[] <@ ltree`, `ltree @> ltree[]`, `@`, `~`, `?`

`gist__ltree_ops` GiST opclass works similarly to `gist_ltree_ops` and
also takes signature length as a parameter. The default value of
`siglen` in `gist__ltree_ops` is 28 bytes.

Example of creating such an index with the default signature length of
28 bytes:

        CREATE INDEX path_gist_idx ON test USING GIST (array_path);

Example of creating such an index with a signature length of 100 bytes:

        CREATE INDEX path_gist_idx ON test USING GIST (array_path gist__ltree_ops(siglen=100));

Note: This index type is lossy.

### F.22.4. Example

This example uses the following data (also available in file
`contrib/ltree/ltreetest.sql` in the source distribution):

    CREATE TABLE test (path ltree);
    INSERT INTO test VALUES ('Top');
    INSERT INTO test VALUES ('Top.Science');
    INSERT INTO test VALUES ('Top.Science.Astronomy');
    INSERT INTO test VALUES ('Top.Science.Astronomy.Astrophysics');
    INSERT INTO test VALUES ('Top.Science.Astronomy.Cosmology');
    INSERT INTO test VALUES ('Top.Hobbies');
    INSERT INTO test VALUES ('Top.Hobbies.Amateurs_Astronomy');
    INSERT INTO test VALUES ('Top.Collections');
    INSERT INTO test VALUES ('Top.Collections.Pictures');
    INSERT INTO test VALUES ('Top.Collections.Pictures.Astronomy');
    INSERT INTO test VALUES ('Top.Collections.Pictures.Astronomy.Stars');
    INSERT INTO test VALUES ('Top.Collections.Pictures.Astronomy.Galaxies');
    INSERT INTO test VALUES ('Top.Collections.Pictures.Astronomy.Astronauts');
    CREATE INDEX path_gist_idx ON test USING GIST (path);
    CREATE INDEX path_idx ON test USING BTREE (path);
    CREATE INDEX path_hash_idx ON test USING HASH (path);

Now, we have a table `test` populated with data describing the hierarchy
shown below:

                            Top
                         /   |  \
                 Science Hobbies Collections
                     /       |              \
            Astronomy   Amateurs_Astronomy Pictures
               /  \                            |
    Astrophysics  Cosmology                Astronomy
                                            /  |    \
                                     Galaxies Stars Astronauts

We can do inheritance:

    ltreetest=> SELECT path FROM test WHERE path <@ 'Top.Science';
                    path
    ------------------------------------
     Top.Science
     Top.Science.Astronomy
     Top.Science.Astronomy.Astrophysics
     Top.Science.Astronomy.Cosmology
    (4 rows)

Here are some examples of path matching:

    ltreetest=> SELECT path FROM test WHERE path ~ '*.Astronomy.*';
                         path
    -----------------------------------------------
     Top.Science.Astronomy
     Top.Science.Astronomy.Astrophysics
     Top.Science.Astronomy.Cosmology
     Top.Collections.Pictures.Astronomy
     Top.Collections.Pictures.Astronomy.Stars
     Top.Collections.Pictures.Astronomy.Galaxies
     Top.Collections.Pictures.Astronomy.Astronauts
    (7 rows)

    ltreetest=> SELECT path FROM test WHERE path ~ '*.!pictures@.Astronomy.*';
                    path
    ------------------------------------
     Top.Science.Astronomy
     Top.Science.Astronomy.Astrophysics
     Top.Science.Astronomy.Cosmology
    (3 rows)

Here are some examples of full text search:

    ltreetest=> SELECT path FROM test WHERE path @ 'Astro*% & !pictures@';
                    path
    ------------------------------------
     Top.Science.Astronomy
     Top.Science.Astronomy.Astrophysics
     Top.Science.Astronomy.Cosmology
     Top.Hobbies.Amateurs_Astronomy
    (4 rows)

    ltreetest=> SELECT path FROM test WHERE path @ 'Astro* & !pictures@';
                    path
    ------------------------------------
     Top.Science.Astronomy
     Top.Science.Astronomy.Astrophysics
     Top.Science.Astronomy.Cosmology
    (3 rows)

Path construction using functions:

    ltreetest=> SELECT subpath(path,0,2)||'Space'||subpath(path,2) FROM test WHERE path <@ 'Top.Science.Astronomy';
                     ?column?
    ------------------------------------------
     Top.Science.Space.Astronomy
     Top.Science.Space.Astronomy.Astrophysics
     Top.Science.Space.Astronomy.Cosmology
    (3 rows)

We could simplify this by creating an SQL function that inserts a label
at a specified position in a path:

    CREATE FUNCTION ins_label(ltree, int, text) RETURNS ltree
        AS 'select subpath($1,0,$2) || $3 || subpath($1,$2);'
        LANGUAGE SQL IMMUTABLE;

    ltreetest=> SELECT ins_label(path,2,'Space') FROM test WHERE path <@ 'Top.Science.Astronomy';
                    ins_label
    ------------------------------------------
     Top.Science.Space.Astronomy
     Top.Science.Space.Astronomy.Astrophysics
     Top.Science.Space.Astronomy.Cosmology
    (3 rows)

### F.22.5. Transforms

The `ltree_plpython3u` extension implements transforms for the `ltree`
type for PL/Python. If installed and specified when creating a function,
`ltree` values are mapped to Python lists. (The reverse is currently not
supported, however.)

### Caution

It is strongly recommended that the transform extension be installed in
the same schema as `ltree`. Otherwise there are installation-time
security hazards if a transform extension's schema contains objects
defined by a hostile user.

### F.22.6. Authors

All work was done by Teodor Sigaev
(`<[teodor@stack.net](mailto:teodor@stack.net)>`) and Oleg Bartunov
(`<[oleg@sai.msu.su](mailto:oleg@sai.msu.su)>`). See
<http://www.sai.msu.su/~megera/postgres/gist/> for additional
information. Authors would like to thank Eugeny Rodichev for helpful
discussions. Comments and bug reports are welcome.

------------------------------------------------------------------------

  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  [Prev](https://www.postgresql.org/docs/lo.html "F.21. lo — manage large objects")   [Up](https://www.postgresql.org/docs/contrib.html "Appendix F. Additional Supplied Modules and Extensions")   [Next](https://www.postgresql.org/docs/pageinspect.html "F.23. pageinspect — low-level inspection of database pages")
  ----------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------------------------- -----------------------------------------------------------------------------------------------------------------------
  F.21. lo --- manage large objects                                                   [Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")                            F.23. pageinspect --- low-level inspection of database pages

  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

[![Citus
Banner](https://github.com/citusdata/pg_cron/raw/main/github-%20banner.png)](https://www.citusdata.com/)

## What is pg_cron?

pg_cron is a simple cron-based job scheduler for PostgreSQL (10 or
higher) that runs inside the database as an extension. It uses the same
syntax as regular cron, but it allows you to schedule PostgreSQL
commands directly from the database. You can also use '\[1-59\] seconds'
to schedule a job based on an interval.

pg_cron also allows you using '\$' to indicate last day of the month.

    -- Delete old data on Saturday at 3:30am (GMT)
    SELECT cron.schedule('30 3 * * 6', $$DELETE FROM events WHERE event_time < now() - interval '1 week'$$);
     schedule
    ----------
           42

    -- Vacuum every day at 10:00am (GMT)
    SELECT cron.schedule('nightly-vacuum', '0 10 * * *', 'VACUUM');
     schedule
    ----------
           43

    -- Change to vacuum at 3:00am (GMT)
    SELECT cron.schedule('nightly-vacuum', '0 3 * * *', 'VACUUM');
     schedule
    ----------
           43

    -- Stop scheduling jobs
    SELECT cron.unschedule('nightly-vacuum' );
     unschedule 
    ------------
     t

    SELECT cron.unschedule(42);
     unschedule
    ------------
              t

    -- Vacuum every Sunday at 4:00am (GMT) in a database other than the one pg_cron is installed in
    SELECT cron.schedule_in_database('weekly-vacuum', '0 4 * * 0', 'VACUUM', 'some_other_database');
     schedule
    ----------
           44

    -- Call a stored procedure every 5 seconds
    SELECT cron.schedule('process-updates', '5 seconds', 'CALL process_updates()');

    -- Process payroll at 12:00 of the last day of each month
    SELECT cron.schedule('process-payroll', '0 12 $ * *', 'CALL process_payroll()');

pg_cron can run multiple jobs in parallel, but it runs at most one
instance of a job at a time. If a second run is supposed to start before
the first one finishes, then the second run is queued and started as
soon as the first run completes.

The schedule uses the standard cron syntax, in which \* means "run every
time period", and a specific number means "but only at this time":

     ┌───────────── min (0 - 59)
     │ ┌────────────── hour (0 - 23)
     │ │ ┌─────────────── day of month (1 - 31) or last day of the month ($)
     │ │ │ ┌──────────────── month (1 - 12)
     │ │ │ │ ┌───────────────── day of week (0 - 6) (0 to 6 are Sunday to
     │ │ │ │ │                  Saturday, or use names; 7 is also Sunday)
     │ │ │ │ │
     │ │ │ │ │
     * * * * *

An easy way to create a cron schedule is:
[crontab.guru](http://crontab.guru/).

The code in pg_cron that handles parsing and scheduling comes directly
from the cron source code by Paul Vixie, hence the same options are
supported.

## Installing pg_cron

Install on Red Hat, CentOS, Fedora, Amazon Linux with PostgreSQL 16
using [PGDG](https://yum.postgresql.org/repopackages/):

    # Install the pg_cron extension
    sudo yum install -y pg_cron_16

Install on Debian, Ubuntu with PostgreSQL 16 using
[apt.postgresql.org](https://wiki.postgresql.org/wiki/Apt):

    # Install the pg_cron extension
    sudo apt-get -y install postgresql-16-cron

You can also install pg_cron by building it from source:

    git clone https://github.com/citusdata/pg_cron.git
    cd pg_cron
    # Ensure pg_config is in your path, e.g.
    export PATH=/usr/pgsql-16/bin:$PATH
    make && sudo PATH=$PATH make install

## Setting up pg_cron

To start the pg_cron background worker when PostgreSQL starts, you need
to add pg_cron to `shared_preload_libraries` in postgresql.conf. Note
that pg_cron does not run any jobs as a long a server is in [hot
standby](https://www.postgresql.org/docs/current/static/hot-standby.html)
mode, but it automatically starts when the server is promoted.

    # add to postgresql.conf

    # required to load pg_cron background worker on start-up
    shared_preload_libraries = 'pg_cron'

By default, the pg_cron background worker expects its metadata tables to
be created in the "postgres" database. However, you can configure this
by setting the `cron.database_name` configuration parameter in
postgresql.conf.

    # add to postgresql.conf

    # optionally, specify the database in which the pg_cron background worker should run (defaults to postgres)
    cron.database_name = 'postgres'

`pg_cron` may only be installed to one database in a cluster. If you
need to run jobs in multiple databases, use
`cron.schedule_in_database()`.

Previously pg_cron could only use GMT time, but now you can adapt your
time by setting `cron.timezone` in postgresql.conf.

    # add to postgresql.conf

    # optionally, specify the timezone in which the pg_cron background worker should run (defaults to GMT). E.g:
    cron.timezone = 'PRC'

After restarting PostgreSQL, you can create the pg_cron functions and
metadata tables using `CREATE EXTENSION pg_cron`.

    -- run as superuser:
    CREATE EXTENSION pg_cron;

    -- optionally, grant usage to regular users:
    GRANT USAGE ON SCHEMA cron TO marco;

### Ensuring pg_cron can start jobs

**Important** : By default, pg_cron uses libpq to open a new connection
to the local database, which needs to be allowed by
[pg_hba.conf](https://www.postgresql.org/docs/current/static/auth-pg-hba-%20conf.html).
It may be necessary to enable `trust` authentication for connections
coming from localhost in for the user running the cron job, or you can
add the password to a [.pgpass
file](https://www.postgresql.org/docs/current/static/libpq-pgpass.html),
which libpq will use when opening a connection.

You can also use a unix domain socket directory as the hostname and
enable `trust` authentication for local connections in
[pg_hba.conf](https://www.postgresql.org/docs/current/static/auth-pg-hba-%20conf.html),
which is normally safe:

    # Connect via a unix domain socket:
    cron.host = '/tmp'

    # Can also be an empty string to look for the default directory:
    cron.host = ''

Alternatively, pg_cron can be configured to use background workers. In
that case, the number of concurrent jobs is limited by the
`max_worker_processes` setting, so you may need to raise that.

    # Schedule jobs via background workers instead of localhost connections
    cron.use_background_workers = on
    # Increase the number of available background workers from the default of 8
    max_worker_processes = 20

For security, jobs are executed in the database in which the
`cron.schedule` function is called with the same permissions as the
current user. In addition, users are only able to see their own jobs in
the `cron.job` table.

    -- View active jobs
    select * from cron.job;

## Viewing job run details

You can view the status of running and recently completed job runs in
the `cron.job_run_details`:

    select * from cron.job_run_details order by start_time desc limit 5;
    ┌───────┬───────┬─────────┬──────────┬──────────┬───────────────────┬───────────┬──────────────────┬───────────────────────────────┬───────────────────────────────┐
    │ jobid │ runid │ job_pid │ database │ username │      command      │  status   │  return_message  │          start_time           │           end_time            │
    ├───────┼───────┼─────────┼──────────┼──────────┼───────────────────┼───────────┼──────────────────┼───────────────────────────────┼───────────────────────────────┤
    │    10 │  4328 │    2610 │ postgres │ marco    │ select process()  │ succeeded │ SELECT 1         │ 2023-02-07 09:30:00.098164+01 │ 2023-02-07 09:30:00.130729+01 │
    │    10 │  4327 │    2609 │ postgres │ marco    │ select process()  │ succeeded │ SELECT 1         │ 2023-02-07 09:29:00.015168+01 │ 2023-02-07 09:29:00.832308+01 │
    │    10 │  4321 │    2603 │ postgres │ marco    │ select process()  │ succeeded │ SELECT 1         │ 2023-02-07 09:28:00.011965+01 │ 2023-02-07 09:28:01.420901+01 │
    │    10 │  4320 │    2602 │ postgres │ marco    │ select process()  │ failed    │ server restarted │ 2023-02-07 09:27:00.011833+01 │ 2023-02-07 09:27:00.72121+01  │
    │     9 │  4320 │    2602 │ postgres │ marco    │ select do_stuff() │ failed    │ job canceled     │ 2023-02-07 09:26:00.011833+01 │ 2023-02-07 09:26:00.22121+01  │
    └───────┴───────┴─────────┴──────────┴──────────┴───────────────────┴───────────┴──────────────────┴───────────────────────────────┴───────────────────────────────┘
    (10 rows)

The records in `cron.job_run_details` are not cleaned automatically, but
every user that can schedule cron jobs also has permission to delete
their own `cron.job_run_details` records.

Especially when you have jobs that run every few seconds, it can be a
good idea to clean up regularly, which can easily be done using pg_cron
itself:

    -- Delete old cron.job_run_details records of the current user every day at noon
    SELECT  cron.schedule('delete-job-run-details', '0 12 * * *', $$DELETE FROM cron.job_run_details WHERE end_time < now() - interval '7 days'$$);

If you do not want to use `cron.job_run_details` at all, then you can
add `cron.log_run = off` to `postgresql.conf`.

## Extension settings

The pg_cron extension supports the following configuration parameters:

  -----------------------------------------------------------------------------------
  Setting                         Default                 Description
  ------------------------------- ----------------------- ---------------------------
  `cron.database_name`            `postgres`              Database in which the
                                                          pg_cron background worker
                                                          should run.

  `cron.enable_superuser_jobs`    `on`                    Allow jobs to be scheduled
                                                          as superusers.

  `cron.host`                     `localhost`             Hostname to connect to
                                                          postgres.

  `cron.launch_active_jobs`       `on`                    When off, disables all
                                                          active jobs without
                                                          requiring a server restart

  `cron.log_min_messages`         `WARNING`               log_min_messages for the
                                                          launcher bgworker.

  `cron.log_run`                  `on`                    Log all run details in
                                                          the`cron.job_run_details`
                                                          table.

  `cron.log_statement`            `on`                    Log all cron statements
                                                          prior to execution.

  `cron.max_running_jobs`         `32`                    Maximum number of jobs that
                                                          can be running at the same
                                                          time.

  `cron.timezone`                 `GMT`                   Timezone in which the
                                                          pg_cron background worker
                                                          should run.

  `cron.use_background_workers`   `off`                   Use background workers
                                                          instead of client
                                                          connections.
  -----------------------------------------------------------------------------------

### Changing settings

To view setting configurations, run:

    SELECT * FROM pg_settings WHERE name LIKE 'cron.%';

Setting can be changed in the postgresql.conf file or with the below
command:

    ALTER SYSTEM SET cron.<parameter> TO '<value>';

`cron.log_min_messages` and `cron.launch_active_jobs` have a [setting
context](https://www.postgresql.org/docs/current/view-pg-settings.html#VIEW-%20PG-SETTINGS)
of `sighup`. They can be finalized by executing
`SELECT pg_reload_conf();`.

All the other settings have a postmaster context and only take effect
after a server restart.

## Example use cases

Articles showing possible ways of using pg_cron:

-   [Auto-partitioning using
    pg_partman](https://www.citusdata.com/blog/2018/01/24/citus-and-pg-partman-creating-a-scalable-time-series-database-on-postgresql/)
-   [Computing rollups in an analytical
    dashboard](https://www.citusdata.com/blog/2017/12/27/real-time-analytics-dashboards-with-citus/)
-   [Deleting old data,
    vacuum](https://www.citusdata.com/blog/2016/09/09/pgcron-run-periodic-jobs-in-postgres/)
-   [Feeding
    cats](http://bonesmoses.org/2016/09/09/pg-phriday-irrelevant-inclinations/)
-   [Routinely invoking a
    function](https://fluca1978.github.io/2019/05/21/pgcron.html)
-   [Postgres as a cron
    server](https://supabase.io/blog/2021/03/05/postgres-as-a-cron-server)

## Managed services

The following table keeps track of which of the major managed Postgres
services support pg_cron.

  -------------------------------------------------------------------------------------------------------------------
  Service                                                                         Supported
  ------------------------------------------------------------------------------- -----------------------------------
  [Aiven](https://aiven.io/postgresql)                                            ✔️

  [Alibaba Cloud](https://www.alibabacloud.com/help/doc-detail/150355.htm)        ✔️

  [Amazon RDS](https://aws.amazon.com/rds/postgresql/)                            ✔️

  [Azure](https://azure.microsoft.com/en-us/services/postgresql/)                 ✔️

  [Crunchy                                                                        ✔️
  Bridge](https://www.crunchydata.com/products/crunchy-bridge/?ref=producthunt)   

  [DigitalOcean](https://www.digitalocean.com/products/managed-databases/)        ✔️

  [Google Cloud](https://cloud.google.com/sql/postgresql/)                        ✔️

  [Heroku](https://elements.heroku.com/addons/heroku-postgresql)                  ❌

  [Instaclustr](https://instaclustr.com)                                          ✔️

  [Neon](https://neon.tech/docs/extensions/extensions-intro#tooling-admin)        ✔️

  [ScaleGrid](https://scalegrid.io/postgresql.html)                               ✔️

  [Scaleway](https://www.scaleway.com/en/database/)                               ✔️

  [Supabase](https://supabase.io/docs/guides/database)                            ✔️

  [Tembo](https://tembo.io)                                                       ✔️

  [YugabyteDB](https://www.yugabyte.com/)                                         ✔️
  -------------------------------------------------------------------------------------------------------------------

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of
Conduct](https://opensource.microsoft.com/codeofconduct/). For more
information see the [Code of Conduct
FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact
<opencode@microsoft.com> with any additional questions or comments.

# pg_embedding

------------------------------------------------------------------------

**IMPORTANT NOTICE:**

As of Sept 29, 2023, Neon is no longer committing to `pg_embedding`.

Support will remain in place for existing users of the extension, but we
strongly encourage migrating to `pgvector`.

For migration instructions, see [Migrate from pg_embedding to
pgvector](https://neon.tech/docs/extensions/pg_embedding#migrate-from-%20pg_embedding-to-pgvector),
in the *Neon documentation*.

------------------------------------------------------------------------

The `pg_embedding` extension enables the using the Hierarchical
Navigable Small World (HNSW) algorithm for vector similarity search in
PostgreSQL.

This extension is based on
[ivf-hnsw](https://github.com/dbaranchuk/ivf-hnsw) implementation of
HNSW the code for the current state-of-the-art billion-scale nearest
neighbor search system\[1\].

## Using the pg_embedding extension

This section describes how to use the `pg_embedding` extension with a
simple example demonstrating the required statements, syntax, and
options.

For information about migrating from `pgvector` to `pg_embedding`, see
[Migrate from pgvector to
pg_embedding](https://neon.tech/docs/extensions/pg_embedding#migrate-from-%20pgvector-to-pgembedding),
in the *Neon documentation*.

### Usage summary

The statements in this usage summary are described in further detail in
the following sections.

    CREATE EXTENSION embedding;
    CREATE TABLE documents(id integer PRIMARY KEY, embedding real[]);
    INSERT INTO documents(id, embedding) VALUES (1, '{0,1,2}'), (2, '{1,2,3}'),  (3, '{1,1,1}');
    SELECT id FROM documents ORDER BY embedding <-> ARRAY[3,3,3] LIMIT 1;

### Enable the extension

To enable the `pg_embedding` extension, run the following
`CREATE EXTENSION` statement:

    CREATE EXTENSION embedding;

### Create a table for your vector data

To store your vector data, create a table similar to the following:

    CREATE TABLE documents(id INTEGER, embedding REAL[]);

This statement generates a table named `documents` with an `embedding`
column for storing vector data. Your table and vector column names may
differ.

### Insert data

To insert vector data, use an `INSERT` statement similar to the
following:

    INSERT INTO documents(id, embedding) VALUES (1, '{0,1,2}'), (2, '{1,2,3}'),  (3, '{1,1,1}');

## Query

The `pg_embedding` extension supports Euclidean (L2), Cosine, and
Manhattan distance metrics.

Euclidean (L2) distance:

    SELECT id FROM documents ORDER BY embedding <-> array[3,3,3] LIMIT 1;

Cosine distance:

    SELECT id FROM documents ORDER BY embedding <=> array[3,3,3] LIMIT 1;

Manhattan distance:

    SELECT id FROM documents ORDER BY embedding <~> array[3,3,3] LIMIT 1;

where:

-   `SELECT id FROM documents` selects the `id` field from all records
    in the `documents` table.
-   `ORDER BY` sorts the selected records in ascending order based on
    the calculated distances. In other words, records with values closer
    to the `[1.1, 2.2, 3.3]` query vector according to the distance
    metric will be returned first.
-   `<->`, `<=>`, and `<~>` operators define the distance metric, which
    calculates the distance between the query vector and each row of the
    dataset.
-   `LIMIT 1` limits the result set to one record after sorting.

In summary, the query retrieves the ID of the record from the
`documents` table whose value is closest to the `[3,3,3]` query vector
according to the specified distance metric.

### Create an HNSW index

To optimize search behavior, you can add an HNSW index. To create the
HNSW index on your vector column, use a `CREATE INDEX` statement as
shown in the following examples. The `pg_embedding` extension supports
indexes for use with Euclidean, Cosine, and Manhattan distance metrics.

Euclidean (L2) distance index:

    CREATE INDEX ON documents USING hnsw(embedding) WITH (dims=3, m=3, efconstruction=5, efsearch=5);
    SET enable_seqscan = off;
    SELECT id FROM documents ORDER BY embedding <-> array[3,3,3] LIMIT 1;

Cosine distance index:

    CREATE INDEX ON documents USING hnsw(embedding ann_cos_ops) WITH (dims=3, m=3, efconstruction=5, efsearch=5);
    SET enable_seqscan = off;
    SELECT id FROM documents ORDER BY embedding <=> array[3,3,3] LIMIT 1;

Manhattan distance index:

    CREATE INDEX ON documents USING hnsw(embedding ann_manhattan_ops) WITH (dims=3, m=3, efconstruction=5, efsearch=5);
    SET enable_seqscan = off;
    SELECT id FROM documents ORDER BY embedding <~> array[3,3,3] LIMIT 1;

### Tuning the HNSW algorithm

The following options allow you to tune the HNSW algorithm when creating
an index:

-   `dims`: Defines the number of dimensions in your vector data. This
    is a required parameter.
-   `m`: Defines the maximum number of links or "edges" created for each
    node during graph construction. A higher value increases accuracy
    (recall) but also increases the size of the index in memory and
    index construction time.
-   `efconstruction`: Influences the trade-off between index quality and
    construction speed. A high `efconstruction` value creates a higher
    quality graph, enabling more accurate search results, but a higher
    value also means that index construction takes longer.
-   `efsearch`: Influences the trade-off between query accuracy (recall)
    and speed. A higher `efsearch` value increases accuracy at the cost
    of speed. This value should be equal to or larger than `k`, which is
    the number of nearest neighbors you want your search to return
    (defined by the `LIMIT` clause in your `SELECT` query).

In summary, to prioritize search speed over accuracy, use lower values
for `m` and `efsearch`. Conversely, to prioritize accuracy over search
speed, use a higher value for `m` and `efsearch`. A higher
`efconstruction` value enables more accurate search results at the cost
of index build time, which is also affected by the size of your dataset.

## How HNSW search works

HNSW is a graph-based approach to indexing multi-dimensional data. It
constructs a multi-layered graph, where each layer is a subset of the
previous one. During a search, the algorithm navigates through the graph
from the top layer to the bottom to quickly find the nearest neighbor.
An HNSW graph is known for its superior performance in terms of speed
and accuracy.

The search process begins at the topmost layer of the HNSW graph. From
the starting node, the algorithm navigates to the nearest neighbor in
the same layer. The algorithm repeats this step until it can no longer
find neighbors more similar to the query vector.

Using the found node as an entry point, the algorithm moves down to the
next layer in the graph and repeats the process of navigating to the
nearest neighbor. The process of navigating to the nearest neighbor and
moving down a layer is repeated until the algorithm reaches the bottom
layer.

In the bottom layer, the algorithm continues navigating to the nearest
neighbor until it can't find any nodes that are more similar to the
query vector. The current node is then returned as the most similar node
to the query vector.

The key idea behind HNSW is that by starting the search at the top layer
and moving down through each layer, the algorithm can quickly navigate
to the area of the graph that contains the node that is most similar to
the query vector. This makes the search process much faster than if it
had to search through every node in the graph.

## References

-   \[1\] Dmitry Baranchuk, Artem Babenko, Yury Malkov; Proceedings of
    the European Conference on Computer Vision (ECCV), 2018, pp. 202-216
    [link](http://openaccess.thecvf.com/content_ECCV_2018/html/Dmitry_Baranchuk_Revisiting_the_Inverted_ECCV_2018_paper.html)

# pg_later

Execute SQL now and get the results later.

A postgres extension to execute queries asynchronously. Built on
[pgmq](https://github.com/tembo-io/pgmq).

[![Tembo Cloud Try
Free](https://camo.githubusercontent.com/6f93fcf7720687518cc3867ba134167383cac65f015dd33d5764b7c3ebcc8327/68747470733a2f2f74656d626f2e696f2f74727946726565427574746f6e2e737667)](https://cloud.tembo.io/sign-%20up)

[![Static
Badge](https://camo.githubusercontent.com/3bd41ea9a83c55b21bc2c818e9d2c6fa5f85199497bfbc472183c8e050aa438f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25343074656d626f2d636f6d6d756e6974793f6c6f676f3d736c61636b266c6162656c3d736c61636b)](https://join.slack.com/t/tembocommunity/shared_invite/zt-20dtnhcmo-%20pLNV7_Aobi50TdTLpfQ~EQ)
[![PGXN
version](https://camo.githubusercontent.com/8c2b4c50e2511b330fa439f97683c20a467c3513b8b9ecd735eabcca7fa46f6a/68747470733a2f2f62616467652e667572792e696f2f70672f70675f6c617465722e737667)](https://pgxn.org/dist/pg_later/)

## Installation

### Run with docker

    docker run -p 5432:5432 -e POSTGRES_PASSWORD=postgres quay.io/tembo/pglater-pg:latest

If you'd like to build from source, you can follow the instructions in
[CONTRIBUTING.md](https://github.com/tembo-%20io/pg_later/blob/main/CONTRIBUTING.md).

### Using the extension

Initialize the extension's backend:

    CREATE EXTENSION pg_later CASCADE;

    SELECT pglater.init();

Execute a SQL query now:

    select pglater.exec(
      'select * from pg_available_extensions order by name limit 2'
    ) as job_id;


     job_id 
    --------
         1
    (1 row)

Come back at some later time, and retrieve the results by providing the
job id:

    select pglater.fetch_results(1);


     pg_later_results
    --------------------
    {
      "query": "select * from pg_available_extensions order by name limit 2",
      "job_id": 1,
      "result": [
        {
          "name": "adminpack",
          "comment": "administrative functions for PostgreSQL",
          "default_version": "2.1",
          "installed_version": null
        },
        {
          "name": "amcheck",
          "comment": "functions for verifying relation integrity",
          "default_version": "1.3",
          "installed_version": null
        }
      ],
      "status": "success"
    }

[![PGXN
version](https://camo.githubusercontent.com/8ec7ca8e29e84c3b3742d3cfa55e2ec109861756c31133d7da62993c695e2aa6/68747470733a2f2f62616467652e667572792e696f2f70672f70675f706172746d616e2e737667)](https://badge.fury.io/pg/pg_partman)

# PostgreSQL Partition Manager

pg_partman is an extension to create and manage both time-based and
number- based table partition sets. As of version 5.0.1, only built-in,
declarative partitioning is supported and the older trigger-based
methods have been deprecated.

The declarative partitioning built into PostgreSQL provides the commands
to create a partitioned table and its children. pg_partman uses the
built-in declarative features that PostgreSQL provides and builds upon
those with additional features and enhancements to make managing
partitions easier. One key way that pg_partman extends partitioning in
Postgres is by providing a means to automate the child table maintenance
over time (Ex. adding new children, dropping old ones based on a
retention policy). pg_partman also has features to turn an existing
table into a partitioned table or vice versa.

A background worker (BGW) process is included to automatically run
partition maintenance without the need of an external scheduler (cron,
etc) in most cases.

Bug reports & feature requests can be directed to the Issues section on
Github - <https://github.com/pgpartman/pg_partman/issues>

For questions, comments, or if you're just not sure where to post,
please use the Discussions section on Github. Feel free to post here no
matter how minor you may feel your issue or question may be -
<https://github.com/pgpartman/pg_partman/discussions>

## DOCUMENTATION

The following list of files is found in the
[doc](https://github.com/pgpartman/pg_partman/blob/development/doc)
folder of the pg_partman github repository. For installation
instructions, please see the next section of this README.

  -------------------------------------------------------------------------------------------------------------------------------------------------------------
  File                                                                                                                      Description
  ------------------------------------------------------------------------------------------------------------------------- -----------------------------------
  [pg_partman.md](https://github.com/pgpartman/pg_partman/blob/development/doc/pg_partman.md)                               Main reference documentation for
                                                                                                                            pg_partman.

  [pg_partman_howto.md](https://github.com/pgpartman/pg_partman/blob/development/doc/pg_partman_howto.md)                   A How-To guide for general usage of
                                                                                                                            pg_partman. Provides examples for
                                                                                                                            setting up new partition sets and
                                                                                                                            migrating existing tables to
                                                                                                                            partitioned tables.

  [migrate_to_partman.md](https://github.com/pgpartman/pg_partman/blob/development/doc/migrate_to_partman.md)               How to migrate existing partition
                                                                                                                            sets to being managed by
                                                                                                                            pg_partman.

  [migrate_to_declarative.md](https://github.com/pgpartman/pg_partman/blob/development/doc/migrate_to_declarative.md)       How to migrate from trigger-based
                                                                                                                            partitioning to declarative
                                                                                                                            partitioning.

  [pg_partman_5.0.1_upgrade.md](https://github.com/pgpartman/pg_partman/blob/development/doc/pg_partman_5.0.1_upgrade.md)   If pg_partman is being upgraded to
                                                                                                                            version 5.x from any prior version,
                                                                                                                            special considerations may need to
                                                                                                                            be made. Please carefully review
                                                                                                                            this document before performing any
                                                                                                                            upgrades to 5.x or higher.

  [fix_missing_procedures.md](https://github.com/pgpartman/pg_partman/blob/development/doc/fix_missing_procedures.md)       If pg_partman had been installed
                                                                                                                            prior to PostgreSQL 11 and upgraded
                                                                                                                            since then, it may be missing
                                                                                                                            procedures. This document outlines
                                                                                                                            how to restore those procedures and
                                                                                                                            preserve the current configuration.
  -------------------------------------------------------------------------------------------------------------------------------------------------------------

## INSTALLATION

Requirement:

-   PostgreSQL \>= 14

Recommended:

-   [pg_jobmon](https://github.com/omniti-labs/pg_jobmon) (\>=v1.4.0).
    PG Job Monitor will automatically be used if it is installed and
    setup properly.

### From Source

In the directory where you downloaded pg_partman, run

    make install

If you do not want the background worker compiled and just want the
plain PL/PGSQL functions, you can run this instead:

    make NO_BGW=1 install

### Package

I do not personally maintain any OS packages for pg_partman, but several
repository maintainers from the PostgreSQL Development Group (PGDG) have
kindly been maintaining packages for the community. Please check the
[PostgreSQL Downloads](https://www.postgresql.org/download/) page to see
if your OS has a package available

### Setup

The background worker must be loaded on database start by adding the
library to shared_preload_libraries in postgresql.conf

    shared_preload_libraries = 'pg_partman_bgw'     # (change requires restart)

You can also set other control variables for the BGW in postgresql.conf.
"dbname" is required at a minimum for maintenance to run on the given
database(s). These can be added/changed at anytime with a simple reload.
See the documentation for more details. An example with some of them:

    pg_partman_bgw.interval = 3600
    pg_partman_bgw.role = 'keith'
    pg_partman_bgw.dbname = 'keith'

Log into PostgreSQL and run the following commands. Schema is optional
(but recommended) and can be whatever you wish, but it cannot be changed
after installation. If you're using the BGW, the database cluster can be
safely started without having the extension first created in the
configured database(s). You can create the extension at any time and the
BGW will automatically pick up that it exists without restarting the
cluster (as long as shared_preload_libraries was set) and begin running
maintenance as configured.

    CREATE SCHEMA partman;
    CREATE EXTENSION pg_partman SCHEMA partman;

pg_partman does not require a superuser to run nor to be installed (see
[Extension
Files](https://www.postgresql.org/docs/current/extend-%20extensions.html#EXTEND-EXTENSIONS-FILES)
section of upstream docs) . If not using a superuser, it is recommended
that a dedicated role is created for running pg_partman functions and to
be the owner of all partition sets that pg_partman maintains. At a
minimum this role will need the following privileges (assuming
pg_partman is installed to the `partman` schema and that dedicated role
is called `partman_user`):

    CREATE ROLE partman_user WITH LOGIN;
    GRANT ALL ON SCHEMA partman TO partman_user;
    GRANT ALL ON ALL TABLES IN SCHEMA partman TO partman_user;
    GRANT EXECUTE ON ALL FUNCTIONS IN SCHEMA partman TO partman_user;
    GRANT EXECUTE ON ALL PROCEDURES IN SCHEMA partman TO partman_user;
    GRANT ALL ON SCHEMA my_partition_schema TO partman_user;
    GRANT TEMPORARY ON DATABASE mydb to partman_user; -- allow creation of temp tables to move data out of default

If you need the role to also be able to create schemas, you will need to
grant create on the database as well. In general this shouldn't be
required as long as you give the above role CREATE privileges on any
pre-existing schemas that will contain partition sets.

    GRANT CREATE ON DATABASE mydb TO partman_user;

I've received many requests for being able to install this extension on
Amazon RDS. As of PostgreSQL 12.5, RDS has made the pg_partman extension
available. Many thanks to the RDS team for including this extension in
their environment!

<https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/PostgreSQL_Partitions.html>

## UPGRADE

Run "make install" same as above or update your respective packages to
put the new script files and libraries in place. Then run the following
in PostgreSQL itself:

    ALTER EXTENSION pg_partman UPDATE TO '<latest version>';

If you are doing a `pg_dump`/`pg_restore` and you've upgraded pg_partman
in place from previous versions, it is recommended you use the
`--column-inserts` option when dumping and/or restoring pg_partman's
configuration tables. This is due to ordering of the configuration
columns possibly being different (upgrades just add the columns onto the
end, whereas the default of a new install may be different).

If upgrading between any major versions of pg_partman (4.x -\> 5.x,
etc), please carefully read all intervening version notes in the
[CHANGELOG](https://github.com/pgpartman/pg_partman/blob/development/CHANGELOG.md),
especially those notes for the major version. There are often additional
instructions and other important considerations for the updates. Extra
special considerations are needed if you are upgrading to 5+ from any
version less than 5.0.0. Please see
[pg_partman_5.0.1_upgrade.md](https://github.com/pgpartman/pg_partman/blob/development/doc/pg_partman_5.0.1_upgrade.md).

IMPORTANT NOTE: Some updates to pg_partman must drop and recreate its
own database objects. If you are revoking PUBLIC privileges from
functions/procedures, that can be added back to objects that are
recreated as part of an update. If restrictions from PUBLIC use are
desired for pg_partman, it is recommended to install it into its own
schema as shown above and the revoke undesired access to that schema.
Otherwise you may have to add an additional step to your extension
upgrade procedures to revoke PUBLIC access again.

## EXAMPLES

For setting up partitioning with pg_partman on a brand new table, or to
migrate an existing normal table to partitioning, see
[pg_partman_howto.md](https://github.com/pgpartman/pg_partman/blob/development/doc/pg_partman_howto.md).

For migrating a trigger-based partitioned table to declarative
partitioning using pg_partman, see
[migrate_to_declarative.md](https://github.com/pgpartman/pg_partman/blob/development/doc/migrate_to_declarative.md).
Note that if you plan to migrate to pg_partman, you will first have to
migrate to a declarative partitioned table before it can be managed by
pg_partman.

Other documents are also available in the
[doc](https://github.com/pgpartman/pg_partman/blob/development/doc)
folder.

See the [pg_partman.md reference
file](https://github.com/pgpartman/pg_partman/blob/development/doc/pg_partman.md)
in the
[doc](https://github.com/pgpartman/pg_partman/blob/development/doc)
folder for full details on all commands and options for pg_partman.

## TESTING

This extension can use the pgTAP unit testing suite to evaluate if it is
working properly - <http://www.pgtap.org>.

***WARNING: You MUST increase max_locks_per_transaction above the
default value of 64. A value of 128 has worked well so far with existing
tests. This is due to the subpartitioning tests that create/destroy
several hundred tables in a single transaction. If you don't do this,
you risk a cluster crash when running subpartitioning tests.***

See the [README
file](https://github.com/pgpartman/pg_partman/blob/development/test/README_test.md)
contained in the test folder for more information on testing.

# **pg_vectorize: a VectorDB for Postgres**

[![pg_vectorize](https://private-user-%20images.githubusercontent.com/15756360/301332899-34d65cba-065b-485f-84a4-76284e9def19.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDMyODczMDUsIm5iZiI6MTc0MzI4NzAwNSwicGF0aCI6Ii8xNTc1NjM2MC8zMDEzMzI4OTktMzRkNjVjYmEtMDY1Yi00ODVmLTg0YTQtNzYyODRlOWRlZjE5LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAzMjklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMzI5VDIyMjMyNVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWIyNWI3NjUyZTRjNmRiNjQ2ZGNjZjlkMjJlNzU2NTZlMTcwNzZkZWMyM2Q3NGRmMGYwM2RlN2UzZWI0YmNiOGImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.LuBEF4D2SYPz9qSsSEtWp1j_zujODp6mIMqHZdsaFO0)](https://tembo.io)

[![Tembo Cloud Try
Free](https://camo.githubusercontent.com/6f93fcf7720687518cc3867ba134167383cac65f015dd33d5764b7c3ebcc8327/68747470733a2f2f74656d626f2e696f2f74727946726565427574746f6e2e737667)](https://cloud.tembo.io/sign-up)

A Postgres extension that automates the transformation and orchestration
of text to embeddings and provides hooks into the most popular LLMs.
This allows you to do vector search and build LLM applications on
existing data with as little as two function calls.

This project relies heavily on the work by
[pgvector](https://github.com/pgvector/pgvector) for vector similarity
search, [pgmq](https://github.com/tembo-io/pgmq) for orchestration in
background workers, and
[SentenceTransformers](https://huggingface.co/sentence-%20transformers).

------------------------------------------------------------------------

[![Static
Badge](https://camo.githubusercontent.com/3bd41ea9a83c55b21bc2c818e9d2c6fa5f85199497bfbc472183c8e050aa438f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25343074656d626f2d636f6d6d756e6974793f6c6f676f3d736c61636b266c6162656c3d736c61636b)](https://join.slack.com/t/tembocommunity/shared_invite/zt-277pu7chi-%20NHtvHWvLhHwyK0Y5Y6vTPw)
[![PGXN
version](https://camo.githubusercontent.com/97d917f9abea5d56f16e0dba8eeaea887d8bb1f78a6e394a9f439e385fa18278/68747470733a2f2f62616467652e667572792e696f2f70672f766563746f72697a652e737667)](https://pgxn.org/dist/vectorize/)
[![OSSRank](https://camo.githubusercontent.com/8d64425100905ff429ef660c984b688dae1ef9275995312b6e72f4191c6bce93/68747470733a2f2f736869656c64732e696f2f656e64706f696e743f75726c3d68747470733a2f2f6f737372616e6b2e636f6d2f736869656c642f33383135)](https://ossrank.com/p/3815)

pg_vectorize powers the [VectorDB
Stack](https://tembo.io/docs/product/stacks/ai/vectordb) on [Tembo
Cloud](https://cloud.tembo.io/) and is available in all hobby tier
instances.

**API Documentation** : <https://tembo.io/pg_vectorize/>

**Source** : <https://github.com/tembo-io/pg_vectorize>

## Features

-   Workflows for both vector search and RAG
-   Integrations with OpenAI's
    [embeddings](https://platform.openai.com/docs/guides/embeddings) and
    [Text-Generation](https://platform.openai.com/docs/guides/text-generation)
    endpoints and a self-hosted container for running [Hugging Face
    Sentence-Transformers](https://huggingface.co/sentence-transformers)
-   Automated creation of Postgres triggers to keep your embeddings up
    to date
-   High level API - one function to initialize embeddings
    transformations, and another function to search

## Table of Contents

-   Features
-   Table of Contents
-   Installation
-   Vector Search Example
-   RAG Example
-   Updating Embeddings
-   Directly Interact with LLMs
-   Importing Pre-existing Embeddings
-   Creating a Table from Existing Embeddings

## Installation

The fastest way to get started is by running the Tembo docker container
and the vector server with docker compose:

    docker compose up -d

Then connect to Postgres:

    docker compose exec -it postgres psql

Enable the extension and its dependencies

    CREATE EXTENSION vectorize CASCADE;

Install into an existing Postgres instance

If you're installing in an existing Postgres instance, you will need the
following dependencies:

Rust:

-   [pgrx toolchain](https://github.com/pgcentralfoundation/pgrx)

Postgres Extensions:

-   [pg_cron](https://github.com/citusdata/pg_cron) \^1.5
-   [pgmq](https://github.com/tembo-io/pgmq) \^1
-   [pgvector](https://github.com/pgvector/pgvector) \^0.5.0

Then set the following either in postgresql.conf or as a configuration
parameter:

    -- requires restart of Postgres
    alter system set shared_preload_libraries = 'vectorize,pg_cron';
    alter system set cron.database_name = 'postgres';

And if you're running the vector-serve container, set the following url
as a configuration parameter in Postgres. The host may need to change
from `localhost` to something else depending on where you are running
the container.

    alter system set vectorize.embedding_service_url = 'http://localhost:3000/v1';

    SELECT pg_reload_conf();

## Vector Search Example

Text-to-embedding transformation can be done with either Hugging Face's
Sentence-Transformers or OpenAI's embeddings. The following examples use
Hugging Face's Sentence-Transformers. See the project
[documentation](https://tembo.io/pg_vectorize/examples/openai_embeddings/)
for OpenAI examples.

Follow the installation steps if you haven't already.

Setup a products table. Copy from the example data provided by the
extension.

    CREATE TABLE products (LIKE vectorize.example_products INCLUDING ALL);
    INSERT INTO products SELECT * FROM vectorize.example_products;


    SELECT * FROM products limit 2;


     product_id | product_name |                      description                       |        last_updated_at        
    ------------+--------------+--------------------------------------------------------+-------------------------------
              1 | Pencil       | Utensil used for writing and often works best on paper | 2023-07-26 17:20:43.639351-05
              2 | Laptop Stand | Elevated platform for laptops, enhancing ergonomics    | 2023-07-26 17:20:43.639351-05

Create a job to vectorize the products table. We'll specify the tables
primary key (product_id) and the columns that we want to search
(product_name and description).

    SELECT vectorize.table(
        job_name    => 'product_search_hf',
        relation    => 'products',
        primary_key => 'product_id',
        columns     => ARRAY['product_name', 'description'],
        transformer => 'sentence-transformers/all-MiniLM-L6-v2',
        schedule    => 'realtime'
    );

This adds a new column to your table, in our case it is named
`product_search_embeddings`, then populates that data with the
transformed embeddings from the `product_name` and `description`
columns.

Then search,

    SELECT * FROM vectorize.search(
        job_name        => 'product_search_hf',
        query           => 'accessories for mobile devices',
        return_columns  => ARRAY['product_id', 'product_name'],
        num_results     => 3
    );


                                           search_results                                        
    ---------------------------------------------------------------------------------------------
     {"product_id": 13, "product_name": "Phone Charger", "similarity_score": 0.8147814132322894}
     {"product_id": 6, "product_name": "Backpack", "similarity_score": 0.7743061352550308}
     {"product_id": 11, "product_name": "Stylus Pen", "similarity_score": 0.7709902653575383}

## RAG Example

Ask raw text questions of the example `products` dataset and get chat
responses from an OpenAI LLM.

Follow the installation steps if you haven't already.

Set the [OpenAI API
key](https://platform.openai.com/docs/guides/embeddings), this is
required to for use with OpenAI's chat-completion models.

    ALTER SYSTEM SET vectorize.openai_key TO '<your api key>';
    SELECT pg_reload_conf();

Create an example table if it does not already exist.

    CREATE TABLE products (LIKE vectorize.example_products INCLUDING ALL);
    INSERT INTO products SELECT * FROM vectorize.example_products;

Initialize a table for RAG. We'll use an open source Sentence
Transformer to generate embeddings.

Create a new column that we want to use as the context. In this case,
we'll concatenate both `product_name` and `description`.

    ALTER TABLE products
    ADD COLUMN context TEXT GENERATED ALWAYS AS (product_name || ': ' || description) STORED;

Initialize the RAG project. We'll use the
`openai/text-embedding-3-small` model to generate embeddings on our
source documents.

    SELECT vectorize.table(
        job_name    => 'product_chat',
        relation    => 'products',
        primary_key => 'product_id',
        columns     => ARRAY['context'],
        transformer => 'openai/text-embedding-3-small',
        schedule    => 'realtime'
    );

Now we can ask questions of the `products` table and get responses from
the `product_chat` agent using the `openai/gpt-3.5-turbo` generative
model.

    SELECT vectorize.rag(
        job_name    => 'product_chat',
        query       => 'What is a pencil?',
        chat_model  => 'openai/gpt-3.5-turbo'
    ) -> 'chat_response';


    "A pencil is an item that is commonly used for writing and is known to be most effective on paper."

And to use a locally hosted Ollama service, change the `chat_model`
parameter:

    SELECT vectorize.rag(
        job_name    => 'product_chat',
        query       => 'What is a pencil?',
        chat_model  => 'ollama/wizardlm2:7b'
    ) -> 'chat_response';


    " A pencil is a writing instrument that consists of a solid or gelignola wood core, known as the \"lead,\" encased in a cylindrical piece of breakable material (traditionally wood or plastic), which serves as the body of the pencil. The tip of the body is tapered to a point for writing, and it can mark paper with the imprint of the lead. When used on a sheet of paper, the combination of the pencil's lead and the paper creates a visible mark that is distinct from unmarked areas of the paper. Pencils are particularly well-suited for writing on paper, as they allow for precise control over the marks made."

💡 Note that the `-> 'chat_response'` addition selects for that field of
the JSON object output. Removing it will show the full JSON object,
including information on which documents were included in the contextual
prompt.

## Updating Embeddings

When the source text data is updated, how and when the embeddings are
updated is determined by the value set to the `schedule` parameter in
`vectorize.table`.

The default behavior is `schedule => '* * * * *'`, which means the
background worker process checks for changes every minute, and updates
the embeddings accordingly. This method requires setting the
`updated_at_col` value to point to a colum on the table indicating the
time that the input text columns were last changed. `schedule` can be
set to any cron-like value.

Alternatively, `schedule => 'realtime` creates triggers on the source
table and updates embeddings anytime new records are inserted to the
source table or existing records are updated.

Statements below would will result in new embeddings being generated
either immediately (`schedule => 'realtime'`) or within the cron
schedule set in the `schedule` parameter.

    INSERT INTO products (product_id, product_name, description, product_category, price)
    VALUES (12345, 'pizza', 'dish of Italian origin consisting of a flattened disk of bread', 'food', 5.99);

    UPDATE products
    SET description = 'sling made of fabric, rope, or netting, suspended between two or more points, used for swinging, sleeping, or resting'
    WHERE product_name = 'Hammock';

## Directly Interact with LLMs

Sometimes you want more control over the handling of embeddings. For
those situations you can directly call various LLM providers using SQL:

For text generation:

    select vectorize.generate(
      input => 'Tell me the difference between a cat and a dog in 1 sentence',
      model => 'openai/gpt-4o'
    );


                                                     generate                                                  
    -----------------------------------------------------------------------------------------------------------
     Cats are generally more independent and solitary, while dogs tend to be more social and loyal companions.
    (1 row)

And for embedding generation:

    select vectorize.encode(
      input => 'Tell me the difference between a cat and a dog in 1 sentence',
      model => 'openai/text-embedding-3-large'
    );


    {0.0028769304,-0.005826319,-0.0035932811, ...}

## Importing Pre-existing Embeddings

If you have already computed embeddings using a compatible model (e.g.,
using Sentence-Transformers directly), you can import these into
pg_vectorize without recomputation:

    -- First create the vectorize project
    SELECT vectorize.table(
        job_name    => 'my_search',
        relation    => 'my_table',
        primary_key => 'id',
        columns     => ARRAY['content'],
        transformer => 'sentence-transformers/all-MiniLM-L6-v2'
    );

    -- Then import your pre-computed embeddings
    SELECT vectorize.import_embeddings(
        job_name            => 'my_search',
        src_table           => 'my_embeddings_table',
        src_primary_key     => 'id',
        src_embeddings_col  => 'embedding'
    );

The embeddings must match the dimensions of the specified transformer
model. For example, 'sentence-transformers/all-MiniLM-L6-v2' expects
384-dimensional vectors.

## Creating a Table from Existing Embeddings

If you have already computed embeddings using a compatible model, you
can create a new vectorize table directly from them:

    -- Create a vectorize table from existing embeddings
    SELECT vectorize.table_from(
        relation => 'my_table',
        columns => ARRAY['content'],
        job_name => 'my_search',
        primary_key => 'id',
        src_table => 'my_embeddings_table',
        src_primary_key => 'id',
        src_embeddings_col => 'embedding',
        transformer => 'sentence-transformers/all-MiniLM-L6-v2'
    );

The embeddings must match the dimensions of the specified transformer
model. This approach ensures your pre-computed embeddings are properly
imported before any automatic updates are enabled.

## Contributing

We welcome contributions from the community! If you're interested in
contributing to `pg_vectorize`, please check out our [Contributing
Guide](https://github.com/tembo-io/pg_vectorize/blob/main/CONTRIBUTING.md).
Your contributions help make this project better for everyone.

## Community Support

If you encounter any issues or have any questions, feel free to join our
[Tembo Community
Slack](https://join.slack.com/t/tembocommunity/shared_invite/zt-2u3ctm86u-XzcyL76T7o~7Mpnt6KUx1g).
We're here to help!

# Postgres Message Queue (PGMQ)

A lightweight message queue. Like [AWS SQS](https://aws.amazon.com/sqs/)
and [RSMQ](https://github.com/smrchy/rsmq) but on Postgres.

[![Tembo Cloud Try
Free](https://camo.githubusercontent.com/6f93fcf7720687518cc3867ba134167383cac65f015dd33d5764b7c3ebcc8327/68747470733a2f2f74656d626f2e696f2f74727946726565427574746f6e2e737667)](https://cloud.tembo.io/sign-%20up)

[![Static
Badge](https://camo.githubusercontent.com/3bd41ea9a83c55b21bc2c818e9d2c6fa5f85199497bfbc472183c8e050aa438f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25343074656d626f2d636f6d6d756e6974793f6c6f676f3d736c61636b266c6162656c3d736c61636b)](https://join.slack.com/t/tembocommunity/shared_invite/zt-293gc1k0k-3K8z~eKW1SEIfrqEI~5_yw)
[![OSSRank](https://camo.githubusercontent.com/6614312452bcfc810c885157d97e197fe39bbcf10c7a0746049072b9e736b743/68747470733a2f2f736869656c64732e696f2f656e64706f696e743f75726c3d68747470733a2f2f6f737372616e6b2e636f6d2f736869656c642f33383039)](https://ossrank.com/p/3809)
[![PGXN
version](https://camo.githubusercontent.com/10c5359a5d2b810e2f7948edacf05da2f64664899fd41f55a80af35d6aed8ae9/68747470733a2f2f62616467652e667572792e696f2f70672f70676d712e737667)](https://pgxn.org/dist/pgmq/)

**Documentation** : <https://tembo.io/pgmq/>

**Source** : <https://github.com/tembo-io/pgmq>

## Features

-   Lightweight - No background worker or external dependencies, just
    Postgres functions packaged in an extension
-   Guaranteed "exactly once" delivery of messages to a consumer within
    a visibility timeout
-   API parity with [AWS SQS](https://aws.amazon.com/sqs/) and
    [RSMQ](https://github.com/smrchy/rsmq)
-   Messages stay in the queue until explicitly removed
-   Messages can be archived, instead of deleted, for long-term
    retention and replayability

Supported on Postgres 14-17.

## Table of Contents

-   Postgres Message Queue (PGMQ)
    -   Features
    -   Table of Contents
    -   Installation
        -   Updating
    -   Client Libraries
    -   SQL Examples
        -   Creating a queue
        -   Send two messages
        -   Read messages
        -   Pop a message
        -   Archive a message
        -   Delete a message
        -   Drop a queue
    -   Configuration
        -   Partitioned Queues
    -   Visibility Timeout (vt)
    -   Who uses pgmq?
    -   ✨ Contributors

## Installation

The fastest way to get started is by running the Tembo Docker image,
where PGMQ comes pre-installed in Postgres.

    docker run -d --name pgmq-postgres -e POSTGRES_PASSWORD=postgres -p 5432:5432 tembo.docker.scarf.sh/tembo/pg17-pgmq:latest

If you'd like to build from source, you can follow the instructions in
[CONTRIBUTING.md](https://github.com/tembo-io/pgmq/blob/main/CONTRIBUTING.md).

### Updating

To update PGMQ versions, follow the instructions in
[UPDATING.md](https://github.com/tembo-io/pgmq/blob/main/pgmq-%20extension/UPDATING.md).

## Client Libraries

-   [Rust](https://github.com/tembo-io/pgmq/tree/main/pgmq-rs)
-   [Python (only for
    psycopg3)](https://github.com/tembo-io/pgmq/tree/main/tembo-pgmq-python)

Community

-   [Dart](https://github.com/Ofceab-Studio/dart_pgmq)
-   [Go](https://github.com/craigpastro/pgmq-go)
-   [Elixir](https://github.com/v0idpwn/pgmq-elixir)
-   [Elixir + Broadway](https://github.com/v0idpwn/off_broadway_pgmq)
-   [Java (Spring Boot)](https://github.com/adamalexandru4/pgmq-spring)
-   [Kotlin JVM (JDBC)](https://github.com/vdsirotkin/pgmq-kotlin-jvm)
-   [Javascript (NodeJs)](https://github.com/Muhammad-Magdi/pgmq-js)
-   [TypeScript
    (NodeJs](https://github.com/waitingsong/pgmq-js/tree/main/packages/pgmq-js) +
    [Midway.js](https://midwayjs.org/))
-   [TypeScript (Deno)](https://github.com/tmountain/deno-pgmq)
-   [.NET](https://github.com/brianpursley/Npgmq)
-   [Python (with
    SQLAlchemy)](https://github.com/jason810496/pgmq-sqlalchemy)

## SQL Examples

    # Connect to Postgres
    psql postgres://postgres:postgres@0.0.0.0:5432/postgres


    -- create the extension in the "pgmq" schema
    CREATE EXTENSION pgmq;

### Creating a queue

Every queue is its own table in the `pgmq` schema. The table name is the
queue name prefixed with `q_`. For example, `pgmq.q_my_queue` is the
table for the queue `my_queue`.

    -- creates the queue
    SELECT pgmq.create('my_queue');


     create
    -------------

    (1 row)

### Send two messages

    -- messages are sent as JSON
    SELECT * from pgmq.send(
      queue_name  => 'my_queue',
      msg         => '{"foo": "bar1"}'
    );

The message id is returned from the send function.

     send
    -----------
             1
    (1 row)



    -- Optionally provide a delay
    -- this message will be on the queue but unable to be consumed for 5 seconds
    SELECT * from pgmq.send(
      queue_name => 'my_queue',
      msg        => '{"foo": "bar2"}',
      delay      => 5
    );


     send
    -----------
             2
    (1 row)

### Read messages

Read `2` message from the queue. Make them invisible for `30` seconds.
If the messages are not deleted or archived within 30 seconds, they will
become visible again and can be read by another consumer.

    SELECT * FROM pgmq.read(
      queue_name => 'my_queue',
      vt         => 30,
      qty        => 2
    );


     msg_id | read_ct |          enqueued_at          |              vt               |     message
    --------+---------+-------------------------------+-------------------------------+-----------------
          1 |       1 | 2023-08-16 08:37:54.567283-05 | 2023-08-16 08:38:29.989841-05 | {"foo": "bar1"}
          2 |       1 | 2023-08-16 08:37:54.572933-05 | 2023-08-16 08:38:29.989841-05 | {"foo": "bar2"}

If the queue is empty, or if all messages are currently invisible, no
rows will be returned.

    SELECT * FROM pgmq.read(
      queue_name => 'my_queue',
      vt         => 30,
      qty        => 1
    );


     msg_id | read_ct | enqueued_at | vt | message
    --------+---------+-------------+----+---------

### Pop a message

    -- Read a message and immediately delete it from the queue. Returns an empty record if the queue is empty or all messages are invisible.
    SELECT * FROM pgmq.pop('my_queue');


     msg_id | read_ct |          enqueued_at          |              vt               |     message
    --------+---------+-------------------------------+-------------------------------+-----------------
          1 |       1 | 2023-08-16 08:37:54.567283-05 | 2023-08-16 08:38:29.989841-05 | {"foo": "bar1"}

### Archive a message

Archiving a message removes it from the queue and inserts it to the
archive table.

    -- Archive message with msg_id=2.
    SELECT pgmq.archive(
      queue_name => 'my_queue',
      msg_id     => 2
    );


     archive
    --------------
     t
    (1 row)

Or archive several messages in one operation using `msg_ids` (plural)
parameter:

First, send a batch of messages

    SELECT pgmq.send_batch(
      queue_name => 'my_queue',
      msgs       => ARRAY['{"foo": "bar3"}','{"foo": "bar4"}','{"foo": "bar5"}']::jsonb[]
    );


     send_batch 
    ------------
              3
              4
              5
    (3 rows)

Then archive them by using the msg_ids (plural) parameter.

    SELECT pgmq.archive(
      queue_name => 'my_queue',
      msg_ids    => ARRAY[3, 4, 5]
    );


     archive 
    ---------
           3
           4
           5
    (3 rows)

Archive tables can be inspected directly with SQL. Archive tables have
the prefix `a_` in the `pgmq` schema.

    SELECT * FROM pgmq.a_my_queue;


     msg_id | read_ct |          enqueued_at          |          archived_at          |              vt               |     message     
    --------+---------+-------------------------------+-------------------------------+-------------------------------+-----------------
          2 |       0 | 2024-08-06 16:03:41.531556+00 | 2024-08-06 16:03:52.811063+00 | 2024-08-06 16:03:46.532246+00 | {"foo": "bar2"}
          3 |       0 | 2024-08-06 16:03:58.586444+00 | 2024-08-06 16:04:02.85799+00  | 2024-08-06 16:03:58.587272+00 | {"foo": "bar3"}
          4 |       0 | 2024-08-06 16:03:58.586444+00 | 2024-08-06 16:04:02.85799+00  | 2024-08-06 16:03:58.587508+00 | {"foo": "bar4"}
          5 |       0 | 2024-08-06 16:03:58.586444+00 | 2024-08-06 16:04:02.85799+00  | 2024-08-06 16:03:58.587543+00 | {"foo": "bar5"}

### Delete a message

Send another message, so that we can delete it.

    SELECT pgmq.send('my_queue', '{"foo": "bar6"}');


     send
    -----------
            6
    (1 row)

Delete the message with id `6` from the queue named `my_queue`.

    SELECT pgmq.delete('my_queue', 6);


     delete
    -------------
     t
    (1 row)

### Drop a queue

Delete the queue `my_queue`.

    SELECT pgmq.drop_queue('my_queue');


     drop_queue
    -----------------
     t
    (1 row)

## Configuration

### Partitioned Queues

You will need to install
[pg_partman](https://github.com/pgpartman/pg_partman/) if you want to
use `pgmq` partitioned queues.

`pgmq` queue tables can be created as a partitioned table by using
`pgmq.create_partitioned()`.
[pg_partman](https://github.com/pgpartman/pg_partman/) handles all
maintenance of queue tables. This includes creating new partitions and
dropping old partitions.

Partitions behavior is configured at the time queues are created, via
`pgmq.create_partitioned()`. This function has three parameters:

`queue_name: text`: The name of the queue. Queues are Postgres tables
prepended with `q_`. For example, `q_my_queue`. The archive is instead
prefixed by `a_`, for example `a_my_queue`.

`partition_interval: text` - The interval at which partitions are
created. This can be either any valid Postgres `Duration` supported by
pg_partman, or an integer value. When it is a duration, queues are
partitioned by the time at which messages are sent to the table
(`enqueued_at`). A value of `'daily'` would create a new partition each
day. When it is an integer value, queues are partitioned by the
`msg_id`. A value of `'100'` will create a new partition every 100
messages. The value must agree with `retention_interval` (time based or
numeric). The default value is `daily`. For archive table, when interval
is an integer value, then it will be partitioned by `msg_id`. In case of
duration it will be partitioned on `archived_at` unlike queue table.

`retention_interval: text` - The interval for retaining partitions. This
can be either any valid Postgres `Duration` supported by pg_partman, or
an integer value. When it is a duration, partitions containing data
greater than the duration will be dropped. When it is an integer value,
any messages that have a `msg_id` less than
`max(msg_id) - retention_interval` will be dropped. For example, if the
max `msg_id` is 100 and the `retention_interval` is 60, any partitions
with `msg_id` values less than 40 will be dropped. The value must agree
with `partition_interval` (time based or numeric). The default is
`'5 days'`. Note: `retention_interval` does not apply to messages that
have been deleted via `pgmq.delete()` or archived with `pgmq.archive()`.
`pgmq.delete()` removes messages forever and `pgmq.archive()` moves
messages to the corresponding archive table forever (for example,
`a_my_queue`).

In order for automatic partition maintenance to take place, several
settings must be added to the `postgresql.conf` file, which is typically
located in the postgres `DATADIR`. `pg_partman_bgw.interval` in
`postgresql.conf`. Below are the default configuration values set in
Tembo docker images.

Add the following to `postgresql.conf`. Note, changing
`shared_preload_libraries` requires a restart of Postgres.

`pg_partman_bgw.interval` sets the interval at which `pg_partman`
conducts maintenance. This creates new partitions and dropping of
partitions falling out of the `retention_interval`. By default,
`pg_partman` will keep 4 partitions "ahead" of the currently active
partition.

    shared_preload_libraries = 'pg_partman_bgw' # requires restart of Postgres
    pg_partman_bgw.interval = 60
    pg_partman_bgw.role = 'postgres'
    pg_partman_bgw.dbname = 'postgres'

## Visibility Timeout (vt)

pgmq guarantees exactly once delivery of a message within a visibility
timeout. The visibility timeout is the amount of time a message is
invisible to other consumers after it has been read by a consumer. If
the message is NOT deleted or archived within the visibility timeout, it
will become visible again and can be read by another consumer. The
visibility timeout is set when a message is read from the queue, via
`pgmq.read()`. It is recommended to set a `vt` value that is greater
than the expected time it takes to process a message. After the
application successfully processes the message, it should call
`pgmq.delete()` to completely remove the message from the queue or
`pgmq.archive()` to move it to the archive table for the queue.

## Who uses pgmq?

As the pgmq community grows, we'd love to see who is using it. Please
send a PR with your company name and @githubhandle.

Currently, officially using pgmq:

1.  [Tembo](https://tembo.io)
    \[[@ChuckHend](https://github.com/ChuckHend)\]
2.  [Supabase](https://supabase.com)
    \[[@Supabase](https://github.com/supabase)\]
3.  [Sprinters](https://sprinters.sh)
    \[[@sprinters-sh](https://github.com/sprinters-sh)\]

## ✨ Contributors

Thanks goes to these incredible people:

[![](https://camo.githubusercontent.com/89fdd4ddb8e1c2077cfbc3fa52755f2e37a5321046d8109900909151a8c97c30/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d74656d626f2d696f2f70676d71)](https://github.com/tembo-io/pgmq/graphs/contributors)

# PostgreSQL HTTP Client

[![CI](https://github.com/pramsey/pgsql-%20http/workflows/CI/badge.svg)](https://github.com/pramsey/pgsql-http/actions)

## Motivation

Wouldn't it be nice to be able to write a trigger that called a web
service? Either to get back a result, or to poke that service into
refreshing itself against the new state of the database?

This extension is for that.

## Examples

URL encode a string.

    SELECT urlencode('my special string''s & things?');


                  urlencode
    -------------------------------------
     my+special+string%27s+%26+things%3F
    (1 row)

URL encode a JSON associative array.

    SELECT urlencode(jsonb_build_object('name','Colin & James','rate','50%'));


                  urlencode
    -------------------------------------
     name=Colin+%26+James&rate=50%25
    (1 row)

Run a GET request and see the content.

    SELECT content
      FROM http_get('http://httpbun.com/ip');


               content
    -----------------------------
     {"origin":"24.69.186.43"}
    (1 row)

Run a GET request with an Authorization header.

    SELECT content::json->'headers'->>'Authorization'
      FROM http((
              'GET',
               'http://httpbun.com/headers',
               http_headers('Authorization','Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9'),
               NULL,
               NULL
            )::http_request);


                       content
    ----------------------------------------------
     Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9
    (1 row)

Read the `status` and `content_type` fields out of a `http_response`
object.

    SELECT status, content_type
      FROM http_get('http://httpbun.com/');


     status |       content_type
    --------+--------------------------
        200 | text/html; charset=utf-8
    (1 row)

Show all the `http_header` in an `http_response` object.

    SELECT (unnest(headers)).*
      FROM http_get('http://httpbun.com/');


          field       |                      value
    ------------------+--------------------------------------------------
     Server           | nginx
     Date             | Wed, 26 Jul 2023 19:52:51 GMT
     Content-Type     | text/html
     Content-Length   | 162
     Connection       | close
     Location         | https://httpbun.org
     server           | nginx
     date             | Wed, 26 Jul 2023 19:52:51 GMT
     content-type     | text/html
     x-powered-by     | httpbun/3c0dc05883dd9212ac38b04705037d50b02f2596
     content-encoding | gzip

Use the PUT command to send a simple text document to a server.

    SELECT status, content_type, content::json->>'data' AS data
      FROM http_put('http://httpbun.com/put', 'some text', 'text/plain');


     status |   content_type   |   data
    --------+------------------+-----------
        200 | application/json | some text

Use the PATCH command to send a simple JSON document to a server.

    SELECT status, content_type, content::json->>'data' AS data
      FROM http_patch('http://httpbun.com/patch', '{"this":"that"}', 'application/json');


     status |   content_type   |      data
    --------+------------------+------------------
        200 | application/json | '{"this":"that"}'

Use the DELETE command to request resource deletion.

    SELECT status, content_type, content::json->>'url' AS url
      FROM http_delete('http://httpbun.com/delete');


     status |   content_type   |            url
    --------+------------------+---------------------------
        200 | application/json | http://httpbun.com/delete

As a shortcut to send data to a GET request, pass a JSONB data argument.

    SELECT status, content::json->'args' AS args
      FROM http_get('http://httpbun.com/get',
                    jsonb_build_object('myvar','myval','foo','bar'));

To POST to a URL using a data payload instead of parameters embedded in
the URL, encode the data in a JSONB as a data payload.

    SELECT status, content::json->'form' AS form
      FROM http_post('http://httpbun.com/post',
                     jsonb_build_object('myvar','myval','foo','bar'));

To access binary content, you must coerce the content from the default
`varchar` representation to a `bytea` representation using the
`text_to_bytea()` function, or the `textsend()` function. Using the
default `varchar::bytea` cast will **not work** , as the cast will stop
the first time it hits a zero-valued byte (common in binary data).

    WITH
      http AS (
        SELECT * FROM http_get('https://httpbingo.org/image/png')
      ),
      headers AS (
        SELECT (unnest(headers)).* FROM http
      )
    SELECT
      http.content_type,
      length(text_to_bytea(http.content)) AS length_binary
    FROM http, headers
    WHERE field ilike 'Content-Type';


     content_type | length_binary
    --------------+---------------
     image/png    |          8090

Similarly, when using POST to send `bytea` binary content to a service,
use the `bytea_to_text` function to prepare the content.

To access only the headers you can do a HEAD-Request. This will not
follow redirections.

    SELECT
        http.status,
        headers.value AS location
    FROM
        http_head('http://google.com') AS http
        LEFT OUTER JOIN LATERAL (SELECT value
            FROM unnest(http.headers)
            WHERE field = 'Location') AS headers
            ON true;


     status |        location
    --------+------------------------
        301 | http://www.google.com/

## Concepts

Every HTTP call is a made up of an `http_request` and an
`http_response`.

         Composite type "public.http_request"
        Column    |       Type        | Modifiers
    --------------+-------------------+-----------
     method       | http_method       |
     uri          | character varying |
     headers      | http_header[]     |
     content_type | character varying |
     content      | character varying |

        Composite type "public.http_response"
        Column    |       Type        | Modifiers
    --------------+-------------------+-----------
     status       | integer           |
     content_type | character varying |
     headers      | http_header[]     |
     content      | character varying |

The utility functions, `http_get()`, `http_post()`, `http_put()`,
`http_delete()` and `http_head()` are just wrappers around a master
function, `http(http_request)` that returns `http_response`.

The `headers` field for requests and response is a PostgreSQL array of
type `http_header` which is just a simple tuple.

      Composite type "public.http_header"
     Column |       Type        | Modifiers
    --------+-------------------+-----------
     field  | character varying |
     value  | character varying |

As seen in the examples, you can unspool the array of `http_header`
tuples into a result set using the PostgreSQL `unnest()` function on the
array. From there you select out the particular header you are
interested in.

## Functions

-   `http_header(field VARCHAR, value VARCHAR)` returns `http_header`
-   `http_headers(field VARCHAR, value VARCHAR, ...)` returns
    `http_header[]`
-   `http(request http_request)` returns `http_response`
-   `http_get(uri VARCHAR)` returns `http_response`
-   `http_get(uri VARCHAR, data JSONB)` returns `http_response`
-   `http_post(uri VARCHAR, content VARCHAR, content_type VARCHAR)`
    returns `http_response`
-   `http_post(uri VARCHAR, data JSONB)` returns `http_response`
-   `http_put(uri VARCHAR, content VARCHAR, content_type VARCHAR)`
    returns `http_response`
-   `http_patch(uri VARCHAR, content VARCHAR, content_type VARCHAR)`
    returns `http_response`
-   `http_delete(uri VARCHAR, content VARCHAR, content_type VARCHAR))`
    returns `http_response`
-   `http_head(uri VARCHAR)` returns `http_response`
-   `http_set_curlopt(curlopt VARCHAR, value varchar)` returns `boolean`
-   `http_reset_curlopt()` returns `boolean`
-   `http_list_curlopt()` returns `setof(curlopt text, value text)`
-   `urlencode(string VARCHAR)` returns `text`
-   `urlencode(data JSONB)` returns `text`

## CURL Options

Select [CURL options](https://curl.se/libcurl/c/curl_easy_setopt.html)
are available to set using the SQL `SET` command and the appropriate
option name.

-   [CURLOPT_CAINFO](https://curl.se/libcurl/c/CURLOPT_CAINFO.html)
-   [CURLOPT_CONNECTTIMEOUT](https://curl.se/libcurl/c/CURLOPT_CONNECTTIMEOUT.html)
-   [CURLOPT_CONNECTTIMEOUT_MS](https://curl.se/libcurl/c/CURLOPT_CONNECTTIMEOUT_MS.html)
-   [CURLOPT_DNS_SERVERS](https://curl.se/libcurl/c/CURLOPT_DNS_SERVERS.html)
-   [CURLOPT_PRE_PROXY](https://curl.se/libcurl/c/CURLOPT_PRE_PROXY.html)
-   [CURLOPT_PROXY](https://curl.se/libcurl/c/CURLOPT_PROXY.html)
-   [CURLOPT_PROXYPASSWORD](https://curl.se/libcurl/c/CURLOPT_PROXYPASSWORD.html)
-   [CURLOPT_PROXYPORT](https://curl.se/libcurl/c/CURLOPT_PROXYPORT.html)
-   [CURLOPT_PROXYUSERPWD](https://curl.se/libcurl/c/CURLOPT_PROXYUSERPWD.html)
-   [CURLOPT_PROXYUSERNAME](https://curl.se/libcurl/c/CURLOPT_PROXYUSERNAME.html)
-   [CURLOPT_PROXY_TLSAUTH_USERNAME](https://curl.se/libcurl/c/CURLOPT_PROXY_TLSAUTH_USERNAME.html)
-   [CURLOPT_PROXY_TLSAUTH_PASSWORD](https://curl.se/libcurl/c/CURLOPT_PROXY_TLSAUTH_PASSWORD.html)
-   [CURLOPT_PROXY_TLSAUTH_TYPE](https://curl.se/libcurl/c/CURLOPT_PROXY_TLSAUTH_TYPE.html)
-   [CURLOPT_SSL_VERIFYHOST](https://curl.se/libcurl/c/CURLOPT_SSL_VERIFYHOST.html)
-   [CURLOPT_SSL_VERIFYPEER](https://curl.se/libcurl/c/CURLOPT_SSL_VERIFYPEER.html)
-   [CURLOPT_SSLCERT](https://curl.se/libcurl/c/CURLOPT_SSLCERT.html)
-   [CURLOPT_SSLCERT_BLOB](https://curl.se/libcurl/c/CURLOPT_SSLCERT_BLOB.html)
-   [CURLOPT_SSLCERTTYPE](https://curl.se/libcurl/c/CURLOPT_SSLCERTTYPE.html)
-   [CURLOPT_SSLKEY](https://curl.se/libcurl/c/CURLOPT_SSLKEY.html)
-   [CURLOPT_SSLKEY_BLOB](https://curl.se/libcurl/c/CURLOPT_SSLKEY_BLOB.html)
-   [CURLOPT_TCP_KEEPALIVE](https://curl.se/libcurl/c/CURLOPT_TCP_KEEPALIVE.html)
-   [CURLOPT_TCP_KEEPIDLE](https://curl.se/libcurl/c/CURLOPT_TCP_KEEPIDLE.html)
-   [CURLOPT_TIMEOUT](https://curl.se/libcurl/c/CURLOPT_TIMEOUT.html)
-   [CURLOPT_TIMEOUT_MS](https://curl.se/libcurl/c/CURLOPT_TIMEOUT_MS.html)
-   [CURLOPT_TLSAUTH_USERNAME](https://curl.se/libcurl/c/CURLOPT_TLSAUTH_USERNAME.html)
-   [CURLOPT_TLSAUTH_PASSWORD](https://curl.se/libcurl/c/CURLOPT_TLSAUTH_PASSWORD.html)
-   [CURLOPT_TLSAUTH_TYPE](https://curl.se/libcurl/c/CURLOPT_TLSAUTH_TYPE.html)
-   [CURLOPT_USERAGENT](https://curl.se/libcurl/c/CURLOPT_USERAGENT.html)
-   [CURLOPT_USERPWD](https://curl.se/libcurl/c/CURLOPT_USERPWD.html)

For example,

    -- Set the curlopt_proxyport option
    SET http.curlopt_proxyport = '12345';

    -- View the curlopt_proxyport option
    SHOW http.curlopt_proxyport;

    -- View *all* currently set options
    SELECT * FROM http_list_curlopt();

Will set the proxy port option for the lifetime of the database
connection. You can reset all CURL options to their defaults using the
`http_reset_curlopt()` function.

You can permanently set the CURL options for a database or role, using
the `ALTER DATABASE` and `ALTER ROLE` commands.

    -- Applies to all roles in the database
    ALTER DATABASE mydb SET http.curlopt_tlsauth_password = 'secret';

    -- Applies to just one role in the database
    ALTER ROLE myapp IN mydb SET http.curlopt_tlsauth_password = 'secret';

## User Agents

Using this extension as a background automated process without
supervision (e.g as a trigger) may have unintended consequences for
other servers. It is considered a best practice to share contact
information with your requests, so that administrators can reach you in
case your HTTP calls get out of control.

Certain API policies (e.g. [Wikimedia User-Agent
policy](https://foundation.wikimedia.org/wiki/Policy:Wikimedia_Foundation_User-%20Agent_Policy))
may even require sharing specific contact information with each request.
Others may disallow (via `robots.txt`) certain agents they don't
recognize.

For such cases you can set the `CURLOPT_USERAGENT` option

    SET http.curlopt_useragent = 'PgBot/2.1 (+http://pgbot.com/bot.html) Contact abuse@pgbot.com';

    SELECT status, content::json->'headers'->>'User-Agent'
      FROM http_get('http://httpbun.com/headers');


     status |                         user_agent
    --------+-----------------------------------------------------------
        200 | PgBot/2.1 (+http://pgbot.com/bot.html) Contact abuse@pgbot.com

## Keep-Alive & Timeouts

By default each request uses a fresh connection and assures that the
connection is closed when the request is done. This behavior reduces the
chance of consuming system resources (sockets) as the extension runs
over extended periods of time.

High-performance applications may wish to enable keep-alive and
connection persistence to reduce latency and enhance throughput. The
following GUC variable changes the behavior of the http extension to
maintain connections as long as possible:

    SET http.curlopt_tcp_keepalive = 1;

By default a 5 second timeout is set for the completion of a request. If
a different timeout is desired the following GUC variable can be used to
set it in milliseconds:

    SET http.curlopt_timeout_msec = 200;

## Installation

### Debian / Ubuntu apt.postgresql.org

Replace 17 with your version of PostgreSQL

    apt install postgresql-17-http

### Compile from Source

#### General Unix

If you have PostgreSQL devel packages and CURL devel packages installed,
you should have `pg_config` and `curl-config` on your path, so you
should be able to just run `make` (or `gmake`), then `make install`,
then in your database `CREATE EXTENSION http`.

If you already installed a previous version and you just want to
upgrade, then `ALTER EXTENSION http UPDATE`.

#### Debian / Ubuntu / APT

Refer to <https://wiki.postgresql.org/wiki/Apt> for pulling packages
from apt.postgresql.org repository.

    sudo apt install \
      postgresql-server-dev-14 \
      libcurl4-openssl-dev \
      make \
      g++

    make
    sudo make install

If there several PostgreSQL installations available, you might need to
edit the Makefile before running `make`:

    #PG_CONFIG = pg_config
    PG_CONFIG = /usr/lib/postgresql/14/bin/pg_config

### Windows

There is a build available at
[postgresonline](http://www.postgresonline.com/journal/archives/371-http-%20extension.html),
not maintained by me.

### Testing

The integration tests are run with `make install && make installcheck`
and expect to find a running instance of [httpbin](http://httpbin.org)
at port 9080. The easiest way to get that is:

    docker run -p 9080:80 kennethreitz/httpbin

## Why This is a Bad Idea

-   "What happens if the web page takes a long time to return?" Your SQL
    call will just wait there until it does. Make sure your web service
    fails fast. Or (dangerous in a different way) run your query within
    [pg_background](https://github.com/vibhorkum/pg_background) or on a
    schedule with [pg_cron](https://github.com/citusdata/pg_cron).
-   "What if the web page returns junk?" Your SQL call will have to test
    for junk before doing anything with the payload.
-   "What if the web page never returns?" Set a short timeout, or send a
    cancel to the request, or just wait forever.
-   "What if a user queries a page they shouldn't?" Restrict function
    access, or just don't install a footgun like this extension where
    users can access it.

## To Do

-   The [background
    worker](https://www.postgresql.org/docs/current/bgworker.html)
    support could be used to set up an HTTP request queue, so that
    pgsql-http can register a request and callback and then return
    immediately.

## F.33. pg_trgm --- support for similarity of text using trigram matching

[Prev](https://www.postgresql.org/docs/pgsurgery.html "F.32. pg_surgery — perform low-level surgery on relation data")
\|
[Up](https://www.postgresql.org/docs/contrib.html "Appendix F. Additional Supplied Modules and Extensions")
\| Appendix F. Additional Supplied Modules and Extensions \|
[Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")
\|
[Next](https://www.postgresql.org/docs/pgvisibility.html "F.34. pg_visibility — visibility map information and utilities")

------------------------------------------------------------------------

## F.33. pg_trgm --- support for similarity of text using trigram matching

[F.33.1. Trigram (or Trigraph)
Concepts](https://www.postgresql.org/docs/pgtrgm.html#PGTRGM-CONCEPTS)

[F.33.2. Functions and
Operators](https://www.postgresql.org/docs/pgtrgm.html#PGTRGM-FUNCS-OPS)

[F.33.3. GUC
Parameters](https://www.postgresql.org/docs/pgtrgm.html#PGTRGM-%20GUC)

[F.33.4. Index
Support](https://www.postgresql.org/docs/pgtrgm.html#PGTRGM-%20INDEX)

[F.33.5. Text Search
Integration](https://www.postgresql.org/docs/pgtrgm.html#PGTRGM-TEXT-SEARCH)

[F.33.6.
References](https://www.postgresql.org/docs/pgtrgm.html#PGTRGM-%20REFERENCES)

[F.33.7.
Authors](https://www.postgresql.org/docs/pgtrgm.html#PGTRGM-AUTHORS)

The `pg_trgm` module provides functions and operators for determining
the similarity of alphanumeric text based on trigram matching, as well
as index operator classes that support fast searching for similar
strings.

This module is considered "trusted", that is, it can be installed by
non- superusers who have `CREATE` privilege on the current database.

### F.33.1. Trigram (or Trigraph) Concepts

A trigram is a group of three consecutive characters taken from a
string. We can measure the similarity of two strings by counting the
number of trigrams they share. This simple idea turns out to be very
effective for measuring the similarity of words in many natural
languages.

### Note

`pg_trgm` ignores non-word characters (non-alphanumerics) when
extracting trigrams from a string. Each word is considered to have two
spaces prefixed and one space suffixed when determining the set of
trigrams contained in the string. For example, the set of trigrams in
the string "`cat`" is " `c`", " `ca`", "`cat`", and "`at`". The set of
trigrams in the string "`foo|bar`" is " `f`", " `fo`", "`foo`", "`oo`",
" `b`", " `ba`", "`bar`", and "`ar`".

### F.33.2. Functions and Operators

The functions provided by the `pg_trgm` module are shown in [Table
F.25](https://www.postgresql.org/docs/pgtrgm.html#PGTRGM-FUNC-TABLE%20%22Table%20F.25.%20pg_trgm%20Functions%22),
the operators in [Table
F.26](https://www.postgresql.org/docs/pgtrgm.html#PGTRGM-OP-TABLE%20%22Table%20F.26.%20pg_trgm%20Operators%22).

**Table F.25.`pg_trgm` Functions**

## Function Description

`similarity` ( `text`, `text` ) → `real` Returns a number that indicates
how similar the two arguments are. The range of the result is zero
(indicating that the two strings are completely dissimilar) to one
(indicating that the two strings are identical).\
`show_trgm` ( `text` ) → `text[]` Returns an array of all the trigrams
in the given string. (In practice this is seldom useful except for
debugging.)\
`word_similarity` ( `text`, `text` ) → `real` Returns a number that
indicates the greatest similarity between the set of trigrams in the
first string and any continuous extent of an ordered set of trigrams in
the second string. For details, see the explanation below.\
`strict_word_similarity` ( `text`, `text` ) → `real` Same as
`word_similarity`, but forces extent boundaries to match word
boundaries. Since we don't have cross-word trigrams, this function
actually returns greatest similarity between first string and any
continuous extent of words of the second string.\
`show_limit` () → `real` Returns the current similarity threshold used
by the `%` operator. This sets the minimum similarity between two words
for them to be considered similar enough to be misspellings of each
other, for example. (*Deprecated* ; instead use `SHOW`
`pg_trgm.similarity_threshold`.)\
`set_limit` ( `real` ) → `real` Sets the current similarity threshold
that is used by the `%` operator. The threshold must be between 0 and 1
(default is 0.3). Returns the same value passed in. (*Deprecated* ;
instead use `SET` `pg_trgm.similarity_threshold`.)

Consider the following example:

    # SELECT word_similarity('word', 'two words');
     word_similarity
    -----------------
                 0.8
    (1 row)

In the first string, the set of trigrams is
`{" w"," wo","wor","ord","rd "}`. In the second string, the ordered set
of trigrams is
`{" t"," tw","two","wo "," w"," wo","wor","ord","rds","ds "}`. The most
similar extent of an ordered set of trigrams in the second string is
`{" w"," wo","wor","ord"}`, and the similarity is `0.8`.

This function returns a value that can be approximately understood as
the greatest similarity between the first string and any substring of
the second string. However, this function does not add padding to the
boundaries of the extent. Thus, the number of additional characters
present in the second string is not considered, except for the
mismatched word boundaries.

At the same time, `strict_word_similarity` selects an extent of words in
the second string. In the example above, `strict_word_similarity` would
select the extent of a single word `'words'`, whose set of trigrams is
`{" w"," wo","wor","ord","rds","ds "}`.

    # SELECT strict_word_similarity('word', 'two words'), similarity('word', 'words');
     strict_word_similarity | similarity
    ------------------------+------------
                   0.571429 |   0.571429
    (1 row)

Thus, the `strict_word_similarity` function is useful for finding the
similarity to whole words, while `word_similarity` is more suitable for
finding the similarity for parts of words.

**Table F.26.`pg_trgm` Operators**

## Operator Description

`text` `%` `text` → `boolean` Returns `true` if its arguments have a
similarity that is greater than the current similarity threshold set by
`pg_trgm.similarity_threshold`.\
`text` `<%` `text` → `boolean` Returns `true` if the similarity between
the trigram set in the first argument and a continuous extent of an
ordered trigram set in the second argument is greater than the current
word similarity threshold set by `pg_trgm.word_similarity_threshold`
parameter.\
`text` `%>` `text` → `boolean` Commutator of the `<%` operator.\
`text` `<<%` `text` → `boolean` Returns `true` if its second argument
has a continuous extent of an ordered trigram set that matches word
boundaries, and its similarity to the trigram set of the first argument
is greater than the current strict word similarity threshold set by the
`pg_trgm.strict_word_similarity_threshold` parameter.\
`text` `%>>` `text` → `boolean` Commutator of the `<<%` operator.\
`text` `<->` `text` → `real` Returns the "distance" between the
arguments, that is one minus the `similarity()` value.\
`text` `<<->` `text` → `real` Returns the "distance" between the
arguments, that is one minus the `word_similarity()` value.\
`text` `<->>` `text` → `real` Commutator of the `<<->` operator.\
`text` `<<<->` `text` → `real` Returns the "distance" between the
arguments, that is one minus the `strict_word_similarity()` value.\
`text` `<->>>` `text` → `real` Commutator of the `<<<->` operator.

### F.33.3. GUC Parameters

`pg_trgm.similarity_threshold` (`real`) \#

Sets the current similarity threshold that is used by the `%` operator.
The threshold must be between 0 and 1 (default is 0.3).

`pg_trgm.word_similarity_threshold` (`real`) \#

Sets the current word similarity threshold that is used by the `<%` and
`%>` operators. The threshold must be between 0 and 1 (default is 0.6).

`pg_trgm.strict_word_similarity_threshold` (`real`) \#

Sets the current strict word similarity threshold that is used by the
`<<%` and `%>>` operators. The threshold must be between 0 and 1
(default is 0.5).

### F.33.4. Index Support

The `pg_trgm` module provides GiST and GIN index operator classes that
allow you to create an index over a text column for the purpose of very
fast similarity searches. These index types support the above-described
similarity operators, and additionally support trigram-based index
searches for `LIKE`, `ILIKE`, `~`, `~*` and `=` queries. The similarity
comparisons are case- insensitive in a default build of `pg_trgm`.
Inequality operators are not supported. Note that those indexes may not
be as efficient as regular B-tree indexes for equality operator.

Example:

    CREATE TABLE test_trgm (t text);
    CREATE INDEX trgm_idx ON test_trgm USING GIST (t gist_trgm_ops);

or

    CREATE INDEX trgm_idx ON test_trgm USING GIN (t gin_trgm_ops);

`gist_trgm_ops` GiST opclass approximates a set of trigrams as a bitmap
signature. Its optional integer parameter `siglen` determines the
signature length in bytes. The default length is 12 bytes. Valid values
of signature length are between 1 and 2024 bytes. Longer signatures lead
to a more precise search (scanning a smaller fraction of the index and
fewer heap pages), at the cost of a larger index.

Example of creating such an index with a signature length of 32 bytes:

    CREATE INDEX trgm_idx ON test_trgm USING GIST (t gist_trgm_ops(siglen=32));

At this point, you will have an index on the `t` column that you can use
for similarity searching. A typical query is

    SELECT t, similarity(t, '_word_ ') AS sml
      FROM test_trgm
      WHERE t % '_word_ '
      ORDER BY sml DESC, t;

This will return all values in the text column that are sufficiently
similar to *`word`* , sorted from best match to worst. The index will be
used to make this a fast operation even over very large data sets.

A variant of the above query is

    SELECT t, t <-> '_word_ ' AS dist
      FROM test_trgm
      ORDER BY dist LIMIT 10;

This can be implemented quite efficiently by GiST indexes, but not by
GIN indexes. It will usually beat the first formulation when only a
small number of the closest matches is wanted.

Also you can use an index on the `t` column for word similarity or
strict word similarity. Typical queries are:

    SELECT t, word_similarity('_word_ ', t) AS sml
      FROM test_trgm
      WHERE '_word_ ' <% t
      ORDER BY sml DESC, t;

and

    SELECT t, strict_word_similarity('_word_ ', t) AS sml
      FROM test_trgm
      WHERE '_word_ ' <<% t
      ORDER BY sml DESC, t;

This will return all values in the text column for which there is a
continuous extent in the corresponding ordered trigram set that is
sufficiently similar to the trigram set of *`word`* , sorted from best
match to worst. The index will be used to make this a fast operation
even over very large data sets.

Possible variants of the above queries are:

    SELECT t, '_word_ ' <<-> t AS dist
      FROM test_trgm
      ORDER BY dist LIMIT 10;

and

    SELECT t, '_word_ ' <<<-> t AS dist
      FROM test_trgm
      ORDER BY dist LIMIT 10;

This can be implemented quite efficiently by GiST indexes, but not by
GIN indexes.

Beginning in PostgreSQL 9.1, these index types also support index
searches for `LIKE` and `ILIKE`, for example

    SELECT * FROM test_trgm WHERE t LIKE '%foo%bar';

The index search works by extracting trigrams from the search string and
then looking these up in the index. The more trigrams in the search
string, the more effective the index search is. Unlike B-tree based
searches, the search string need not be left-anchored.

Beginning in PostgreSQL 9.3, these index types also support index
searches for regular-expression matches (`~` and `~*` operators), for
example

    SELECT * FROM test_trgm WHERE t ~ '(foo|bar)';

The index search works by extracting trigrams from the regular
expression and then looking these up in the index. The more trigrams
that can be extracted from the regular expression, the more effective
the index search is. Unlike B-tree based searches, the search string
need not be left-anchored.

For both `LIKE` and regular-expression searches, keep in mind that a
pattern with no extractable trigrams will degenerate to a full-index
scan.

The choice between GiST and GIN indexing depends on the relative
performance characteristics of GiST and GIN, which are discussed
elsewhere.

### F.33.5. Text Search Integration

Trigram matching is a very useful tool when used in conjunction with a
full text index. In particular it can help to recognize misspelled input
words that will not be matched directly by the full text search
mechanism.

The first step is to generate an auxiliary table containing all the
unique words in the documents:

    CREATE TABLE words AS SELECT word FROM
            ts_stat('SELECT to_tsvector(''simple'', bodytext) FROM documents');

where `documents` is a table that has a text field `bodytext` that we
wish to search. The reason for using the `simple` configuration with the
`to_tsvector` function, instead of using a language-specific
configuration, is that we want a list of the original (unstemmed) words.

Next, create a trigram index on the word column:

    CREATE INDEX words_idx ON words USING GIN (word gin_trgm_ops);

Now, a `SELECT` query similar to the previous example can be used to
suggest spellings for misspelled words in user search terms. A useful
extra test is to require that the selected words are also of similar
length to the misspelled word.

### Note

Since the `words` table has been generated as a separate, static table,
it will need to be periodically regenerated so that it remains
reasonably up-to- date with the document collection. Keeping it exactly
current is usually unnecessary.

### F.33.6. References

GiST Development Site <http://www.sai.msu.su/~megera/postgres/gist/>

Tsearch2 Development Site
<http://www.sai.msu.su/~megera/postgres/gist/tsearch/V2/>

### F.33.7. Authors

Oleg Bartunov `<[oleg@sai.msu.su](mailto:oleg@sai.msu.su)>`, Moscow,
Moscow University, Russia

Teodor Sigaev `<[teodor@sigaev.ru](mailto:teodor@sigaev.ru)>`, Moscow,
Delta- Soft Ltd.,Russia

Alexander Korotkov
`<[a.korotkov@postgrespro.ru](mailto:a.korotkov@postgrespro.ru)>`,
Moscow, Postgres Professional, Russia

Documentation: Christopher Kings-Lynne

This module is sponsored by Delta-Soft Ltd., Moscow, Russia.

------------------------------------------------------------------------

  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  [Prev](https://www.postgresql.org/docs/pgsurgery.html "F.32. pg_surgery — perform low-level surgery on relation data")   [Up](https://www.postgresql.org/docs/contrib.html "Appendix F. Additional Supplied Modules and Extensions")   [Next](https://www.postgresql.org/docs/pgvisibility.html "F.34. pg_visibility — visibility map information and utilities")
  ------------------------------------------------------------------------------------------------------------------------ ------------------------------------------------------------------------------------------------------------- ----------------------------------------------------------------------------------------------------------------------------
  F.32. pg_surgery --- perform low-level surgery on relation data                                                          [Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")                            F.34. pg_visibility --- visibility map information and utilities

  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

# pgvector

Open-source vector similarity search for Postgres

Store your vectors with the rest of your data. Supports:

-   exact and approximate nearest neighbor search
-   single-precision, half-precision, binary, and sparse vectors
-   L2 distance, inner product, cosine distance, L1 distance, Hamming
    distance, and Jaccard distance
-   any language with a Postgres client

Plus [ACID](https://en.wikipedia.org/wiki/ACID) compliance,
point-in-time recovery, JOINs, and all of the other [great
features](https://www.postgresql.org/about/) of Postgres

[![Build
Status](https://github.com/pgvector/pgvector/actions/workflows/build.yml/badge.svg)](https://github.com/pgvector/pgvector/actions)

## Installation

### Linux and Mac

Compile and install the extension (supports Postgres 13+)

    cd /tmp
    git clone --branch v0.8.0 https://github.com/pgvector/pgvector.git
    cd pgvector
    make
    make install # may need sudo

See the installation notes if you run into issues

You can also install it with Docker, Homebrew, PGXN, APT, Yum, pkg, or
conda- forge, and it comes preinstalled with Postgres.app and many
hosted providers. There are also instructions for [GitHub
Actions](https://github.com/pgvector/setup-pgvector).

### Windows

Ensure [C++ support in Visual
Studio](https://learn.microsoft.com/en-%20us/cpp/build/building-on-the-command-line?view=msvc-170#download-and-install-%20the-tools)
is installed, and run:

    call "C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\Build\vcvars64.bat"

Note: The exact path will vary depending on your Visual Studio version
and edition

Then use `nmake` to build:

    set "PGROOT=C:\Program Files\PostgreSQL\16"
    cd %TEMP%
    git clone --branch v0.8.0 https://github.com/pgvector/pgvector.git
    cd pgvector
    nmake /F Makefile.win
    nmake /F Makefile.win install

See the installation notes if you run into issues

You can also install it with Docker or conda-forge.

## Getting Started

Enable the extension (do this once in each database where you want to
use it)

    CREATE EXTENSION vector;

Create a vector column with 3 dimensions

    CREATE TABLE items (id bigserial PRIMARY KEY, embedding vector(3));

Insert vectors

    INSERT INTO items (embedding) VALUES ('[1,2,3]'), ('[4,5,6]');

Get the nearest neighbors by L2 distance

    SELECT * FROM items ORDER BY embedding <-> '[3,1,2]' LIMIT 5;

Also supports inner product (`<#>`), cosine distance (`<=>`), and L1
distance (`<+>`)

Note: `<#>` returns the negative inner product since Postgres only
supports `ASC` order index scans on operators

## Storing

Create a new table with a vector column

    CREATE TABLE items (id bigserial PRIMARY KEY, embedding vector(3));

Or add a vector column to an existing table

    ALTER TABLE items ADD COLUMN embedding vector(3);

Also supports half-precision, binary, and sparse vectors

Insert vectors

    INSERT INTO items (embedding) VALUES ('[1,2,3]'), ('[4,5,6]');

Or load vectors in bulk using `COPY`
([example](https://github.com/pgvector/pgvector-%20python/blob/master/examples/loading/example.py))

    COPY items (embedding) FROM STDIN WITH (FORMAT BINARY);

Upsert vectors

    INSERT INTO items (id, embedding) VALUES (1, '[1,2,3]'), (2, '[4,5,6]')
        ON CONFLICT (id) DO UPDATE SET embedding = EXCLUDED.embedding;

Update vectors

    UPDATE items SET embedding = '[1,2,3]' WHERE id = 1;

Delete vectors

    DELETE FROM items WHERE id = 1;

## Querying

Get the nearest neighbors to a vector

    SELECT * FROM items ORDER BY embedding <-> '[3,1,2]' LIMIT 5;

Supported distance functions are:

-   `<->` - L2 distance
-   `<#>` - (negative) inner product
-   `<=>` - cosine distance
-   `<+>` - L1 distance
-   `<~>` - Hamming distance (binary vectors)
-   `<%>` - Jaccard distance (binary vectors)

Get the nearest neighbors to a row

    SELECT * FROM items WHERE id != 1 ORDER BY embedding <-> (SELECT embedding FROM items WHERE id = 1) LIMIT 5;

Get rows within a certain distance

    SELECT * FROM items WHERE embedding <-> '[3,1,2]' < 5;

Note: Combine with `ORDER BY` and `LIMIT` to use an index

#### Distances

Get the distance

    SELECT embedding <-> '[3,1,2]' AS distance FROM items;

For inner product, multiply by -1 (since `<#>` returns the negative
inner product)

    SELECT (embedding <#> '[3,1,2]') * -1 AS inner_product FROM items;

For cosine similarity, use 1 - cosine distance

    SELECT 1 - (embedding <=> '[3,1,2]') AS cosine_similarity FROM items;

#### Aggregates

Average vectors

    SELECT AVG(embedding) FROM items;

Average groups of vectors

    SELECT category_id, AVG(embedding) FROM items GROUP BY category_id;

## Indexing

By default, pgvector performs exact nearest neighbor search, which
provides perfect recall.

You can add an index to use approximate nearest neighbor search, which
trades some recall for speed. Unlike typical indexes, you will see
different results for queries after adding an approximate index.

Supported index types are:

-   HNSW
-   IVFFlat

## HNSW

An HNSW index creates a multilayer graph. It has better query
performance than IVFFlat (in terms of speed-recall tradeoff), but has
slower build times and uses more memory. Also, an index can be created
without any data in the table since there isn't a training step like
IVFFlat.

Add an index for each distance function you want to use.

L2 distance

    CREATE INDEX ON items USING hnsw (embedding vector_l2_ops);

Note: Use `halfvec_l2_ops` for `halfvec` and `sparsevec_l2_ops` for
`sparsevec` (and similar with the other distance functions)

Inner product

    CREATE INDEX ON items USING hnsw (embedding vector_ip_ops);

Cosine distance

    CREATE INDEX ON items USING hnsw (embedding vector_cosine_ops);

L1 distance

    CREATE INDEX ON items USING hnsw (embedding vector_l1_ops);

Hamming distance

    CREATE INDEX ON items USING hnsw (embedding bit_hamming_ops);

Jaccard distance

    CREATE INDEX ON items USING hnsw (embedding bit_jaccard_ops);

Supported types are:

-   `vector` - up to 2,000 dimensions
-   `halfvec` - up to 4,000 dimensions
-   `bit` - up to 64,000 dimensions
-   `sparsevec` - up to 1,000 non-zero elements

### Index Options

Specify HNSW parameters

-   `m` - the max number of connections per layer (16 by default)

-   `ef_construction` - the size of the dynamic candidate list for
    constructing the graph (64 by default)

    CREATE INDEX ON items USING hnsw (embedding vector_l2_ops) WITH (m =
    16, ef_construction = 64);

A higher value of `ef_construction` provides better recall at the cost
of index build time / insert speed.

### Query Options

Specify the size of the dynamic candidate list for search (40 by
default)

    SET hnsw.ef_search = 100;

A higher value provides better recall at the cost of speed.

Use `SET LOCAL` inside a transaction to set it for a single query

    BEGIN;
    SET LOCAL hnsw.ef_search = 100;
    SELECT ...
    COMMIT;

### Index Build Time

Indexes build significantly faster when the graph fits into
`maintenance_work_mem`

    SET maintenance_work_mem = '8GB';

A notice is shown when the graph no longer fits

    NOTICE:  hnsw graph no longer fits into maintenance_work_mem after 100000 tuples
    DETAIL:  Building will take significantly more time.
    HINT:  Increase maintenance_work_mem to speed up builds.

Note: Do not set `maintenance_work_mem` so high that it exhausts the
memory on the server

Like other index types, it's faster to create an index after loading
your initial data

You can also speed up index creation by increasing the number of
parallel workers (2 by default)

    SET max_parallel_maintenance_workers = 7; -- plus leader

For a large number of workers, you may need to increase
`max_parallel_workers` (8 by default)

The index options also have a significant impact on build time (use the
defaults unless seeing low recall)

### Indexing Progress

Check [indexing
progress](https://www.postgresql.org/docs/current/progress-%20reporting.html#CREATE-INDEX-PROGRESS-REPORTING)

    SELECT phase, round(100.0 * blocks_done / nullif(blocks_total, 0), 1) AS "%" FROM pg_stat_progress_create_index;

The phases for HNSW are:

1.  `initializing`
2.  `loading tuples`

## IVFFlat

An IVFFlat index divides vectors into lists, and then searches a subset
of those lists that are closest to the query vector. It has faster build
times and uses less memory than HNSW, but has lower query performance
(in terms of speed-recall tradeoff).

Three keys to achieving good recall are:

1.  Create the index *after* the table has some data
2.  Choose an appropriate number of lists - a good place to start is
    `rows / 1000` for up to 1M rows and `sqrt(rows)` for over 1M rows
3.  When querying, specify an appropriate number of probes (higher is
    better for recall, lower is better for speed) - a good place to
    start is `sqrt(lists)`

Add an index for each distance function you want to use.

L2 distance

    CREATE INDEX ON items USING ivfflat (embedding vector_l2_ops) WITH (lists = 100);

Note: Use `halfvec_l2_ops` for `halfvec` (and similar with the other
distance functions)

Inner product

    CREATE INDEX ON items USING ivfflat (embedding vector_ip_ops) WITH (lists = 100);

Cosine distance

    CREATE INDEX ON items USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);

Hamming distance

    CREATE INDEX ON items USING ivfflat (embedding bit_hamming_ops) WITH (lists = 100);

Supported types are:

-   `vector` - up to 2,000 dimensions
-   `halfvec` - up to 4,000 dimensions
-   `bit` - up to 64,000 dimensions

### Query Options

Specify the number of probes (1 by default)

    SET ivfflat.probes = 10;

A higher value provides better recall at the cost of speed, and it can
be set to the number of lists for exact nearest neighbor search (at
which point the planner won't use the index)

Use `SET LOCAL` inside a transaction to set it for a single query

    BEGIN;
    SET LOCAL ivfflat.probes = 10;
    SELECT ...
    COMMIT;

### Index Build Time

Speed up index creation on large tables by increasing the number of
parallel workers (2 by default)

    SET max_parallel_maintenance_workers = 7; -- plus leader

For a large number of workers, you may also need to increase
`max_parallel_workers` (8 by default)

### Indexing Progress

Check [indexing
progress](https://www.postgresql.org/docs/current/progress-%20reporting.html#CREATE-INDEX-PROGRESS-REPORTING)

    SELECT phase, round(100.0 * tuples_done / nullif(tuples_total, 0), 1) AS "%" FROM pg_stat_progress_create_index;

The phases for IVFFlat are:

1.  `initializing`
2.  `performing k-means`
3.  `assigning tuples`
4.  `loading tuples`

Note: `%` is only populated during the `loading tuples` phase

## Filtering

There are a few ways to index nearest neighbor queries with a `WHERE`
clause.

    SELECT * FROM items WHERE category_id = 123 ORDER BY embedding <-> '[3,1,2]' LIMIT 5;

A good place to start is creating an index on the filter column. This
can provide fast, exact nearest neighbor search in many cases. Postgres
has a number of [index
types](https://www.postgresql.org/docs/current/indexes-%20types.html)
for this: B-tree (default), hash, GiST, SP-GiST, GIN, and BRIN.

    CREATE INDEX ON items (category_id);

For multiple columns, consider a [multicolumn
index](https://www.postgresql.org/docs/current/indexes-multicolumn.html).

    CREATE INDEX ON items (location_id, category_id);

Exact indexes work well for conditions that match a low percentage of
rows. Otherwise, approximate indexes can work better.

    CREATE INDEX ON items USING hnsw (embedding vector_l2_ops);

With approximate indexes, filtering is applied *after* the index is
scanned. If a condition matches 10% of rows, with HNSW and the default
`hnsw.ef_search` of 40, only 4 rows will match on average. For more
rows, increase `hnsw.ef_search`.

    SET hnsw.ef_search = 200;

Starting with 0.8.0, you can enable iterative index scans, which will
automatically scan more of the index when needed.

    SET hnsw.iterative_scan = strict_order;

If filtering by only a few distinct values, consider [partial
indexing](https://www.postgresql.org/docs/current/indexes-partial.html).

    CREATE INDEX ON items USING hnsw (embedding vector_l2_ops) WHERE (category_id = 123);

If filtering by many different values, consider
[partitioning](https://www.postgresql.org/docs/current/ddl-partitioning.html).

    CREATE TABLE items (embedding vector(3), category_id int) PARTITION BY LIST(category_id);

## Iterative Index Scans

*Added in 0.8.0*

With approximate indexes, queries with filtering can return less results
since filtering is applied *after* the index is scanned. Starting with
0.8.0, you can enable iterative index scans, which will automatically
scan more of the index until enough results are found (or it reaches
`hnsw.max_scan_tuples` or `ivfflat.max_probes`).

Iterative scans can use strict or relaxed ordering.

Strict ensures results are in the exact order by distance

    SET hnsw.iterative_scan = strict_order;

Relaxed allows results to be slightly out of order by distance, but
provides better recall

    SET hnsw.iterative_scan = relaxed_order;
    # or
    SET ivfflat.iterative_scan = relaxed_order;

With relaxed ordering, you can use a [materialized
CTE](https://www.postgresql.org/docs/current/queries-with.html#QUERIES-WITH-%20CTE-MATERIALIZATION)
to get strict ordering

    WITH relaxed_results AS MATERIALIZED (
        SELECT id, embedding <-> '[1,2,3]' AS distance FROM items WHERE category_id = 123 ORDER BY distance LIMIT 5
    ) SELECT * FROM relaxed_results ORDER BY distance;

For queries that filter by distance, use a materialized CTE and place
the distance filter outside of it for best performance (due to the
[current
behavior](https://www.postgresql.org/message-%20id/flat/CAOdR5yGUoMQ6j7M5hNUXrySzaqZVGf_Ne%2B8fwZMRKTFxU1nbJg%40mail.gmail.com)
of the Postgres executor)

    WITH nearest_results AS MATERIALIZED (
        SELECT id, embedding <-> '[1,2,3]' AS distance FROM items ORDER BY distance LIMIT 5
    ) SELECT * FROM nearest_results WHERE distance < 5 ORDER BY distance;

Note: Place any other filters inside the CTE

### Iterative Scan Options

Since scanning a large portion of an approximate index is expensive,
there are options to control when a scan ends.

#### HNSW

Specify the max number of tuples to visit (20,000 by default)

    SET hnsw.max_scan_tuples = 20000;

Note: This is approximate and does not affect the initial scan

Specify the max amount of memory to use, as a multiple of `work_mem` (1
by default)

    SET hnsw.scan_mem_multiplier = 2;

Note: Try increasing this if increasing `hnsw.max_scan_tuples` does not
improve recall

#### IVFFlat

Specify the max number of probes

    SET ivfflat.max_probes = 100;

Note: If this is lower than `ivfflat.probes`, `ivfflat.probes` will be
used

## Half-Precision Vectors

Use the `halfvec` type to store half-precision vectors

    CREATE TABLE items (id bigserial PRIMARY KEY, embedding halfvec(3));

## Half-Precision Indexing

Index vectors at half precision for smaller indexes

    CREATE INDEX ON items USING hnsw ((embedding::halfvec(3)) halfvec_l2_ops);

Get the nearest neighbors

    SELECT * FROM items ORDER BY embedding::halfvec(3) <-> '[1,2,3]' LIMIT 5;

## Binary Vectors

Use the `bit` type to store binary vectors
([example](https://github.com/pgvector/pgvector-%20python/blob/master/examples/imagehash/example.py))

    CREATE TABLE items (id bigserial PRIMARY KEY, embedding bit(3));
    INSERT INTO items (embedding) VALUES ('000'), ('111');

Get the nearest neighbors by Hamming distance

    SELECT * FROM items ORDER BY embedding <~> '101' LIMIT 5;

Also supports Jaccard distance (`<%>`)

## Binary Quantization

Use expression indexing for binary quantization

    CREATE INDEX ON items USING hnsw ((binary_quantize(embedding)::bit(3)) bit_hamming_ops);

Get the nearest neighbors by Hamming distance

    SELECT * FROM items ORDER BY binary_quantize(embedding)::bit(3) <~> binary_quantize('[1,-2,3]') LIMIT 5;

Re-rank by the original vectors for better recall

    SELECT * FROM (
        SELECT * FROM items ORDER BY binary_quantize(embedding)::bit(3) <~> binary_quantize('[1,-2,3]') LIMIT 20
    ) ORDER BY embedding <=> '[1,-2,3]' LIMIT 5;

## Sparse Vectors

Use the `sparsevec` type to store sparse vectors

    CREATE TABLE items (id bigserial PRIMARY KEY, embedding sparsevec(5));

Insert vectors

    INSERT INTO items (embedding) VALUES ('{1:1,3:2,5:3}/5'), ('{1:4,3:5,5:6}/5');

The format is `{index1:value1,index2:value2}/dimensions` and indices
start at 1 like SQL arrays

Get the nearest neighbors by L2 distance

    SELECT * FROM items ORDER BY embedding <-> '{1:3,3:1,5:2}/5' LIMIT 5;

## Hybrid Search

Use together with Postgres [full-text
search](https://www.postgresql.org/docs/current/textsearch-intro.html)
for hybrid search.

    SELECT id, content FROM items, plainto_tsquery('hello search') query
        WHERE textsearch @@ query ORDER BY ts_rank_cd(textsearch, query) DESC LIMIT 5;

You can use [Reciprocal Rank
Fusion](https://github.com/pgvector/pgvector-%20python/blob/master/examples/hybrid_search/rrf.py)
or a [cross-
encoder](https://github.com/pgvector/pgvector-%20python/blob/master/examples/hybrid_search/cross_encoder.py)
to combine results.

## Indexing Subvectors

Use expression indexing to index subvectors

    CREATE INDEX ON items USING hnsw ((subvector(embedding, 1, 3)::vector(3)) vector_cosine_ops);

Get the nearest neighbors by cosine distance

    SELECT * FROM items ORDER BY subvector(embedding, 1, 3)::vector(3) <=> subvector('[1,2,3,4,5]'::vector, 1, 3) LIMIT 5;

Re-rank by the full vectors for better recall

    SELECT * FROM (
        SELECT * FROM items ORDER BY subvector(embedding, 1, 3)::vector(3) <=> subvector('[1,2,3,4,5]'::vector, 1, 3) LIMIT 20
    ) ORDER BY embedding <=> '[1,2,3,4,5]' LIMIT 5;

## Performance

### Tuning

Use a tool like [PgTune](https://pgtune.leopard.in.ua/) to set initial
values for Postgres server parameters. For instance, `shared_buffers`
should typically be 25% of the server's memory. You can find the config
file with:

    SHOW config_file;

And check individual settings with:

    SHOW shared_buffers;

Be sure to restart Postgres for changes to take effect.

### Loading

Use `COPY` for bulk loading data
([example](https://github.com/pgvector/pgvector-%20python/blob/master/examples/loading/example.py)).

    COPY items (embedding) FROM STDIN WITH (FORMAT BINARY);

Add any indexes *after* loading the initial data for best performance.

### Indexing

See index build time for HNSW and IVFFlat.

In production environments, create indexes concurrently to avoid
blocking writes.

    CREATE INDEX CONCURRENTLY ...

### Querying

Use `EXPLAIN ANALYZE` to debug performance.

    EXPLAIN ANALYZE SELECT * FROM items ORDER BY embedding <-> '[3,1,2]' LIMIT 5;

#### Exact Search

To speed up queries without an index, increase
`max_parallel_workers_per_gather`.

    SET max_parallel_workers_per_gather = 4;

If vectors are normalized to length 1 (like [OpenAI
embeddings](https://platform.openai.com/docs/guides/embeddings/which-distance-%20function-should-i-use)),
use inner product for best performance.

    SELECT * FROM items ORDER BY embedding <#> '[3,1,2]' LIMIT 5;

#### Approximate Search

To speed up queries with an IVFFlat index, increase the number of
inverted lists (at the expense of recall).

    CREATE INDEX ON items USING ivfflat (embedding vector_l2_ops) WITH (lists = 1000);

### Vacuuming

Vacuuming can take a while for HNSW indexes. Speed it up by reindexing
first.

    REINDEX INDEX CONCURRENTLY index_name;
    VACUUM table_name;

## Monitoring

Monitor performance with
[pg_stat_statements](https://www.postgresql.org/docs/current/pgstatstatements.html)
(be sure to add it to `shared_preload_libraries`).

    CREATE EXTENSION pg_stat_statements;

Get the most time-consuming queries with:

    SELECT query, calls, ROUND((total_plan_time + total_exec_time) / calls) AS avg_time_ms,
        ROUND((total_plan_time + total_exec_time) / 60000) AS total_time_min
        FROM pg_stat_statements ORDER BY total_plan_time + total_exec_time DESC LIMIT 20;

Note: Replace `total_plan_time + total_exec_time` with `total_time` for
Postgres \< 13

Monitor recall by comparing results from approximate search with exact
search.

    BEGIN;
    SET LOCAL enable_indexscan = off; -- use exact search
    SELECT ...
    COMMIT;

## Scaling

Scale pgvector the same way you scale Postgres.

Scale vertically by increasing memory, CPU, and storage on a single
instance. Use existing tools to tune parameters and monitor performance.

Scale horizontally with
[replicas](https://www.postgresql.org/docs/current/hot-standby.html), or
use [Citus](https://github.com/citusdata/citus) or another approach for
sharding
([example](https://github.com/pgvector/pgvector-%20python/blob/master/examples/citus/example.py)).

## Languages

Use pgvector from any language with a Postgres client. You can even
generate and store vectors in one language and query them in another.

  ------------------------------------------------------------------------------------------------------
  Language                            Libraries / Examples
  ----------------------------------- ------------------------------------------------------------------
  C                                   [pgvector-c](https://github.com/pgvector/pgvector-c)

  C++                                 [pgvector-cpp](https://github.com/pgvector/pgvector-cpp)

  C#, F#, Visual Basic                [pgvector-dotnet](https://github.com/pgvector/pgvector-dotnet)

  Crystal                             [pgvector-crystal](https://github.com/pgvector/pgvector-crystal)

  D                                   [pgvector-d](https://github.com/pgvector/pgvector-d)

  Dart                                [pgvector-dart](https://github.com/pgvector/pgvector-dart)

  Elixir                              [pgvector-elixir](https://github.com/pgvector/pgvector-elixir)

  Erlang                              [pgvector-erlang](https://github.com/pgvector/pgvector-erlang)

  Fortran                             [pgvector-fortran](https://github.com/pgvector/pgvector-fortran)

  Gleam                               [pgvector-gleam](https://github.com/pgvector/pgvector-gleam)

  Go                                  [pgvector-go](https://github.com/pgvector/pgvector-go)

  Haskell                             [pgvector-haskell](https://github.com/pgvector/pgvector-haskell)

  Java, Kotlin, Groovy, Scala         [pgvector-java](https://github.com/pgvector/pgvector-java)

  JavaScript, TypeScript              [pgvector-node](https://github.com/pgvector/pgvector-node)

  Julia                               [pgvector-julia](https://github.com/pgvector/pgvector-julia)

  Lisp                                [pgvector-lisp](https://github.com/pgvector/pgvector-lisp)

  Lua                                 [pgvector-lua](https://github.com/pgvector/pgvector-lua)

  Nim                                 [pgvector-nim](https://github.com/pgvector/pgvector-nim)

  OCaml                               [pgvector-ocaml](https://github.com/pgvector/pgvector-ocaml)

  Perl                                [pgvector-perl](https://github.com/pgvector/pgvector-perl)

  PHP                                 [pgvector-php](https://github.com/pgvector/pgvector-php)

  Python                              [pgvector-python](https://github.com/pgvector/pgvector-python)

  R                                   [pgvector-r](https://github.com/pgvector/pgvector-r)

  Raku                                [pgvector-raku](https://github.com/pgvector/pgvector-raku)

  Ruby                                [pgvector-ruby](https://github.com/pgvector/pgvector-ruby),
                                      [Neighbor](https://github.com/ankane/neighbor)

  Rust                                [pgvector-rust](https://github.com/pgvector/pgvector-rust)

  Swift                               [pgvector-swift](https://github.com/pgvector/pgvector-swift)

  Zig                                 [pgvector-zig](https://github.com/pgvector/pgvector-zig)
  ------------------------------------------------------------------------------------------------------

## Frequently Asked Questions

#### How many vectors can be stored in a single table?

A non-partitioned table has a limit of 32 TB by default in Postgres. A
partitioned table can have thousands of partitions of that size.

#### Is replication supported?

Yes, pgvector uses the write-ahead log (WAL), which allows for
replication and point-in-time recovery.

#### What if I want to index vectors with more than 2,000 dimensions?

You can use half-precision indexing to index up to 4,000 dimensions or
binary quantization to index up to 64,000 dimensions. Another option is
[dimensionality
reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction).

#### Can I store vectors with different dimensions in the same column?

You can use `vector` as the type (instead of `vector(3)`).

    CREATE TABLE embeddings (model_id bigint, item_id bigint, embedding vector, PRIMARY KEY (model_id, item_id));

However, you can only create indexes on rows with the same number of
dimensions (using
[expression](https://www.postgresql.org/docs/current/indexes-%20expressional.html)
and
[partial](https://www.postgresql.org/docs/current/indexes-partial.html)
indexing):

    CREATE INDEX ON embeddings USING hnsw ((embedding::vector(3)) vector_l2_ops) WHERE (model_id = 123);

and query with:

    SELECT * FROM embeddings WHERE model_id = 123 ORDER BY embedding::vector(3) <-> '[3,1,2]' LIMIT 5;

#### Can I store vectors with more precision?

You can use the `double precision[]` or `numeric[]` type to store
vectors with more precision.

    CREATE TABLE items (id bigserial PRIMARY KEY, embedding double precision[]);

    -- use {} instead of [] for Postgres arrays
    INSERT INTO items (embedding) VALUES ('{1,2,3}'), ('{4,5,6}');

Optionally, add a [check
constraint](https://www.postgresql.org/docs/current/ddl-constraints.html)
to ensure data can be converted to the `vector` type and has the
expected dimensions.

    ALTER TABLE items ADD CHECK (vector_dims(embedding::vector) = 3);

Use [expression
indexing](https://www.postgresql.org/docs/current/indexes-%20expressional.html)
to index (at a lower precision):

    CREATE INDEX ON items USING hnsw ((embedding::vector(3)) vector_l2_ops);

and query with:

    SELECT * FROM items ORDER BY embedding::vector(3) <-> '[3,1,2]' LIMIT 5;

#### Do indexes need to fit into memory?

No, but like other index types, you'll likely see better performance if
they do. You can get the size of an index with:

    SELECT pg_size_pretty(pg_relation_size('index_name'));

## Troubleshooting

#### Why isn't a query using an index?

The query needs to have an `ORDER BY` and `LIMIT`, and the `ORDER BY`
must be the result of a distance operator (not an expression) in
ascending order.

    -- index
    ORDER BY embedding <=> '[3,1,2]' LIMIT 5;

    -- no index
    ORDER BY 1 - (embedding <=> '[3,1,2]') DESC LIMIT 5;

You can encourage the planner to use an index for a query with:

    BEGIN;
    SET LOCAL enable_seqscan = off;
    SELECT ...
    COMMIT;

Also, if the table is small, a table scan may be faster.

#### Why isn't a query using a parallel table scan?

The planner doesn't consider [out-of-line
storage](https://www.postgresql.org/docs/current/storage-toast.html) in
cost estimates, which can make a serial scan look cheaper. You can
reduce the cost of a parallel scan for a query with:

    BEGIN;
    SET LOCAL min_parallel_table_scan_size = 1;
    SET LOCAL parallel_setup_cost = 1;
    SELECT ...
    COMMIT;

or choose to store vectors inline:

    ALTER TABLE items ALTER COLUMN embedding SET STORAGE PLAIN;

#### Why are there less results for a query after adding an HNSW index?

Results are limited by the size of the dynamic candidate list
(`hnsw.ef_search`), which is 40 by default. There may be even less
results due to dead tuples or filtering conditions in the query.
Enabling iterative index scans can help address this.

Also, note that `NULL` vectors are not indexed (as well as zero vectors
for cosine distance).

#### Why are there less results for a query after adding an IVFFlat index?

The index was likely created with too little data for the number of
lists. Drop the index until the table has more data.

    DROP INDEX index_name;

Results can also be limited by the number of probes (`ivfflat.probes`).
Enabling iterative index scans can address this.

Also, note that `NULL` vectors are not indexed (as well as zero vectors
for cosine distance).

## Reference

-   Vector
-   Halfvec
-   Bit
-   Sparsevec

### Vector Type

Each vector takes `4 * dimensions + 8` bytes of storage. Each element is
a single-precision floating-point number (like the `real` type in
Postgres), and all elements must be finite (no `NaN`, `Infinity` or
`-Infinity`). Vectors can have up to 16,000 dimensions.

### Vector Operators

  Operator   Description                   Added
  ---------- ----------------------------- -------------
  \+         element-wise addition         
  \-         element-wise subtraction      
  \*         element-wise multiplication   0.5.0
                                           concatenate
  \<-\>      Euclidean distance            
  \<#\>      negative inner product        
  \<=\>      cosine distance               
  \<+\>      taxicab distance              0.7.0

### Vector Functions

  -------------------------------------------------------------------------
  Function                  Description             Added
  ------------------------- ----------------------- -----------------------
  binary_quantize(vector) → binary quantize         0.7.0
  bit                                               

  cosine_distance(vector,   cosine distance         
  vector) → double                                  
  precision                                         

  inner_product(vector,     inner product           
  vector) → double                                  
  precision                                         

  l1_distance(vector,       taxicab distance        0.5.0
  vector) → double                                  
  precision                                         

  l2_distance(vector,       Euclidean distance      
  vector) → double                                  
  precision                                         

  l2_normalize(vector) →    Normalize with          0.7.0
  vector                    Euclidean norm          

  subvector(vector,         subvector               0.7.0
  integer, integer) →                               
  vector                                            

  vector_dims(vector) →     number of dimensions    
  integer                                           

  vector_norm(vector) →     Euclidean norm          
  double precision                                  
  -------------------------------------------------------------------------

### Vector Aggregate Functions

  Function               Description   Added
  ---------------------- ------------- -------
  avg(vector) → vector   average       
  sum(vector) → vector   sum           0.5.0

### Halfvec Type

Each half vector takes `2 * dimensions + 8` bytes of storage. Each
element is a half-precision floating-point number, and all elements must
be finite (no `NaN`, `Infinity` or `-Infinity`). Half vectors can have
up to 16,000 dimensions.

### Halfvec Operators

  Operator   Description                   Added
  ---------- ----------------------------- -------------
  \+         element-wise addition         0.7.0
  \-         element-wise subtraction      0.7.0
  \*         element-wise multiplication   0.7.0
                                           concatenate
  \<-\>      Euclidean distance            0.7.0
  \<#\>      negative inner product        0.7.0
  \<=\>      cosine distance               0.7.0
  \<+\>      taxicab distance              0.7.0

### Halfvec Functions

  --------------------------------------------------------------------------
  Function                   Description             Added
  -------------------------- ----------------------- -----------------------
  binary_quantize(halfvec) → binary quantize         0.7.0
  bit                                                

  cosine_distance(halfvec,   cosine distance         0.7.0
  halfvec) → double                                  
  precision                                          

  inner_product(halfvec,     inner product           0.7.0
  halfvec) → double                                  
  precision                                          

  l1_distance(halfvec,       taxicab distance        0.7.0
  halfvec) → double                                  
  precision                                          

  l2_distance(halfvec,       Euclidean distance      0.7.0
  halfvec) → double                                  
  precision                                          

  l2_norm(halfvec) → double  Euclidean norm          0.7.0
  precision                                          

  l2_normalize(halfvec) →    Normalize with          0.7.0
  halfvec                    Euclidean norm          

  subvector(halfvec,         subvector               0.7.0
  integer, integer) →                                
  halfvec                                            

  vector_dims(halfvec) →     number of dimensions    0.7.0
  integer                                            
  --------------------------------------------------------------------------

### Halfvec Aggregate Functions

  Function                 Description   Added
  ------------------------ ------------- -------
  avg(halfvec) → halfvec   average       0.7.0
  sum(halfvec) → halfvec   sum           0.7.0

### Bit Type

Each bit vector takes `dimensions / 8 + 8` bytes of storage. See the
[Postgres
docs](https://www.postgresql.org/docs/current/datatype-bit.html) for
more info.

### Bit Operators

  Operator   Description        Added
  ---------- ------------------ -------
  \<\~\>     Hamming distance   0.7.0
  \<%\>      Jaccard distance   0.7.0

### Bit Functions

  -----------------------------------------------------------------------
  Function                Description             Added
  ----------------------- ----------------------- -----------------------
  hamming_distance(bit,   Hamming distance        0.7.0
  bit) → double precision                         

  jaccard_distance(bit,   Jaccard distance        0.7.0
  bit) → double precision                         
  -----------------------------------------------------------------------

### Sparsevec Type

Each sparse vector takes `8 * non-zero elements + 16` bytes of storage.
Each element is a single-precision floating-point number, and all
elements must be finite (no `NaN`, `Infinity` or `-Infinity`). Sparse
vectors can have up to 16,000 non-zero elements.

### Sparsevec Operators

  Operator   Description              Added
  ---------- ------------------------ -------
  \<-\>      Euclidean distance       0.7.0
  \<#\>      negative inner product   0.7.0
  \<=\>      cosine distance          0.7.0
  \<+\>      taxicab distance         0.7.0

### Sparsevec Functions

  ----------------------------------------------------------------------------
  Function                     Description             Added
  ---------------------------- ----------------------- -----------------------
  cosine_distance(sparsevec,   cosine distance         0.7.0
  sparsevec) → double                                  
  precision                                            

  inner_product(sparsevec,     inner product           0.7.0
  sparsevec) → double                                  
  precision                                            

  l1_distance(sparsevec,       taxicab distance        0.7.0
  sparsevec) → double                                  
  precision                                            

  l2_distance(sparsevec,       Euclidean distance      0.7.0
  sparsevec) → double                                  
  precision                                            

  l2_norm(sparsevec) → double  Euclidean norm          0.7.0
  precision                                            

  l2_normalize(sparsevec) →    Normalize with          0.7.0
  sparsevec                    Euclidean norm          
  ----------------------------------------------------------------------------

## Installation Notes - Linux and Mac

### Postgres Location

If your machine has multiple Postgres installations, specify the path to
[pg_config](https://www.postgresql.org/docs/current/app-pgconfig.html)
with:

    export PG_CONFIG=/Library/PostgreSQL/17/bin/pg_config

Then re-run the installation instructions (run `make clean` before
`make` if needed). If `sudo` is needed for `make install`, use:

    sudo --preserve-env=PG_CONFIG make install

A few common paths on Mac are:

-   EDB installer - `/Library/PostgreSQL/17/bin/pg_config`
-   Homebrew (arm64) - `/opt/homebrew/opt/postgresql@17/bin/pg_config`
-   Homebrew (x86-64) - `/usr/local/opt/postgresql@17/bin/pg_config`

Note: Replace `17` with your Postgres server version

### Missing Header

If compilation fails with
`fatal error: postgres.h: No such file or directory`, make sure Postgres
development files are installed on the server.

For Ubuntu and Debian, use:

    sudo apt install postgresql-server-dev-17

Note: Replace `17` with your Postgres server version

### Missing SDK

If compilation fails and the output includes
`warning: no such sysroot directory` on Mac, your Postgres installation
points to a path that no longer exists.

    pg_config --cppflags

Reinstall Postgres to fix this.

### Portability

By default, pgvector compiles with `-march=native` on some platforms for
best performance. However, this can lead to `Illegal instruction` errors
if trying to run the compiled extension on a different machine.

To compile for portability, use:

    make OPTFLAGS=""

## Installation Notes - Windows

### Missing Header

If compilation fails with
`Cannot open include file: 'postgres.h': No such file or directory`,
make sure `PGROOT` is correct.

### Mismatched Architecture

If compilation fails with `error C2196: case value '4' already used`,
make sure `vcvars64.bat` was called. Then run
`nmake /F Makefile.win clean` and re- run the installation instructions.

### Missing Symbol

If linking fails with
`unresolved external symbol float_to_shortest_decimal_bufn` with
Postgres 17.0-17.2, upgrade to Postgres 17.3+.

### Permissions

If installation fails with `Access is denied`, re-run the installation
instructions as an administrator.

## Additional Installation Methods

### Docker

Get the [Docker image](https://hub.docker.com/r/pgvector/pgvector) with:

    docker pull pgvector/pgvector:pg17

This adds pgvector to the [Postgres
image](https://hub.docker.com/_/postgres) (replace `17` with your
Postgres server version, and run it the same way).

You can also build the image manually:

    git clone --branch v0.8.0 https://github.com/pgvector/pgvector.git
    cd pgvector
    docker build --pull --build-arg PG_MAJOR=17 -t myuser/pgvector .

If you increase `maintenance_work_mem`, make sure `--shm-size` is at
least that size to avoid an error with parallel HNSW index builds.

    docker run --shm-size=1g ...

### Homebrew

With Homebrew Postgres, you can use:

    brew install pgvector

Note: This only adds it to the `postgresql@17` and `postgresql@14`
formulas

### PGXN

Install from the [PostgreSQL Extension
Network](https://pgxn.org/dist/vector) with:

    pgxn install vector

### APT

Debian and Ubuntu packages are available from the [PostgreSQL APT
Repository](https://wiki.postgresql.org/wiki/Apt). Follow the [setup
instructions](https://wiki.postgresql.org/wiki/Apt#Quickstart) and run:

    sudo apt install postgresql-17-pgvector

Note: Replace `17` with your Postgres server version

### Yum

RPM packages are available from the [PostgreSQL Yum
Repository](https://yum.postgresql.org/). Follow the [setup
instructions](https://www.postgresql.org/download/linux/redhat/) for
your distribution and run:

    sudo yum install pgvector_17
    # or
    sudo dnf install pgvector_17

Note: Replace `17` with your Postgres server version

### pkg

Install the FreeBSD package with:

    pkg install postgresql16-pgvector

or the port with:

    cd /usr/ports/databases/pgvector
    make install

### conda-forge

With Conda Postgres, install from
[conda-forge](https://anaconda.org/conda-%20forge/pgvector) with:

    conda install -c conda-forge pgvector

This method is
[community-maintained](https://github.com/conda-forge/pgvector-%20feedstock)
by [@mmcauliffe](https://github.com/mmcauliffe)

### Postgres.app

Download the [latest release](https://postgresapp.com/downloads.html)
with Postgres 15+.

## Hosted Postgres

pgvector is available on [these
providers](https://github.com/pgvector/pgvector/issues/54).

## Upgrading

Install the latest version (use the same method as the original
installation). Then in each database you want to upgrade, run:

    ALTER EXTENSION vector UPDATE;

You can check the version in the current database with:

    SELECT extversion FROM pg_extension WHERE extname = 'vector';

## Thanks

Thanks to:

-   [PASE: PostgreSQL Ultra-High-Dimensional Approximate Nearest
    Neighbor Search
    Extension](https://dl.acm.org/doi/pdf/10.1145/3318464.3386131)
-   [Faiss: A Library for Efficient Similarity Search and Clustering of
    Dense Vectors](https://github.com/facebookresearch/faiss)
-   [Using the Triangle Inequality to Accelerate
    k-means](https://cdn.aaai.org/ICML/2003/ICML03-022.pdf)
-   [k-means++: The Advantage of Careful
    Seeding](https://theory.stanford.edu/~sergei/papers/kMeansPP-soda.pdf)
-   [Concept Decompositions for Large Sparse Text Data using
    Clustering](https://www.cs.utexas.edu/users/inderjit/public_papers/concept_mlj.pdf)
-   [Efficient and Robust Approximate Nearest Neighbor Search using
    Hierarchical Navigable Small World
    Graphs](https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf)

## History

View the
[changelog](https://github.com/pgvector/pgvector/blob/master/CHANGELOG.md)

## Contributing

Everyone is encouraged to help improve this project. Here are a few ways
you can help:

-   [Report bugs](https://github.com/pgvector/pgvector/issues)
-   Fix bugs and [submit pull
    requests](https://github.com/pgvector/pgvector/pulls)
-   Write, clarify, or fix documentation
-   Suggest or add new features

To get started with development:

    git clone https://github.com/pgvector/pgvector.git
    cd pgvector
    make
    make install

To run all tests:

    make installcheck        # regression tests
    make prove_installcheck  # TAP tests

To run single tests:

    make installcheck REGRESS=functions                            # regression test
    make prove_installcheck PROVE_TESTS=test/t/001_ivfflat_wal.pl  # TAP test

To enable assertions:

    make clean && PG_CFLAGS="-DUSE_ASSERT_CHECKING" make && make install

To enable benchmarking:

    make clean && PG_CFLAGS="-DIVFFLAT_BENCH" make && make install

To show memory usage:

    make clean && PG_CFLAGS="-DHNSW_MEMORY -DIVFFLAT_MEMORY" make && make install

To get k-means metrics:

    make clean && PG_CFLAGS="-DIVFFLAT_KMEANS_DEBUG" make && make install

Resources for contributors

-   [Extension Building
    Infrastructure](https://www.postgresql.org/docs/current/extend-pgxs.html)
-   [Index Access Method Interface
    Definition](https://www.postgresql.org/docs/current/indexam.html)
-   [Generic WAL
    Records](https://www.postgresql.org/docs/current/generic-wal.html)

## Chapter 44. PL/Python --- Python Procedural Language

[Prev](https://www.postgresql.org/docs/plperl-under-the-hood.html "43.8. PL/Perl Under the Hood")
\|
[Up](https://www.postgresql.org/docs/server-programming.html "Part V. Server Programming")
\| Part V. Server Programming \|
[Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")
\|
[Next](https://www.postgresql.org/docs/plpython-funcs.html "44.1. PL/Python Functions")

------------------------------------------------------------------------

## Chapter 44. PL/Python --- Python Procedural Language

**Table of Contents**

[44.1. PL/Python
Functions](https://www.postgresql.org/docs/plpython-%20funcs.html)

[44.2. Data Values](https://www.postgresql.org/docs/plpython-data.html)

[44.2.1. Data Type
Mapping](https://www.postgresql.org/docs/plpython-%20data.html#PLPYTHON-DATA-TYPE-MAPPING)

[44.2.2. Null,
None](https://www.postgresql.org/docs/plpython-%20data.html#PLPYTHON-DATA-NULL)

[44.2.3. Arrays,
Lists](https://www.postgresql.org/docs/plpython-%20data.html#PLPYTHON-ARRAYS)

[44.2.4. Composite
Types](https://www.postgresql.org/docs/plpython-%20data.html#PLPYTHON-DATA-COMPOSITE-TYPES)

[44.2.5. Set-Returning
Functions](https://www.postgresql.org/docs/plpython-%20data.html#PLPYTHON-DATA-SET-RETURNING-FUNCS)

[44.3. Sharing
Data](https://www.postgresql.org/docs/plpython-sharing.html)

[44.4. Anonymous Code
Blocks](https://www.postgresql.org/docs/plpython-%20do.html)

[44.5. Trigger
Functions](https://www.postgresql.org/docs/plpython-%20trigger.html)

[44.6. Database
Access](https://www.postgresql.org/docs/plpython-%20database.html)

[44.6.1. Database Access
Functions](https://www.postgresql.org/docs/plpython-%20database.html#PLPYTHON-DATABASE-ACCESS-FUNCS)

[44.6.2. Trapping
Errors](https://www.postgresql.org/docs/plpython-%20database.html#PLPYTHON-TRAPPING)

[44.7. Explicit
Subtransactions](https://www.postgresql.org/docs/plpython-%20subtransaction.html)

[44.7.1. Subtransaction Context
Managers](https://www.postgresql.org/docs/plpython-%20subtransaction.html#PLPYTHON-SUBTRANSACTION-CONTEXT-MANAGERS)

[44.8. Transaction
Management](https://www.postgresql.org/docs/plpython-%20transactions.html)

[44.9. Utility
Functions](https://www.postgresql.org/docs/plpython-util.html)

[44.10. Python 2 vs. Python
3](https://www.postgresql.org/docs/plpython-%20python23.html)

[44.11. Environment
Variables](https://www.postgresql.org/docs/plpython-%20envar.html)

The PL/Python procedural language allows PostgreSQL functions and
procedures to be written in the [Python
language](https://www.python.org).

To install PL/Python in a particular database, use
`CREATE EXTENSION plpython3u`.

### Tip

If a language is installed into `template1`, all subsequently created
databases will have the language installed automatically.

PL/Python is only available as an "untrusted" language, meaning it does
not offer any way of restricting what users can do in it and is
therefore named `plpython3u`. A trusted variant `plpython` might become
available in the future if a secure execution mechanism is developed in
Python. The writer of a function in untrusted PL/Python must take care
that the function cannot be used to do anything unwanted, since it will
be able to do anything that could be done by a user logged in as the
database administrator. Only superusers can create functions in
untrusted languages such as `plpython3u`.

### Note

Users of source packages must specially enable the build of PL/Python
during the installation process. (Refer to the installation instructions
for more information.) Users of binary packages might find PL/Python in
a separate subpackage.

------------------------------------------------------------------------

  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  [Prev](https://www.postgresql.org/docs/plperl-under-the-hood.html "43.8. PL/Perl Under the Hood")   [Up](https://www.postgresql.org/docs/server-programming.html "Part V. Server Programming")   [Next](https://www.postgresql.org/docs/plpython-funcs.html "44.1. PL/Python Functions")
  --------------------------------------------------------------------------------------------------- -------------------------------------------------------------------------------------------- -----------------------------------------------------------------------------------------
  43.8. PL/Perl Under the Hood                                                                        [Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")           44.1. PL/Python Functions

  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## F.47. uuid-ossp --- a UUID generator

[Prev](https://www.postgresql.org/docs/unaccent.html "F.46. unaccent — a text search dictionary which removes diacritics")
\|
[Up](https://www.postgresql.org/docs/contrib.html "Appendix F. Additional Supplied Modules and Extensions")
\| Appendix F. Additional Supplied Modules and Extensions \|
[Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")
\|
[Next](https://www.postgresql.org/docs/xml2.html "F.48. xml2 — XPath querying and XSLT functionality")

------------------------------------------------------------------------

## F.47. uuid-ossp --- a UUID generator

[F.47.1. `uuid-ossp`
Functions](https://www.postgresql.org/docs/uuid-%20ossp.html#UUID-OSSP-FUNCTIONS-SECT)

[F.47.2. Building
`uuid-ossp`](https://www.postgresql.org/docs/uuid-%20ossp.html#UUID-OSSP-BUILDING)

[F.47.3.
Author](https://www.postgresql.org/docs/uuid-ossp.html#UUID-OSSP-%20AUTHOR)

The `uuid-ossp` module provides functions to generate universally unique
identifiers (UUIDs) using one of several standard algorithms. There are
also functions to produce certain special UUID constants. This module is
only necessary for special requirements beyond what is available in core
PostgreSQL. See [Section
9.14](https://www.postgresql.org/docs/functions-%20uuid.html "9.14. UUID Functions")
for built-in ways to generate UUIDs.

This module is considered "trusted", that is, it can be installed by
non- superusers who have `CREATE` privilege on the current database.

### F.47.1. `uuid-ossp` Functions

[Table
F.33](https://www.postgresql.org/docs/uuid-ossp.html#UUID-OSSP-%20FUNCTIONS "Table F.33. Functions for UUID Generation")
shows the functions available to generate UUIDs. The relevant standards
ITU-T Rec. X.667, ISO/IEC 9834-8:2005, and [RFC
4122](https://datatracker.ietf.org/doc/html/rfc4122) specify four
algorithms for generating UUIDs, identified by the version numbers 1, 3,
4, and 5. (There is no version 2 algorithm.) Each of these algorithms
could be suitable for a different set of applications.

**Table F.33. Functions for UUID Generation**

## Function Description

`uuid_generate_v1` () → `uuid` Generates a version 1 UUID. This involves
the MAC address of the computer and a time stamp. Note that UUIDs of
this kind reveal the identity of the computer that created the
identifier and the time at which it did so, which might make it
unsuitable for certain security- sensitive applications.\
`uuid_generate_v1mc` () → `uuid` Generates a version 1 UUID, but uses a
random multicast MAC address instead of the real MAC address of the
computer.\
`uuid_generate_v3` ( *`namespace`* `uuid`, *`name`* `text` ) → `uuid`
Generates a version 3 UUID in the given namespace using the specified
input name. The namespace should be one of the special constants
produced by the `uuid_ns_*()` functions shown in [Table
F.34](https://www.postgresql.org/docs/uuid-ossp.html#UUID-OSSP-CONSTANTS%20%22Table%20F.34.%20Functions%20Returning%20UUID%20Constants%22).
(It could be any UUID in theory.) The name is an identifier in the
selected namespace. For example:

    SELECT uuid_generate_v3(uuid_ns_url(), 'http://www.postgresql.org');

The name parameter will be MD5-hashed, so the cleartext cannot be
derived from the generated UUID. The generation of UUIDs by this method
has no random or environment-dependent element and is therefore
reproducible.\
`uuid_generate_v4` () → `uuid` Generates a version 4 UUID, which is
derived entirely from random numbers.\
`uuid_generate_v5` ( *`namespace`* `uuid`, *`name`* `text` ) → `uuid`
Generates a version 5 UUID, which works like a version 3 UUID except
that SHA-1 is used as a hashing method. Version 5 should be preferred
over version 3 because SHA-1 is thought to be more secure than MD5.

**Table F.34. Functions Returning UUID Constants**

## Function Description

`uuid_nil` () → `uuid` Returns a "nil" UUID constant, which does not
occur as a real UUID.\
`uuid_ns_dns` () → `uuid` Returns a constant designating the DNS
namespace for UUIDs.\
`uuid_ns_url` () → `uuid` Returns a constant designating the URL
namespace for UUIDs.\
`uuid_ns_oid` () → `uuid` Returns a constant designating the ISO object
identifier (OID) namespace for UUIDs. (This pertains to ASN.1 OIDs,
which are unrelated to the OIDs used in PostgreSQL.)\
`uuid_ns_x500` () → `uuid` Returns a constant designating the X.500
distinguished name (DN) namespace for UUIDs.

### F.47.2. Building `uuid-ossp`

Historically this module depended on the OSSP UUID library, which
accounts for the module's name. While the OSSP UUID library can still be
found at <http://www.ossp.org/pkg/lib/uuid/>, it is not well maintained,
and is becoming increasingly difficult to port to newer platforms.
`uuid-ossp` can now be built without the OSSP library on some platforms.
On FreeBSD and some other BSD-derived platforms, suitable UUID creation
functions are included in the core `libc` library. On Linux, macOS, and
some other platforms, suitable functions are provided in the `libuuid`
library, which originally came from the `e2fsprogs` project (though on
modern Linux it is considered part of `util-linux-ng`). When invoking
`configure`, specify `--with-uuid=bsd` to use the BSD functions, or
`--with-uuid=e2fs` to use `e2fsprogs`' `libuuid`, or `--with-uuid=ossp`
to use the OSSP UUID library. More than one of these libraries might be
available on a particular machine, so `configure` does not automatically
choose one.

### F.47.3. Author

Peter Eisentraut `<[peter_e@gmx.net](mailto:peter_e@gmx.net)>`

------------------------------------------------------------------------

  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  [Prev](https://www.postgresql.org/docs/unaccent.html "F.46. unaccent — a text search dictionary which removes diacritics")   [Up](https://www.postgresql.org/docs/contrib.html "Appendix F. Additional Supplied Modules and Extensions")   [Next](https://www.postgresql.org/docs/xml2.html "F.48. xml2 — XPath querying and XSLT functionality")
  ---------------------------------------------------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------------------------- --------------------------------------------------------------------------------------------------------
  F.46. unaccent --- a text search dictionary which removes diacritics                                                         [Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")                            F.48. xml2 --- XPath querying and XSLT functionality

  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

## F.48. xml2 --- XPath querying and XSLT functionality

[Prev](https://www.postgresql.org/docs/uuid-ossp.html "F.47. uuid-ossp — a UUID generator")
\|
[Up](https://www.postgresql.org/docs/contrib.html "Appendix F. Additional Supplied Modules and Extensions")
\| Appendix F. Additional Supplied Modules and Extensions \|
[Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")
\|
[Next](https://www.postgresql.org/docs/contrib-prog.html "Appendix G. Additional Supplied Programs")

------------------------------------------------------------------------

## F.48. xml2 --- XPath querying and XSLT functionality

[F.48.1. Deprecation
Notice](https://www.postgresql.org/docs/xml2.html#XML2-DEPRECATION)

[F.48.2. Description of
Functions](https://www.postgresql.org/docs/xml2.html#XML2-FUNCTIONS)

[F.48.3.
`xpath_table`](https://www.postgresql.org/docs/xml2.html#XML2-XPATH-%20TABLE)

[F.48.4. XSLT
Functions](https://www.postgresql.org/docs/xml2.html#XML2-XSLT)

[F.48.5. Author](https://www.postgresql.org/docs/xml2.html#XML2-AUTHOR)

The `xml2` module provides XPath querying and XSLT functionality.

### F.48.1. Deprecation Notice

From PostgreSQL 8.3 on, there is XML-related functionality based on the
SQL/XML standard in the core server. That functionality covers XML
syntax checking and XPath queries, which is what this module does, and
more, but the API is not at all compatible. It is planned that this
module will be removed in a future version of PostgreSQL in favor of the
newer standard API, so you are encouraged to try converting your
applications. If you find that some of the functionality of this module
is not available in an adequate form with the newer API, please explain
your issue to
`<[pgsql- hackers@lists.postgresql.org](mailto:pgsql-hackers@lists.postgresql.org)>`
so that the deficiency can be addressed.

### F.48.2. Description of Functions

[Table
F.35](https://www.postgresql.org/docs/xml2.html#XML2-FUNCTIONS-TABLE%20%22Table%20F.35.%20xml2%20Functions%22)
shows the functions provided by this module. These functions provide
straightforward XML parsing and XPath queries.

**Table F.35.`xml2` Functions**

## Function Description

`xml_valid` ( *`document`* `text` ) → `boolean` Parses the given
document and returns true if the document is well-formed XML. (Note:
this is an alias for the standard PostgreSQL function
`xml_is_well_formed()`. The name `xml_valid()` is technically incorrect
since validity and well-formedness have different meanings in XML.)\
`xpath_string` ( *`document`* `text`, *`query`* `text` ) → `text`
Evaluates the XPath query on the supplied document, and casts the result
to `text`.\
`xpath_number` ( *`document`* `text`, *`query`* `text` ) → `real`
Evaluates the XPath query on the supplied document, and casts the result
to `real`.\
`xpath_bool` ( *`document`* `text`, *`query`* `text` ) → `boolean`
Evaluates the XPath query on the supplied document, and casts the result
to `boolean`.\
`xpath_nodeset` ( *`document`* `text`, *`query`* `text`, *`toptag`*
`text`, *`itemtag`* `text` ) → `text` Evaluates the query on the
document and wraps the result in XML tags. If the result is multivalued,
the output will look like:

    <toptag>
    <itemtag>Value 1 which could be an XML fragment</itemtag>
    <itemtag>Value 2....</itemtag>
    </toptag>

If either *`toptag`* or *`itemtag`* is an empty string, the relevant tag
is omitted.\
`xpath_nodeset` ( *`document`* `text`, *`query`* `text`, *`itemtag`*
`text` ) → `text` Like `xpath_nodeset(document, query, toptag, itemtag)`
but result omits *`toptag`*.\
`xpath_nodeset` ( *`document`* `text`, *`query`* `text` ) → `text` Like
`xpath_nodeset(document, query, toptag, itemtag)` but result omits both
tags.\
`xpath_list` ( *`document`* `text`, *`query`* `text`, *`separator`*
`text` ) → `text` Evaluates the query on the document and returns
multiple values separated by the specified separator, for example
`Value 1,Value 2,Value 3` if *`separator`* is `,`.\
`xpath_list` ( *`document`* `text`, *`query`* `text` ) → `text` This is
a wrapper for the above function that uses `,` as the separator.

### F.48.3. `xpath_table`

    xpath_table(text key, text document, text relation, text xpaths, text criteria) returns setof record

`xpath_table` is a table function that evaluates a set of XPath queries
on each of a set of documents and returns the results as a table. The
primary key field from the original document table is returned as the
first column of the result so that the result set can readily be used in
joins. The parameters are described in [Table
F.36](https://www.postgresql.org/docs/xml2.html#XML2-XPATH-TABLE-PARAMETERS%20%22Table%20F.36.%20xpath_table%20Parameters%22).

**Table F.36.`xpath_table` Parameters**

  -----------------------------------------------------------------------
  Parameter                           Description
  ----------------------------------- -----------------------------------
  *`key`*                             the name of the "key" field ---
                                      this is just a field to be used as
                                      the first column of the output
                                      table, i.e., it identifies the
                                      record from which each output row
                                      came (see note below about multiple
                                      values)

  *`document`*                        the name of the field containing
                                      the XML document

  *`relation`*                        the name of the table or view
                                      containing the documents

  *`xpaths`*                          one or more XPath expressions,
                                      separated by `|`

  *`criteria`*                        the contents of the WHERE clause.
                                      This cannot be omitted, so use
                                      `true` or `1=1` if you want to
                                      process all the rows in the
                                      relation
  -----------------------------------------------------------------------

These parameters (except the XPath strings) are just substituted into a
plain SQL SELECT statement, so you have some flexibility --- the
statement is

`SELECT <key>, <document> FROM <relation> WHERE <criteria>`

so those parameters can be *anything* valid in those particular
locations. The result from this SELECT needs to return exactly two
columns (which it will unless you try to list multiple fields for key or
document). Beware that this simplistic approach requires that you
validate any user-supplied values to avoid SQL injection attacks.

The function has to be used in a `FROM` expression, with an `AS` clause
to specify the output columns; for example

    SELECT * FROM
    xpath_table('article_id',
                'article_xml',
                'articles',
                '/article/author|/article/pages|/article/title',
                'date_entered > ''2003-01-01'' ')
    AS t(article_id integer, author text, page_count integer, title text);

The `AS` clause defines the names and types of the columns in the output
table. The first is the "key" field and the rest correspond to the XPath
queries. If there are more XPath queries than result columns, the extra
queries will be ignored. If there are more result columns than XPath
queries, the extra columns will be NULL.

Notice that this example defines the `page_count` result column as an
integer. The function deals internally with string representations, so
when you say you want an integer in the output, it will take the string
representation of the XPath result and use PostgreSQL input functions to
transform it into an integer (or whatever type the `AS` clause
requests). An error will result if it can't do this --- for example if
the result is empty --- so you may wish to just stick to `text` as the
column type if you think your data has any problems.

The calling `SELECT` statement doesn't necessarily have to be just
`SELECT *` --- it can reference the output columns by name or join them
to other tables. The function produces a virtual table with which you
can perform any operation you wish (e.g., aggregation, joining, sorting
etc.). So we could also have:

    SELECT t.title, p.fullname, p.email
    FROM xpath_table('article_id', 'article_xml', 'articles',
                     '/article/title|/article/author/@id',
                     'xpath_string(article_xml,''/article/@date'') > ''2003-03-20'' ')
           AS t(article_id integer, title text, author_id integer),
         tblPeopleInfo AS p
    WHERE t.author_id = p.person_id;

as a more complicated example. Of course, you could wrap all of this in
a view for convenience.

#### F.48.3.1. Multivalued Results

The `xpath_table` function assumes that the results of each XPath query
might be multivalued, so the number of rows returned by the function may
not be the same as the number of input documents. The first row returned
contains the first result from each query, the second row the second
result from each query. If one of the queries has fewer values than the
others, null values will be returned instead.

In some cases, a user will know that a given XPath query will return
only a single result (perhaps a unique document identifier) --- if used
alongside an XPath query returning multiple results, the single-valued
result will appear only on the first row of the result. The solution to
this is to use the key field as part of a join against a simpler XPath
query. As an example:

    CREATE TABLE test (
        id int PRIMARY KEY,
        xml text
    );

    INSERT INTO test VALUES (1, '<doc num="C1">
    <line num="L1"><a>1</a><b>2</b><c>3</c></line>
    <line num="L2"><a>11</a><b>22</b><c>33</c></line>
    </doc>');

    INSERT INTO test VALUES (2, '<doc num="C2">
    <line num="L1"><a>111</a><b>222</b><c>333</c></line>
    <line num="L2"><a>111</a><b>222</b><c>333</c></line>
    </doc>');

    SELECT * FROM
      xpath_table('id','xml','test',
                  '/doc/@num|/doc/line/@num|/doc/line/a|/doc/line/b|/doc/line/c',
                  'true')
      AS t(id int, doc_num varchar(10), line_num varchar(10), val1 int, val2 int, val3 int)
    WHERE id = 1 ORDER BY doc_num, line_num

     id | doc_num | line_num | val1 | val2 | val3
    ----+---------+----------+------+------+------
      1 | C1      | L1       |    1 |    2 |    3
      1 |         | L2       |   11 |   22 |   33

To get `doc_num` on every line, the solution is to use two invocations
of `xpath_table` and join the results:

    SELECT t.*,i.doc_num FROM
      xpath_table('id', 'xml', 'test',
                  '/doc/line/@num|/doc/line/a|/doc/line/b|/doc/line/c',
                  'true')
        AS t(id int, line_num varchar(10), val1 int, val2 int, val3 int),
      xpath_table('id', 'xml', 'test', '/doc/@num', 'true')
        AS i(id int, doc_num varchar(10))
    WHERE i.id=t.id AND i.id=1
    ORDER BY doc_num, line_num;

     id | line_num | val1 | val2 | val3 | doc_num
    ----+----------+------+------+------+---------
      1 | L1       |    1 |    2 |    3 | C1
      1 | L2       |   11 |   22 |   33 | C1
    (2 rows)

### F.48.4. XSLT Functions

The following functions are available if libxslt is installed:

#### F.48.4.1. `xslt_process`

    xslt_process(text document, text stylesheet, text paramlist) returns text

This function applies the XSL stylesheet to the document and returns the
transformed result. The `paramlist` is a list of parameter assignments
to be used in the transformation, specified in the form `a=1,b=2`. Note
that the parameter parsing is very simple-minded: parameter values
cannot contain commas!

There is also a two-parameter version of `xslt_process` which does not
pass any parameters to the transformation.

### F.48.5. Author

John Gray `<[jgray@azuli.co.uk](mailto:jgray@azuli.co.uk)>`

Development of this module was sponsored by Torchbox
Ltd. (www.torchbox.com). It has the same BSD license as PostgreSQL.

------------------------------------------------------------------------

  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  [Prev](https://www.postgresql.org/docs/uuid-ossp.html "F.47. uuid-ossp — a UUID generator")   [Up](https://www.postgresql.org/docs/contrib.html "Appendix F. Additional Supplied Modules and Extensions")   [Next](https://www.postgresql.org/docs/contrib-prog.html "Appendix G. Additional Supplied Programs")
  --------------------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------------------
  F.47. uuid-ossp --- a UUID generator                                                          [Home](https://www.postgresql.org/docs/index.html "PostgreSQL 17.4 Documentation")                            Appendix G. Additional Supplied Programs

  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
